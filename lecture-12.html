<!DOCTYPE html>
<html>
  <head>
    <title>NYU OS 202 Lecture Slides</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">Start Lecture #12

**Remark**: Go over the previous homework.

**Remark**: Midterm covers the notes through chapter 6\. Also labs 1 and 2. Not the programs in the labs but the OS concepts. Indeed both are on the practice exam.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
# Chapter 3 Memory Management

Also called **storage management** or **space management**.

The memory manager must deal with the **storage hierarchy** present in modern
machines.

  * The hierarchy consists of registers (the highest level), cache, central memory, disk.
  * Various (hardware and software) memory managers move data from level to level of the hierarchy.
  * We are concerned with the central memory ↔ disk boundary.

The same questions are asked about the cache ↔ central memory boundary when
one studies computer architecture. Surprisingly, the terminology is almost
completely different!

  * When should we move data up to a higher level? 

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * Fetch on demand (e.g., demand paging, which is dominant now and which we shall study in detail).
    * Prefetch 
      * Read-ahead for file I/O.
      * Software or hardware prefetching.
      * Large cache lines and pages.
      * Extreme example. Entire job present whenever running.
  * Unless the top level has sufficient memory for the entire system, we must also decide when to move data down to a lower level. This is normally called evicting the data (from the higher level).
  * In OS we concentrate on the central-memory/disk layers and transitions.
  * In architecture we concentrate on the cache/central-memory layers and transitions (and use different terminology).

We will see in the next few weeks that there are three independent decision:

  1. Should we have segmentation.
  2. Should we have paging.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  3. Should we employ fetch on demand.

Memory management implements **address translation**.

  * Convert virtual addresses to physical addresses. 
    * Also called logical to real address translation.
    * A **virtual address** is the address expressed in the program.
    * A **physical address** is the address understood by the computer hardware.
  * The translation from virtual to physical addresses is performed by the **Memory Management Unit** or (MMU).
  * Another example of address translation is the conversion of _relative_ addresses to _absolute_ addresses by the linker.
  * The translation from virtual to physical addresses might be trivial (e.g., the identity). As we shall see, it is definitely not trivial in a modern general purpose OS.
  * The translation might be difficult (i.e., slow). 
    * Often includes addition/shift/mask--not too bad.
    * Often includes memory references. 
      * VERY serious.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
      * Solution is to cache translations in a **Translation Lookaside Buffer** (TLB). Sometimes called a translation buffer (TB).

**Homework:** What is the difference between a physical address and a virtual address?

#### **When** is address translation performed?

  1. At program writing time 
    * Programmer explicitly states where everything goes.
    * No longer done.
  2. At compile time 
    * Compiler generates _physical_ addresses.
    * Requires knowledge of where the compilation unit will be loaded.
    * No linker.
    * Loader is trivial.
    * Primitive.
    * Rarely used (MSDOS .COM files).
  


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  3. At link-edit time (the linker lab) 
    * Compiler 
      * Generates relative (a.k.a. relocatable) addresses for each compilation unit.
      * References external addresses.
    * Linkage editor 
      * Converts relocatable addresses to absolute (i.e., relocates relative addresses).
      * Resolves external references.
      * Must also converts virtual to physical addresses by knowing where the linked program will be loaded. Linker lab does this, but it is trivial since we assume the linked program will be loaded at 0.
    * Loader is still trivial.
    * Hardware requirements are small.
    * A program can be loaded only where specified and **cannot** move once loaded.
    * Not used much any more.
  

  4. At load time 

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * Similar to at link-edit time, but do **not** fix the starting address.
    * Program can be loaded anywhere.
    * Program can move but cannot be split.
    * Need modest hardware: base/limit registers.
    * Loader sets the base/limit registers.
    * No longer common.
  

  5. At execution time 
    * Addresses translated dynamically during execution.
    * Hardware needed to perform the virtual to physical address translation quickly.
    * Currently dominates.
    * Much more information later.

#### Extensions

  1. Dynamic Loading 
    * When executing a call, check if the module is loaded.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * If it is not loaded, have a linking loader load it and update the tables to indicate that it now is loaded and where it is.
    * This procedure slows down all calls to the routine (not just the first one that must load the module) unless you rewrite code dynamically.
    * Not used much.
  

  2. Dynamic Linking. 
    * This is covered later.
    * Commonly used.

**Note:** I will place ** before each memory management scheme.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 3.1 No Memory Management

The entire process remains in memory from start to finish and does not move.

The sum of the memory requirements of all jobs in the system cannot exceed the
size of physical memory.

![monoprogramming](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/monop
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
rog
ramming.png)

#### Monoprogramming

The good old days when everything was easy.

  * No address translation done by the OS (i.e., address translation is _not_ performed dynamically during execution).
  * Either reload the OS for each job (or don't have an OS, which is almost the same), or protect the OS from the job. 
    * One way to protect (part of) the OS is to have it in ROM.
    * Of course, must have the OS (read-write) data in RAM.
    * Can have a separate OS address space that is accessible only in supervisor mode.
    * Might put just some drivers in ROM (BIOS).
  * The user employs **overlays** if the memory needed by a job exceeds the size of physical memory. 
    * Programmer breaks program into pieces.
    * A root piece is always memory resident.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * The root contains calls to load and unload various pieces.
    * Programmer's responsibility to ensure that a piece is already loaded when it is called.
    * No longer used, but we couldn't have gotten to the moon in the 60s without it (I think).
    * Overlays have been replaced by dynamic address translation and other features (e.g., demand paging) that have the system support logical address sizes greater than physical address sizes.
    * Fred Brooks (leader of IBM's OS/360 project and author of The mythical man month) remarked that the OS/360 linkage editor was terrific, especially in its support for overlays, but by the time it came out, overlays were no longer used.

#### Running Multiple Programs Without a Memory Abstraction

This can be done via swapping if you have only one program loaded at a time. A
more general version of swapping is discussed below.

One can also support a limited form of multiprogramming, similar to MFT (wh
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ich
is described next). In this limited version, the loader relocates all relative
addresses, thus permitting multiple processes to coexist in physical memory
the way your linker permitted multiple modules in a single process to coexist.

![MFT](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/MFT.png)


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
#### **Multiprogramming with Fixed Partitions

Two goals of multiprogramming are to improve CPU utilization, by overlapping
CPU and I/O, and to permit short jobs to finish quickly.

  * This scheme was used by IBM for system 360 OS/MFT (multiprogramming with a fixed number of tasks).
  * An alternative would have a single input list instead of one queue for each partition. 
    * With this alternative, if there are no big jobs, one can use the big partition for little jobs.
    * The single list is not a queue since would want to remove the first job for each partition.
    * I don't think IBM did this.
  * You can think of the input lists(s) as the ready list(s) with a scheduling policy of run to completion in each partition.
  * Each partition was **mono**programmed, the **multi**programming occurred **across** partitions.
  * The partition boundaries are **not** movable (must reboot to move a job
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
). 
    * So the partitions are of fixed size.
    * MFT can have large **internal fragmentation**, i.e., wasted space _inside_ a region of memory assigned to a process.
  * Each process has a single segment (i.e., the virtual address space is contiguous). We will discuss segments later.
  * The physical address space is also contiguous (i.e., the program is stored as one piece).
  * No sharing of memory between process.
  * No dynamic address translation.
  * OS/MFT is an example of address translation during load time. 
    * The system must establish addressability.
    * That is, the system must set a register to the location at which the process was loaded (the bottom of the partition).  Actually this is done with a user-mode instruction so could be called execution time, but it is only done once at the very beginning. 
    * This register (often called a base register by ibm) is part of the programmer visible register set. Soon we will meet base/limit registers, which, although related to the IBM base register above, have the important difference of being outside the programmer's control or view.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * Also called **relocation**.
    * In addition, since the linker/assembler allow the use of addresses as data, the loader itself relocates these at load time.
  * Storage keys are used for protection. 
    * An alternative protection method is base/limit registers, which are discussed below.
    * An advantage of the base/limit scheme is that it is easier to move a job.
    * But MFT didn't move jobs so this disadvantage of storage keys is moot.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 3.2 A Memory Abstraction: Address Spaces


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.2.1 The Notion of an Address Space

Just as the process concept creates a kind of abstract CPU to run programs
(each process acts as thought it is the only one running), the address space
creates a kind of abstract memory for programs to live in.

Addresses spaces do for processes, what you so kindly did for modules in the
linker lab: permit each to believe it has its own memory starting at address
zero.

#### Base and Limit Registers

Base and limit registers are additional hardware, invisible to the programmer,
that supports multiprogramming by automatically adding the base address (i.e.,
the value in the base register) to every relative address when that address
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
 is
accessed at run time.

In addition the relative address is compared against the value in the limit
register and if larger, the processes aborted since it has exceeded its memory
bound. Compare this to your error checking in the linker lab.

The base and limit register are set by the OS when the process starts.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.2.2 Swapping

Moving an _entire_ processes between disk and memory is called **swapping**.

#### Multiprogramming with Variable Partitions

Both the **number** and **size** of the partitions change with time.

![MVT](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/MVT.png)

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

  * OS/MVT (multiprogramming with a varying number of tasks).
  * Also early PDP-10 OS.
  * A process still has only one segment (as with MFT). That is, the virtual address space is contiguous.
  * The physical address is also contiguous, that is, the process is stored as one piece in memory.
  * The process can be of any size up to the size of the machine and the process size can change with time.
  * A single ready list.
  * A process can move (might be swapped back in a different place).
  * This is dynamic address translation (i.e., during run time).
  * Must perform an addition on every memory reference (i.e. on every address translation) to add the start address of the partition (the base register).
  * The hardware used was called a DAT (dynamic address translation) box by IBM.
  * OS/MVT eliminates **Internal Fragmentation**, which is defined to be unusable space within a process.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * Find a region the exact right size.
    * Not quite true, can't get a piece with 108755 bytes. Would get say 10880. But internal fragmentation is _much_ reduced compared to MFT. Indeed, we say that internal fragmentation has been eliminated.
  * Introduces **external fragmentation**, i.e., holes _outside_ any region of memory assigned to a process.
  * What do you do _if no hole is big enough_ for the request? 
    * Can compactify 
      * Transition from bar 3 to bar 4 in diagram below.
      * This is expensive.
      * Not suitable for real time (MIT ping pong).
    * Can swap out one process to bring in another, e.g., bars 5-6 and 6-7 in the diagram.
  * There are never two more holes than processes. Why? 
    * Because next to a process there might be a process or a hole but next to a hole there must be a process.
    * So can have runs of processes, but not of holes.
    * If following a process, one is equally likely to have a process or a hole, you get about twice as many processes as holes.
  * Base and limit registers are used. 

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * Storage keys would not good since compactifying or moving would require changing many keys.
    * Storage keys might need a fine granularity to permit the boundaries to move by small amounts (to reduce internal fragmentation). Hence many keys would need to be changed.

**Homework:** 3\. A swapping system eliminates holes by compaction. Assume a random distribution of holes and data segments, assume the data segments are much bigger than the holes, and assume a time to read or write a 32-bit memory word of 4ns. About how long does it take to compact 4 GB? For simplicity, assume that word 0 is part of a hole and the highest word in memory conatains valid data.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.2.3 Managing Free Memory

#### The Placement Question

MVT Introduces the **Placement Question**. That is, into which hole
(partition) should one we place the process when several holes are big enough?

There are several possibilities, including best fit, worst fit, first fit,
circular first fit, quick fit, next fit, and Buddy.

  * First fit chooses the first eligible hole (i.e., the first one big enough).
  * Chooses the smallest eligible hole. Best fit doesn't waste big holes, but does leave slivers and is expensive to run. More expensive to run than first fit since keeps going after finding a hole bigger than the size needed. Tends to leave ``slivers''.
  * Worst fit avoids slivers, but quickly eliminates all big holes so a big job will require compaction.
  * Quick fit keeps lists of some common sizes (but has other problems, see
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
 Tanenbaum).
  * Buddy system 
    * Round request to next highest power of two (causes _internal fragmentation_).
    * Look in list of blocks this size (as with quick fit).
    * If list empty, go higher and split into buddies.
    * When returning coalesce with buddy.
    * Do splitting and coalescing recursively, i.e. keep coalescing until can't and keep splitting until successful.
    * See Tanenbaum (look in the index) or an algorithms book for more details.

A current favorite is circular first fit, also known as next fit.

  * Use the first hole that is big enough (first fit) but start looking where you left off last time.
  * Doesn't waste time constantly trying to use small holes that have failed before, but does tend to use many of the big holes, which can be a problem.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
**Homework:** 4\. Consider a swapping system in which memory consists of the following hole sizes in memory order: 10MB, 4MB, 20MB, 18MB 7MB, 9MB, 12MB, and 15MB. Using first fit, Which hole is taken for successive segment requests of

  1. 12MB
  2. 10MB
  3. 9MB
Now repeat for best fit, worst fit, and next fit.

#### Implementing Free Memory

Buddy comes with its own implementation. How about the others?

##### Memory Management with Bitmaps

Divide memory into blocks and associate a bit with each block, used to
indicate if the corresponding block is free or allocated. To find a chunk of
size N blocks need to find N _consecutive_ bits indicating a free block.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

The only design question is how much memory does one bit represent.

  * Big: Serious internal fragmentation.
  * Small: Many bits to store and process.
  

##### Memory Management with Linked Lists

Instead of a bit map, use a linked list of nodes where each node corresponds
to a region of memory either allocated to a process or still available (a
hole).

  * Each item on list gives the length and starting location of the corresponding region of memory and says whether it is a hole or Process.
  * The items on the list are not taken from the memory to be used by processes.
  * The list is kept in order of starting address.
  * Merge adjacent holes when freeing memory.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Use either a singly or doubly linked list.
  

##### Memory Management using Boundary Tags

See Knuth, _The Art of Computer Programming_ vol 1.

  * Use the same memory for list items as for processes.
  * Don't need an entry in linked list for the blocks in use, just the avail blocks are linked.
  * The avail blocks themselves are linked, not a node that points to an avail block.
  * When a block is returned, we can look at the boundary tag of the adjacent blocks and see if they are avail. If so they must be merged with the returned block.
  * For the blocks currently in use, just need a hole/process bit at each end and the length. Keep this in the block itself.
  * We do not need to traverse the list when returning a block can use boundary tags to find predecessor.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
#### The Replacement Question

MVT also Introduces the **Replacement Question**. That is, which victim should
we swap out when we need to free up some memory?

This is an example of the suspend arc mentioned in process scheduling.

We will study this question more when we discuss demand paging in which case
we swap out only _part_ of a process.

Considerations in choosing a victim

  * Cannot replace a job that is _pinned_, i.e. whose memory is tied down. For example, if _Direct Memory Access_ (DMA) I/O is scheduled for this process, the job is pinned until the DMA is complete.
  * Victim selection for swapping is a medium term scheduling decision 
    * A job that has been blocked for a long time is a good candidate.
    * Often choose as a victim a job that has been in memory for a long tim
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
e.
  * Another question is how long should it stay swapped out.
  * For demand paging, where swaping out a page is not as drastic as swapping out a job, choosing the victim is an important memory management decision and we shall study several policies.

**Notes:**

  1. So far the schemes presented so far have had two properties: 
    1. Each job is stored contiguously in memory. That is, the job is contiguous in _physical_ addresses.
    2. Each job cannot use more memory than exists in the system. That is, the virtual addresses space cannot exceed the physical address space.
  2. Tanenbaum now attacks the second item. I wish to do both and start with the first.
  3. Tanenbaum (and most of the world) uses the term paging to mean what I call demand paging. This is unfortunate as it mixes together two concepts. 
    1. Paging (dicing the address space) to solve the placement problem and essentially eliminate external fragmentation.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    2. Demand fetching, to permit the total memory requirements of all loaded jobs to exceed the size of physical memory.
  4. Most of the world uses the term virtual memory as a synonym for demand paging. Again I consider this unfortunate. 
    1. Demand paging is a fine term and is quite descriptive.
    2. Virtual memory should be used in contrast with physical memory to describe any virtual to physical address translation.

#### ** (Non-Demand) Paging


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![mapping-pages](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/mapping-
pages.png)

Paging is the simplest scheme to remove the requirement of contiguous physical
memory.

  * The program is logically divided into fixed size pieces called **pages**. The partitioning is invisible to the user.
  * This is a division of the virtual address space and is shown in the middle column on the right.
  * Tanenbaum (and others) sometimes calls pages **virtual pages**.
  * The physical memory is similarly divided into fixed size pieces called **page frames.**
  * This is a division of the physical address space and is shown in the third column on the right.
  * Page frames are often called simply **frames**.
  * The size of a page (the page size) equals the size of a frame (the frame size).

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Place each page in a frame. Since all pages and all frames are the same size everything fits perfectly and there is no external fragmentation.
  * Note that the program is **not** contiguous in physical memory. For example, in the diagram page 1 comes **after** page page 2 in physical memory. How can this possible work?
  * We need to find an arbitrary **vritual** address in **physical** memory.
  * The key is that we maintain a table (called the **page table**) having an entry for each page. The **page table entry** or PTE for page p contains the number of the frame f that contains page p.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![paging](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/paging.png)  

##### Virtual to Physical Address Translation

The figure on the right shows how to find in physical memory a given virtual
address.

  1. The address is divided into two part: the page number and the offset. The page number equals the (virtual) address divided by the page size. The offset equals the address mod the page size.
  2. The page number is used to index the page table.
  3. The table entry gives the frame number for this page.
  4. The frame number is combined with the offset to obtain the physical address.

**Example:** Assume a decimal machine with page size = frame size = 1000.  
Assume PTE 3 contains 459.  
Then virtual address 3372 corresponds to physical address 459372.  

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Referring to the diagram p#=3, f#=459, and offset=372.

Properties of (non-demand) paging (without segmentation).

  * The entire process must be memory resident to run.
  * No holes, i.e., no external fragmentation.
  * If there are 500 frames available and the page size is 4KB, then any job requiring ≤ 2MB will fit, even if the available frames are scattered over memory.
  * Hence (non-demand) paging is useful; indeed, it was used (but no longer).
  * Introduces internal fragmentation approximately equal to 1/2 the page size for every process (really every segment).
  * Can have a job unable to run due to insufficient memory and have some (but not enough) memory available. This is _not_ called external fragmentation since it is not due to memory being fragmented.
  * Eliminates the _placement_ question. All pages are equally good since don't have external fragmentation.
  * The _replacement_ question remains.
  * Since page boundaries occur at random points in the program and can cha
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
nge from run to run (the page size can change with no effect on the program--other than performance), pages are not appropriate units of memory to use for protection and sharing. Segmentation, which is discussed later, is sometimes more appropriate for protection and sharing.
  * However, most current OSes do not have segmentation so they use pages for protection and sharing. If all you have is a hammer, everything looks like a nail.
  * Virtual address space remains contiguous.
  

##### Cost of Address Translation in Paging

  * Each memory reference turns into 2 memory references. 
    1. Reference the page table.
    2. Reference central memory.
  * This would be a disaster!
  * But it isn't done that way. Instead,the MMU caches page#->frame# translations. This cache is kept near the processor and can be accessed rapidly.
  * This cache is called a translation lookaside buffer (TLB) or translation buffer (TB).

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * For the above example, after referencing virtual address 3372, there would be an entry in the TLB containing the mapping 3->459.
  * Hence a subsequent access to virtual address 3881 would be translated to physical address 459881 without an extra memory reference. Naturally, a memory reference for location 459881 itself would be required.

Choice of page size is discuss below.

**Homework:** 7\. Using the page table of Fig. 3.9, give the physical address corresponding to each of the following virtual addresses.

  1. 20
  2. 4100
  3. 8300


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 3.3: Virtual Memory (meaning Fetch on Demand)

The idea is to enable a program to execute even if only the active portion of
its address space is memory resident. That is, we are to swap in and swap out
**portions** of a program and can run a program even if some (perhaps most) of
the program is **not** in memory.

In a crude sense this could be called automatic overlays.

Advantages

  * The system can run a program larger than the total physical memory, i.e., the virtual address size of the process can exceed the physical address size of the computer.
  * Even if each program is smaller than physical memory, the sum of the memory of all the running programs can exceed physical memory.
  * Fetch-on-demand likely increase the multiprogramming level since the to
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
tal size of the active, i.e. loaded, programs (running + ready + blocked) can exceed the size of the physical memory.
  * Since some portions of a program are rarely if ever used, it is an inefficient use of memory to have them loaded all the time. Fetch-on-demand will not load them if not used and will (hopefully) unload them in favor of other portions if they are not used for a long time.
  * Simpler for the **user** than overlays or aliasing variables (older techniques to run large programs using limited memory).

Disadvantages

  * More complicated for the OS.
  * Execution time less predictable (depends on other jobs).
  * Can over-commit memory (more later).

#### The Memory Management Unit and Virtual to Physical Address Translation

The memory management unit is a piece of hardware in the processor that,
together with the OS, translates virtual addresses (i.e., the addresses in 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
the
program) into physical addresses (i.e., real hardware addresses in the
memory). The memory management unit is abbreviated as and normally referred to
as the **MMU**.

(The idea of an MMU and virtual to physical address translation applies
equally well to non-demand paging and in olden days the meaning of paging and
virtual memory included that case as well. Sadly, in my opinion, modern usage
of the term paging and virtual memory are limited to fetch-on-demand memory
systems, typically some form of demand paging.)


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![demand-paging](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/demand-
paging.png)


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### ** 3.3.1 Paging (Meaning Demand Paging)

The idea is to fetch pages from disk to memory when they are referenced,hoping
to get the most actively used pages in memory. The choice of page size is
discussed below.

Demand paging is very common: More complicated variants, multilevel-level
paging and paging plus segmentation (both of which we will discuss), have been
used and the former dominates modern operating systems.

Started by the Atlas system at Manchester University in the 60s (paper by
Fortheringham).

Each PTE continues to contain the frame number if the page is loaded. But what
if the page is not loaded (i.e., the page exists only on disk)?

The PTE has a flag indicating if the page is loaded (can think of the X in 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
the
diagram on the right as indicating that this flag is not set). If the page is
not loaded, the location on disk could be kept in the PTE, but normally it is
not (discussed below).

When a reference is made to a non-loaded page (sometimes called a non-existent
page, but that is a bad name), the system has a lot of work to do. (We give
more details below.)

  1. Choose a free frame, if one exists.
  2. What if there is no free frame?  
Make one!

    1. Choose a victim frame. This is the **replacement** question about which we will have **much** more to say latter.
    2. Write the victim back to disk if it is dirty.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    3. Update the victim PTE to show that it is not loaded.
    4. Now we have a free frame.
  3. Copy the referenced page from disk to the free frame.
  4. Update the PTE of the referenced page to show that it is loaded and give the frame number.
  5. Do the standard paging address translation (p#,off)->(f#,off).

Really not done quite this way as we shall see later

**Homework:** 14\. A machine has a 32-bit address space and an 8-KB page. The page table is entirely in hardware, with one 32-bit word per entry. When a process starts, the page table is copied to the hardware from memory, at one word every 100 nsec. If each process runs for 100 msec (including the time to load the page table), what fraction of the CPU time is devoted to loading the page tables?


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.3.2 Page tables

A discussion of page tables is also appropriate for (non-demand) paging, but
the issues are more important with demand paging for at least two reasons.

  1. The total size of the active processes is no longer limited to the size of physical memory. Since the total size of the processes is greater, the total size of the page tables is greater and hence concerns over the size of the page table are more acute.
  2. With demand paging an important question is the choice of a victim page to page out. Data in the page table can be useful in this choice.

We must be able access to the page table very quickly since it is needed for
every memory access.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![big-page-table](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/big-page-
table.png)

Unfortunate laws of hardware.

  * Big and fast are essentially incompatible.
  * Big and fast and low cost is hopeless.

So we can't just say, put the page table in fast processor registers, and let
it be huge, and sell the system for $1000.

The simplest solution is to put the page table in main memory as shown on the
right. However this solution seems to be both too slow and too big.

  * The solution seems too slow since all memory references now require two reference.
  * We will soon see how to largely eliminate the extra reference by using 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
a TLB.
  * The solution seems too big. 
    * Currently we are considering _contiguous_ virtual addresses ranges (i.e., the virtual addresses have no holes).
    * One often puts the stack at one end of the virtual address space and the global (or static) data at the other end and let them grow towards each other.
    * The **virtual** memory in between is unused and can be enormous.
    * That does **not** sound so bad. Why should we care about virtual memory?
    * Since unused virtual memory can be huge (in address range), the page table, which is stored in **real** memory, will mostly contain unneeded PTEs.
    * This scheme worked fine when the maximum virtual address size was comparable in size with the total physical address space (e.g., the PDP-11 of the 1970s) but that is no longer the case.
  * A fix is to use multiple levels of mapping. We will see two examples below: multilevel page tables and segmentation plus paging.

#### Structure of a Page Table Entry

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

Each page has a corresponding page table entry (PTE). The information in a PTE
is used by the hardware and its format is machine dependent; thus the OS
routines that access PTEs are not portable. Information set by and used by the
OS is normally kept in other OS tables.

(Actually some systems, those with software TLB reload, do not require
hardware access to the page table.)

The page table is indexed by the page number; thus the page number is **not**
stored in the table.

The following fields are often present in a PTE.

  1. The _Frame Number_. This field is the main reason for the table. It gives the virtual to physical address translation. It is the only field in the page table for (non-demand) paging.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  2. The _Valid_ bit. This tells if the page is currently loaded (i.e., is in a frame). If the bit is set, the page is in memory and the frame number in the PTE is is valid It is also called the _presence_ or _presence/absence_ bit. If a page is accessed whose valid bit is unset, a _page fault_ is generated by the hardware.
  3. The _Modified_ or _Dirty_ bit. Indicates that some part of the page has been written since it was loaded. This is needed when the page is evicted so that the OS can tell if the page must be written back to disk.
  4. The _Referenced_ or _Used_ bit. Indicates that some word in the page has been referenced. Used to select a victim: unreferenced pages make good victims by the locality property (discussed below).
  5. _Protection_ bits. For example one can mark text pages as execute only. This requires that boundaries between regions with different protection are on page boundaries. Normally many consecutive (in logical address) pages have the same protection so many page protection bits are redundant. Protection is more naturally done with segmentation, but in many current systems, it is done with paging since the systems don't utilize segmentation (even though the hardware supports it).

**Question**: Why not store the disk addresses of non-resident pages in the
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
 PTE?  
**Answer**: On most systems the PTEs are accessed by the hardware automatically on a TLB miss (see immediately below). Thus the format of the PTEs is determined by the hardware and contains only information used on page hits. Hence the disk address, which is only used on page faults, is not present.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.3.3 Speeding Up Paging

As mentioned above, the simple scheme of storing the page table in its
entirety in central memory alone appears to be both too slow and too big. We
address both these issues here, but note that a second solution (segmentation)
to the size question is discussed later.

####  Translation Lookaside Buffers (and General Associative Memory)

**Note:** Tanenbaum suggests that associative memory and translation lookaside buffer are synonyms. This is wrong. Associative memory is a general concept of which translation lookaside buffer is a specific example.

An **associative memory** is a _content addressable memory_. That is you
access the memory by giving the _value_ of some field (called the index) and
the hardware searches all the records and returns the record whose index field

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
contains the requested value.

For example

    
    
      Name  | Animal | Mood     | Color
      ======+========+==========+======
      Moris | Cat    | Finicky  | Grey
      Fido  | Dog    | Friendly | Black
      Izzy  | Iguana | Quiet    | Brown
      Bud   | Frog   | Smashed  | Green
    

If the index field is Animal and Iguana is given, the associative memory
returns

    
    
      Izzy  | Iguana | Quiet    | Brown

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    

A **Translation Lookaside Buffer** or **TLB** is an associate memory where the
index field is the page number. The other fields include the frame number,
dirty bit, valid bit, etc.

Note that, unlike the situation with a the page table, the page number **is**
stored in the TLB; indeed it is the index field.

A TLB is _small and expensive_ but at least it is _fast_. When the page number
is in the TLB, the frame number is returned very quickly.

On a miss, a _TLB reload_ is performed. The page number is looked up in the
page table. The record found is placed in the TLB and a victim is discarded
(not really discarded, dirty and referenced bits are copied back to the PTE
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
).
There is no placement question since all TLB entries are accessed at once and
hence are equally suitable. But there is a replacement question.

**Homework:** 22\. A computer whose processes have 1024 pages in their address spaces keeps its page tables in memory. The overhead required for reading a word from the page table is 5 nsec. To reduce this overhead, the computer has a TLB, which holds 32 (virtual page, physical page frame) pairs, and can do a look up in 1 nsec. What hit rate is needed to reduce the mean overhead to 2 nsec?

As the size of the TLB has grown, some processors have switched from single-
level, fully-associative, unified TLBs to multi-level, set-associative,
separate instruction and data, TLBs.

We are actually discussing caching, but using different terminology.

  * Page frames are a cache for pages (one could say that central memory is
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
 a cache of the disk).
  * The TLB is a cache of the page table.
  * Also the processor almost surely has a cache (most likely several) of central memory.
  * In all the cases, we have small-and-fast acting as a cache of big-and-slow. However what is big-and-slow in one level of caching, can be small-and-fast in another level.

#### Software TLB Management

The words above assume that, on a TLB miss, the MMU (i.e., hardware and not
the OS) loads the TLB with the needed PTE and then performs the virtual to
physical address translation.

Some newer systems do this in software, i.e., the OS **is** involved.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![2-level-page-tables](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/2
-level-page-table.png)

#### Multilevel Page Tables

Recall the diagram above showing the data and stack growing towards each
other. Most of the virtual memory is the unused space between the data and
stack regions. However, with demand paging this space does _not_ waste real
memory. But the single large page table **does** waste real memory.

The idea of multi-level page tables (a similar idea is used in Unix i-node-
based file systems, which we study later when we do I/O) is to add a level of
indirection and have a page table containing pointers to page tables.

  * Imagine one big page table, which we will (eventually) call the second level page table.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * We want to apply paging to this large table, viewing it as simply memory not as a page table. So we (logically) cut it into pieces each the size of a page. Note that many (typically 1024 or 2048) PTEs fit in one page so there are far fewer of these pages than PTEs.
  * Now construct a _first level page table_ containing PTEs that point to the pages produced in the previous bullet.
  * This first level PT is small enough to store in memory. It contains one PTE for every page of PTEs in the 2nd level PT, which reduces space by a factor of one or two thousand.
  * But since we still have the 2nd level PT, we have made the world bigger not smaller!
  * Don't store in memory those 2nd level page tables all of whose PTEs refer to unused memory. That is _use demand paging on the (second level) page table_!

This idea can be extended to three or more levels. The largest I know of has
four levels. We will be content with two levels.

#### Address Translation With a 2-Level Page Table

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

For a two level page table the virtual address is divided into three pieces

    
    
      +-----+-----+-------+
      | P#1 | P#2 | Offset|
      +-----+-----+-------+
    

  * P#1 gives the index into the first level page table.
  * Follow the pointer in the corresponding PTE to reach the frame containing the relevant 2nd level page table.
  * P#2 gives the index into this 2nd level page table.
  * Follow the pointer in the corresponding PTE to reach the frame containing the (originally) requested page.
  * Offset gives the offset in this frame where the originally requested word is located.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Do an example on the board.

The VAX used a 2-level page table structure, but with some wrinkles (see
Tanenbaum for details).

Naturally, there is no need to stop at 2 levels. In fact the SPARC has 3
levels and the Motorola 68030 has 4 (and the number of bits of Virtual Address
used for P#1, P#2, P#3, and P#4 can be varied). More recently, x86-64 also has
4-levels.

#### Inverted Page Tables

For many systems the virtual address range is much bigger that the size of
physical memory. In particular, with 64-bit addresses, the range is 264 bytes,
which is 16 million terabytes. If the page size is 4KB and a PTE is 4 bytes, a
full page table would be 16 thousand terabytes.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

A two level table would still need 16 terabytes for the first level table,
which is stored in memory. A three level table reduces this to 16 gigabytes,
which is still large and only a 4-level table gives a reasonable memory
footprint of 16 megabytes.

An alternative is to instead keep a table indexed by frame number. The content
of entry f contains the number of the page currently loaded in frame f. This
is often called a **frame table** as well as an **inverted page table**.

Now there is one entry per **frame**. Again using 4KB pages and 4 byte PTEs,
we see that the table would be a constant 0.1% of the size of real memory.

But on a TLB miss, the system must **search** the inverted page table, which
would be hopelessly slow except that some tricks are employed. Specifically
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
,
hashing is used.

Also it is often convenient to have an inverted table as we will see when we
study global page replacement algorithms. Some systems keep both page and
inverted page tables.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 3.4 Page Replacement Algorithms (PRAs)

These are solutions to the _replacement_ question. Good solutions take
advantage of _locality_ when choosing the victim page to replace.

  1. **Temporal locality**: If a word is referenced now, it is _likely_ to be referenced in the near future.  
This argues for _caching_ referenced words, i.e. keeping the referenced word
near the processor for a while.

  2. **Spatial locality**: If a word is referenced now, nearby words are _likely_ to be referenced in the near future.  
This argues for _prefetching_ words around the currently referenced word.

  3. Temporal and spacial locality are lumped together into **locality**: If any word in a page is referenced, each word in the page is likely to be referenced. So it is good to bring in the entire page on a miss and to keep the page in memory for a while.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
When programs begin there is no history so nothing to base locality on. At
this point the paging system is said to be undergoing a cold start.

Programs exhibit phase changes in which the set of pages referenced changes
abruptly (similar to a cold start). An example would occurs in your linker lab
when you finish pass 1 and start pass 2. At the point of a phase change, many
page faults occur because locality is poor.

Pages belonging to processes that have terminated are of course perfect
choices for victims.

Pages belonging to processes that have been blocked for a long time are good
choices as well.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### Random PRA

A lower bound on performance. Any decent scheme should do better.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.1 The Optimal Page Replacement Algorithm

Replace the page whose **next** reference will be furthest in the future.

  * Also called Belady's min algorithm.
  * Provably optimal. That is, no algorithm generates fewer page faults.
  * Unimplementable: Requires predicting the future.
  * Good upper bound on performance.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.2 The Not Recently Used (NRU) PRA

Divide the frames into four classes and make a random selection from the
lowest nonempty class.

  0. Not referenced, not modified.
  1. Not referenced, modified.
  2. Referenced, not modified.
  3. Referenced, modified.

Assumes that in each PTE there are two extra flags R (for referenced;
sometimes called U, for used) and M (for modified, often called D, for dirty).

NRU is based on the belief that a page in a lower priority class is a better
victim, i.e., is less important.

  * If a page is not referenced, locality suggests that it probably will not referenced again soon and hence is a good candidate for eviction.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * If a clean page (i.e., one that is not modified) is chosen to evict, the OS does not have to write it back to disk and hence the cost of the eviction is lower than for a dirty page.

Implementation

  * When a page is brought in, the OS resets R and M (i.e. R=M=0).
  * On a read, the hardware sets R.
  * On a write, the hardware sets R and M.

Old cartoons often had prisoners wearing broad horizontal stripes and using
sledge hammers to break up rocks.

This gives what I sometimes call the prisoner problem: If you do a good job of
making little ones out of big ones, but a poor job job of the reverse, you
soon wind up with all little ones.

In this case we do a great job setting R but rarely reset it. We need more

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
resets. Therefore, every k clock ticks, the OS resets all R bits.

**Question**: Why not reset M as well?  
**Answer**: If a dirty page has a clear M, we will not copy the page back to disk when it is evicted, and thus the only accurate version of the page will be lost!

What if the hardware doesn't set these bits?  
Answer: The OS can uses tricks. When the bits are reset, the PTE is made to
indicate that the page is not resident (which is a lie). On the ensuing page
fault, the OS sets the appropriate bit(s).

So now the R and M bits tell us the NRU class

  0. If R=M=0, the page has not been referenced (recently) and is not modified (clean).
  1. If R=0 and M=1, the page has not been referenced and has been modified (dirty).

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  2. If R=1 and M=0, the page has been referenced and is clean.
  3. If R=1 and M=1, the page has been referenced and is dirty.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.3 FIFO PRA

Simple algorithm. Basically, we try to be fair to the pages: the first one
loaded is the first one evicted.

The natural implementation is to have a queue of nodes each referring to a
resident page (i.e., pointing to a frame).

  * When a page is loaded, a node referring to the page is appended to the tail of the queue.
  * When a page needs to be evicted, the head node is removed and the page referenced is chosen as the victim.

This sound reasonable at first, but it is not a good policy. The trouble is
that a page referenced say every other memory reference and thus **very**
likely to be referenced soon will be evicted because we only look at the
**first** reference to a page, when we should be particularly interested in
**recent** references to the page.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.4 Second chance PRA

Similar to the FIFO PRA, but altered so that a page recently referenced is
given a second chance.

  * When a page is loaded, a node referring to the page is appended to the tail of list queue. The list is maintained in apparent order of loading (i.e. apparently, but not really, FIFO) The R bit of the page is cleared.
  * When a page needs to be evicted, the head node is removed and the page referenced is the **potential** victim.
  * If the R bit is unset (the page hasn't been referenced recently), then the page **is** the victim.
  * If the R bit is set, the page is given a second chance. Specifically, the R bit is cleared, the time of loading is changed to NOW, and the node referring to this page is appended to the rear of the queue (so it appears to have just been loaded), and the current head node becomes the potential victim.
  * What if all the R bits are set?
  * We will move each page from the front to the rear and will arrive at the initial condition but with all the R bits now clear. Hence we will remove
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
 the same page as fifo would have removed, but will have spent more time doing so.
  * As in NRU we periodically clear all the R bits.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.5 Clock PRA

Same algorithm as 2nd chance, but uses a better implementation, namely a
circular list with a single pointer serving as both head and tail pointer.

We assume that the most common operation is to choose a victim and replace it
by a new page.

  * We use a circular list for the nodes and have a pointer pointing to the head entry. Think of the list as the hours on a clock and the pointer as the hour hand. (Hence the name clock PRA.)
  * The operation we need to support efficiently is replace the oldest, unreferenced page by a given new page.
  * Examine the node pointed to by the clock hand. If the R bit of the corresponding page is set, we give the page a second chance: clear the R bit and set the time to NOW (so the page looks freshly loaded), advance the hand, and examine the next node.
  * Eventually we will reach a node whose R bit is clear. The corresponding page is the victim.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Replace the victim with the new page (may involve 2 I/Os as always).
  * Update the node to refer to this new page.
  * Move the hand forward another hour so that the new page is at the rear.

#### LIFO PRA

This is terrible! Why?  
Ans: All but the last frame are frozen once loaded so you can replace only one
frame. This is especially bad after a phase shift in the program as now the
program is references mostly new pages but only one frame is available to hold
them.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.6 Least Recently Used (LRU) PRA

When a page fault occurs, choose as victim that page that has been unused for
the longest time, i.e. the one that has been least recently used.

LRU is definitely

  1. Implementable: The _past_ is knowable.
  2. Good: Simulation studies have shown this.
  3. Difficult. Essentially the system needs to either: 
    * Keep a time stamp in each PTE, updated **on each reference** and scan all the PTEs when choosing a victim to find the PTE with the oldest timestamp.
    * Keep the PTEs in a linked list in usage order, which means **on each reference** moving the corresponding PTE to the end of the list.

**Homework:** 28\. If FIFO page replacement is used with four page frames and eight pages, how many page faults will occur with the reference string 0172327103 if the four frames are initially empty? Now repeat this problem f
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
or LRU.

**Homework:** 36\. A computer has four page frames. The time of loading, time of last access, and the R and M bits for each page are shown below (the times in clock ticks).

PageLoadedLast ref.RM

0

126

280

1

0

1


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
230

265

0

1

2

140

270

0

0

3


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
110

285

1

1

  1. Which page will NRU replace?
  2. Which page will FIFO replace?
  3. Which page will LRU replace?
  4. Which page will second chance replace?

#### A Hardware Cutesy in Tanenbaum

A clever hardware method to determine the LRU page.

  * For n pages, keep an nxn bit matrix.
  * On a reference to page i, set row i to all 1s and column i to all 0s.
  * At any time the 1 bits in the rows are ordered by inclusion. I.e. one r
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ow's 1s are a subset of another row's 1s, which is a subset of a third. (Tanenbaum forgets to mention this.)
  * So the row with the fewest 1s is a subset of all the others and is hence least recently used.
  * This row also has the smallest value, when treated as an unsigned binary number. So the hardware can do a comparison of the rows rather than counting the number of 1 bits.
  * Cute, but still impractical.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.7 Simulating (Approximating) LRU in Software

#### The Not Frequently Used (NFU) PRA

Keep a count of how frequently each page is used and evict the one that has
been the lowest score. Specifically:

  * Include a counter (and reference bit R) in each PTE.
  * Set the counter to zero when the page is brought into memory.
  * Every k clocks, perform the following for each PTE. 
    1. Add R to the counter.
    2. Clear R.
  * Choose as victim the PTE with lowest count.

Rcounter

1

10000000

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

0

01000000

1

10100000

1

11010000

0

01101000

0

00110100

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

1

10011010

1

11001101

0

01100110

#### The Aging PRA

NFU doesn't distinguish between old references and recent ones. The following
modification does distinguish.

  * Include a counter (and reference bit, R) in each PTE.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Set the counter to zero when the page is brought into memory.
  * Every k clock ticks, perform the following for each PTE. 
    1. Shift the counter right one bit.
    2. Insert R as the new high order bit of the counter.
    3. Clear R.
  * Choose as victim the PTE with lowest count.

Aging does indeed give more weight to later references, but an n bit counter
maintains data for only n time intervals; whereas NFU maintains data for at
least 2n intervals.

**Homework:** 30\. A small computer on a smart card has four page frames. At the first clock tick, the R bits are 0111 (page 0 is 0, the rest are 1). At subsequent clock ticks, the values are 1011, 1010, 1101, 0010, 1010, 1100 and 0001. If the aging algorithm is used with an 8-bit counter, give the values of the four counters after the last tick.

**Homework:** 42\. It has been observed that the number of instructions exe
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
cuted between page faults is directly proportional to the number of page frames allocated to a program. If the available memory is doubled, the mean interval between page faults is also doubled. Suppose that a normal instruction takes 1 us, but if a page fault occurs, it takes 2001 us. If a program takes 60 sec to run, during which time it gets 15,000 page faults, how long would it take to run if twice as much memory were available?


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.8 The Working Set Page Replacement Algorithm (Peter Denning)

#### The Working Set Policy

The goals are first to specify which pages a given process needs to have
memory resident in order for the process to run without too many page faults
and second to ensure that these pages are indeed resident.

But this is impossible since it requires predicting the future. So we again
make the assumption that the near future is well approximated by the immediate
past.

We measure time in units of memory references, so t=1045 means the time when
the 1045th memory reference is issued. In fact we measure time separately for
each process, so t=1045 really means the time when _this_ process made its

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
1045th memory reference.

**Definition**: **w(k,t)**, the **working set** at time t (with window k) is the set of pages referenced by the last k memory references ending at reference t.

The idea of the working set policy is to ensure that each process keeps its
working set in memory.

  * Allocate |w(t,k)| frames to each process. This number differs for each process and changes with time.
  * On a fault, evict a page not in the working set.
  * If a process is suspended and swapped out; the working set then can be used to say which pages should be brought back when the process is resumed.

Unfortunately, determining w(t,k) precisely is quite time consuming. It is
never done in real systems. Instead approximations are used as we shall see

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

**Homework:** Describe a process (i.e., a program) that runs for a long time (say hours) and always has a working set size less than 10. Assume k=100,000 and the page size is 4KB. The program need not be practical or useful.

**Homework:** Describe a process that runs for a long time and (except for the very beginning of execution) always has a working set size greater than 1000. Again assume k=100,000 and the page size is 4KB. The program need not be practical or useful.

The definition of Working Set is local to a process. That is, each process has
a working set; there is no system wide working set other than the union of all
the working sets of each process.

However, the working set of a single process has effects on the demand paging
behavior and victim selection of other processes. If a process's working set

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
is growing in size, i.e., |w(t,k)| is increasing as t increases, then we need
to obtain new frames from other processes. A process with a working set
decreasing in size is a source of free frames. We will see below that this is
an interesting amalgam of local and global replacement policies.

Interesting questions concerning the working set include:

  1. What value should be used for k?  
Experiments have been done and k is surprisingly robust (i.e., for a given
system, a fixed value works reasonably for a wide variety of job mixes).

  2. How should we calculate w(t,k)?  
Hard to do exactly so ...

... Various approximations to the working set, have been devised. We will
study two: Using virtual time instead of memory references (immediately
below), and Page Fault Frequency (part of section 3.5.1). In 3.4.9 we will see

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
the popular WSClock algorithm that includes an approximation of the working
set as well as several other ideas.

#### Using Virtual Time

Instead of counting memory references and declaring a page in the working set
if it was used within k references, we declare a page in the working set if it
was used in the past τ seconds. This is easier to do since the system already
keeps track of time for scheduling (and perhaps accounting). Note that the
time is measured only while this process is running, i.e., we are using
virtual time.

#### A Possible Working-Set Algorithm

What follows is a possible working-set algorithm using virtual time.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Use reference and modify bits R and M as described above. As usual, the OS
clears both bits when a page is loaded and clears R every m milliseconds. The
hardware sets M on writes and sets R on every access.

Add a field time of last use to the PTE. The procedure for setting this field
is below.

If the reference bit is 1, the page has been referenced within the last m
milliseconds, which we assume is significant shorter than τ seconds. Hence a
page with R=1 is in the working set.

To choose a victim when a page fault occurs (also setting the time of last use
field) we scan the page table and treat each resident page as follows. Since
we are interested only in resident pages, we would rather scan a page frame

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
table.

  * If the R bit is 1, the page is in the working set as we said above and hence is not evicted. We set its the time of last use to the current (virtual) time. This approximation can be wrong by at most m milliseconds.
  * If the R bit is 0, but the stored time of last use is less than τ seconds ago, the page is again in the working set so is not evicted.
  * If the R bit is 0 and the stored time of last use is more than τ seconds ago, the page is not in the working set and is evicted (but we keep scanning).
  * If no page was chosen for eviction, evict the page with R=0 that has the earliest time of last use.
  * If all pages have R=1, use some other method (say a random, preferably clean, page).


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.9 The WSClock Page Replacement Algorithm

The WSClock algorithm combines aspects of the working set algorithm (with
virtual time) and the clock implementation of second chance. It also
distinguishes clean from dirty and referenced from non-referenced in the
spirit of NRU.

As in clock we create a circular list of nodes with a hand pointing to the
next node to examine. There is one such node for every resident page of this
process; thus the nodes can be thought of as a list of frames or a kind of
inverted page table.

As in working set we store in each node the referenced and modified bits R and
M and the time of last use. R and M ar treated as above

  * R and M are cleared when the page is read in.
  * R is set by the hardware on a reference and cleared periodically by the OS (say every m milliseconds).

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * M is set by the hardware on a write.

We discuss below the setting of the time of last use and the clearing of M.

We use virtual time and declare a page old if its last reference is more than
τ seconds in the past. We again assume τ seconds is much longer than m
milliseconds. Other pages are declared young (i.e., in the working set).

As with clock, on every page fault a victim is found by scanning the list of
resident pages starting with the page indicated by the clock hand.

  * If R=1, the page has been recently referenced. As in second chance, R is set to 0, the time of last use is set to _now_, and the hand advances.
  * If R=M=0 and the page is old, this is our victim. Since the page is clean, we can start writing the new page immediately. Update the PTE and advance the hand. The algorithm has completed.
  * If R=M=0 and the page is young, the page is in the working set so is no
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
t evicted. Advance the hand and continue.
  * If R=0 and M=1, we have a dirty page that is not recently referenced (it might still be young, i.e., in the working set). We clear M, schedule an I/O to write the dirty page, and advance the hand.

It is possible to go all around the clock without finding a victim. In that
case

  * If writes were scheduled on old pages, one of these will become the victim once it becomes clean.
  * If no writes were scheduled, we will never find a victim (since nothing will change) so pick a page at random (or perhaps the oldest of the young pages).
  * If only young pages are scheduled for writing, again no victim will be chosen, and we pick a page at random (if every page is dirty, we must wait for one to become clean).

An alternative treatment of WSClock, including more details of its interaction

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
with the I/O subsystem, can be found [here](wsclock-davis.html).


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.10 Summary of Page Replacement Algorithms

  

AlgorithmComment

Random

Poor, used for comparison

Optimal

Unimplementable, used for comparison

NRU

Crude

FIFO


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Not good ignores frequency of use

Second Chance

Improvement over FIFO

Clock

Better implementation of Second Chance

LIFO

Horrible, useless

LRU

Great but impractical

NFU


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Crude LRU approximation

Aging

Better LRU approximation

Working Set

Good, but expensive

WSClock

Good approximation to working set


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.4.A Belady's Anomaly

Consider a system that has no pages loaded and that uses the FIFO PRU.  
Consider the following reference string (sequence of pages referenced by a
given process).

    
    
      0 1 2 3 0 1 4 0 1 2 3 4
    

What happens if we run the process on a tiny machine with only 3 frames? What
if we run it on a bigger (but still tiny) machine with 4 frames?

  * If we have 3 frames this generates 9 page faults (do it).
  * If we have 4 frames this generates 10 page faults (do it).
  * That's crazy! A bigger machine has **more** faults?!
  * That's why its called an anomaly!


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Theory has been developed and certain PRA (so called stack algorithms) cannot
suffer this anomaly for any reference string. FIFO is clearly not a stack
algorithm. LRU is.

Repeat the above calculations for LRU.

**Note**: A former OS student, Alec Jacobson, has extended the above to a repeating string so that, if _N_ cycles of the repeating pattern are included, a FIFO replacement policy with 4 frames has _N_ more faults than one with only 3 frames. His blog entry is [here](http://www.alecjacobson.com/weblog/?p=746) and a local (de-blogged) copy is [here.](belady-jacobson.html)


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 3.5 Design Issues for (Demand) Paging Systems


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
###  3.5.1 Local vs Global Allocation Policies

A **local** PRA is one is which a victim page is chosen among the pages of the
same process that requires a new frame. That is the number of frames for each
process is fixed. So LRU for a local policy means that, on a page fault, we
evict the page least recently used by **this** process. A **global** policy is
one in which the choice of victim is made among all pages of all processes.

**Question**: Why is a purely local policy impractical/impossible.  
**Answer:** A new process has zero frames. With a purely local policy, the new process would never get a frame. More realistically, if you arranged for the first fault to be satisfied before restricting to purely local, a new process would remain with only one frame.

A more reasonable local policy would be to wait until a process has been

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
running a while before restricting it to existing frames or give the process
an initial allocation of frames based on the size of the executable.

In general, a global policy seems to work better. For example, consider LRU.
With a local policy, the local LRU page might have been **more** recently used
than many resident pages of other processes. A global policy needs to be
coupled with a good method to decide how many frames to give to each process.
By the working set principle, each process should be given |w(k,t)| frames at
time t, but this value is hard to calculate exactly.

If a process is given too few frames (i.e., well below |w(k,t)|), its faulting
rate will rise dramatically. If this occurs for many or all the processes, the
resulting situation in which the system is doing very little useful work du
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
e
to the high I/O requirements for all the page faults is called **thrashing**.

#### Page Fault Frequency (PFF)

An approximation to the working set policy that is useful for determining how
many frames a process needs (but not which pages) is the **Page Fault
Frequency** algorithm.

  * For each process keep track of the page fault frequency, which is the number of faults divided by the number of references.
  * Actually, must use a window or a weighted calculation since you are interested in the recent page fault frequency.
  * Actually, it is too expensive to calculate the number of references so, as above, we approximate this by the amount of (virtual) time.
  * If the PFF is exceptionally low, free some of this processes frames (e.g., limit victim selection to this process for a while).
  * If the PFF is too high, allocate more frames to this process. Either 

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    1. Raise its number of frames if using a local policy; or
    2. Bar its frames from eviction (for a while) if using a global policy.

**Question**: What if there are not enough frames in the entire system? That is, what if the PFF is too high for all processes?  
**Answer**: Reduce the MPL as we now discuss.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.5.2: Load Control

![process-states](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/process-
states.png)

To reduce the overall _memory pressure_, we must reduce the multiprogramming
level (or install more memory while the system is running, which is not
possible with current technology).

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

This gives a connection between memory management and process management.
These are the suspend/resume arcs we saw way back when and are shown again in
the diagram on the right.

When the PFF (or another indicator) is too high, we choose a process and
_suspend_ it, thereby swapping it to disk and releasing all its frames. When
the frequency gets low, we can _resume_ one or more suspended processes. We
also need a policy to decide when a suspended process should be resumed even
at the cost of suspending another.

This is called _medium-term scheduling_. Since suspending or resuming a
process can take seconds, we clearly do not perform this scheduling decision
every few milliseconds as we do for short-term scheduling. A time scale of
minutes would be more appropriate.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.5.3: Page Size

**Question**: Why must the Page size be a multiple of the disk block size.  
**Answer**: When copying out a page if you have a partial disk block, you must do a read/modify/write (i.e., 2 I/Os).

Characteristics of a large page size.

  * Good for demand paging I/O: 
    * We will learn later this term that the total time for performing 8 I/O operations each of size 1KB is much larger that the time for a single 8KB I/O. Hence it is better to swap in/out one big page than several small pages.
    * But if the page is too big you will be swapping in data that are not local and hence might well not be used.
  * Large internal fragmentation (1/2 page size).
  * Small page table (process size / page size * size of PTE).
  * These last two can be analyzed together by setting the derivative of the sum equal to 0. The minimum overhead occurs at a page size of 

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    
              sqrt(2 * process size * size of PTE)
        

Since the term inside the sqrt is typically millions of byte2, we see that
modern practice of having the page size a few kilobytes is near the minimum
point.

  * A very large page size leads to very few pages. A process will have many faults if it references more regions than the number of (large) frames that the process has been allocated.

A small page size has the opposite characteristics.

**Homework:** Consider a 32-bit address machine using paging with 8KB pages and 4 byte PTEs. How many bits are used for the offset and what is the size of the largest page table? Repeat the question for 128KB pages.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.5.4: Separate Instruction and Data (I and D) Spaces

This was used when machine have very small virtual address spaces.
Specifically the PDP-11, with 16-bit addresses, could address only 216 bytes
or 64KB, a severe limitation. With separate I and D spaces there could be 64KB
of instructions and 64KB of data.

Separate I and D are no longer needed with modern architectures having large
address spaces.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.5.5 Shared pages

Permit several processes to each have the same page loaded in the same frame.
Of course this can only be done if the processes are using the same program
and/or data.

  * Really should share segments.
  * Must keep reference counts or something so that, when a process terminates, pages it shares with another process are not automatically discarded.
  * Similarly, a reference count would make a widely shared page (correctly) look like a poor choice for a victim.
  * A good place to store the reference count would be in a structure pointed to by both PTEs. If stored in the PTEs themselves, we must somehow keep the count consistent among processes.
  * If you want the pages to be initially shared for reading but want each process's updates to be private, then use so called copy on write techniques.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
**Homework:** Can a page shared between two processes be read-only for one process and read-write for the other? 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.5.6 Shared Libraries (Dynamic-Linking)

In addition to sharing individual pages, process can share entire library
routines. The technique used is called **dynamic linking** and the objects
produced are called **shared libraries** or **dynamically-linked libraries
(DLLs)**. (The traditional linking you did in lab1 is today often called
**static linking**).

  * With dynamic linking, frequently used routines are not linked into the program. Instead, just a stub is linked.
  * When the routine is called (or when the process begins), the stub checks to see if the real routine has been loaded by _another_ program). 
    * If it has not been loaded, load it (really page it in as needed).
    * If it is already loaded, _share_ it. The read-write data must be shared copy-on-write.
  * _Advantages_ of dynamic linking. 
    * Saves RAM: Only one copy of a routing is in memory even when it is used concurrently by many processes. For example even a big server with hundreds of active processes will have only one copy of printf in memory. (In fact with demand paging only part of the routine will be in memory.)

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * Saves disk space: Files containing executable programs no longer contain copies of the shared libraries.
    * A bug fix to a dynamically linked library fixes all applications that use that library, **without** having to relink these applications.
  * _Disadvantages_ of dynamic linking. 
    * New bugs in dynamically linked library infect all applications.
    * Applications change even when they haven't changed.
  * A _Technical Difficulty_ with dynamic linking. The shared library has different _virtual_ addresses in each process so addresses relative to the beginning of the module cannot be used (they would need to be relocated to different addresses in the multiple copies of the module). Instead _position-independent code_ must be used. For example, jumps within the module would use PC-relative addresses. 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.5.7 Mapped Files

The idea of **memory-mapped files** is to use the mechanisms in place for
demand paging (and segmentation, if present) to implement I/O.

A system call is used to map a file into a portion of the address space. (No
page can be part of a file and part of regular memory; the mapped file would
be a complete segment if segmentation is present).

The implementation of demand paging we have presented assumes that the entire
process is stored on disk. This portion of secondary storage is called the
backing store for the pages. Sometimes it is called a paging disk. For memory-
mapped files, the file itself is the backing store.

Once the file is mapped into memory, reads and writes become loads and stores.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.5.8 Cleaning Policy (Paging Daemons)

In practice there is rarely no free frame to because many systems use a
_paging daemon_, a process that, whenever active, evicts pages to increase the
number of free frames. The daemon is activated when the number of free pages
falls below a low water mark and suspended when the number rises above a high
water mark.

Some replacement algorithm must be chosen, and naturally dirty pages must be
written back to disk prior to eviction.

Since we have studied replacement algorithms, we can suggest an implementation
of the daemon. If a clock-like algorithm is used for victim selection, one can
have a two handed clock with one hand (the paging daemon) staying ahead of 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
the
other (the one invoked by the need for a free frame).

The front hand simply writes out any page it hits that is dirty and thus the
trailing hand is likely to see clean pages and hence is more quickly able to
find a suitable victim.

Note that our WSClock implementation had a page cleaner built in (look at the
implementation when R=0 and M=1).

_Unless specifically requested_, you may ignore paging daemons when answering
exam questions.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.5.9 Virtual Memory Interface

Skipped.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 3.6 Implementation Issues


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.6.1 Operating System Involvement with Paging

When must the operating system be involved with demand paging?

  * During process creation. The OS must allocate a page table and a region on disk to hold the pages that are not memory resident. A few pages of the process must be loaded.
  * The Ready->Running transition. Real memory must be allocated for the page table if the table has been swapped out (which is permitted when the process is not running).  
Some hardware register(s) must be set to point to the page table. There can be
many page tables resident (since there are many ready processes), but the
hardware must be told the location of the page table for the running process--
the active page table.  
The TLB must be cleared (unless it contains a process id field).

  * Processing a page fault. Much OS effort is needed; see 3.6.2 just below.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Process termination. Free the page table (and the disk region for swapped out pages, see below.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.6.2 Page Fault Handling

What happens when a process, say process A, gets a page fault? Compare the
following with the processing for a trap command and for an interrupt.

  1. The hardware detects the fault and traps to the kernel (switches to supervisor mode and saves state).
  2. Some assembly language code saves more state, establishes the C-language (or another programming language) environment, and calls the OS.
  3. The OS determines that a page fault occurred and which page was referenced.
  4. If the virtual address is invalid, process A is killed. If the virtual address is valid, the OS must find a free frame. If there is no free frames, the OS selects a victim frame. (Really, the paging daemon does this prior to the fault occurring, but it is easier to pretend that it is done here.) Call the process owning the victim frame, process B. (If the page replacement algorithm is local, then B=A.)
  5. The PTE of the victim page is updated to show that the page is no longer resident.
  6. If the victim page is dirty, the OS schedules an I/O write to copy the
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
 frame to disk and blocks A waiting for this I/O to occur.
  7. Assuming process A needed to be blocked (i.e., the victim page is dirty) the scheduler is invoked to perform a context switch. 
    * Tanenbaum forgot some here.
    * The process selected by the scheduler (say process C) runs.
    * Perhaps C is preempted for D or perhaps C blocks and D runs and then perhaps D is blocked and E runs, etc.
    * When the I/O to write the victim frame completes, a disk interrupt occurs. Assume processes C is running at the time.
    * Hardware trap / assembly code / OS determines I/O done.
    * The scheduler marks A as ready.
    * The scheduler picks a process to run, maybe A, maybe B, maybe C, maybe another processes.
    * At some point the scheduler does pick process A to run. Recall that at this point A is still executing OS code.
  8. Now the O/S has a free frame (this may be much later in wall clock time if a victim frame had to be written). The O/S schedules an I/O to read the desired page into this free frame. Process A is blocked (perhaps for the second time) and hence the process scheduler is invoked to perform a context switch.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  9. Again, another process is selected by the scheduler as above and eventually a disk interrupt occurs when the I/O completes (trap / asm / OS determines I/O done). The PTE in process A is updated to indicate that the page is in memory.
  10. The O/S may need to fix up process A (e.g., reset the program counter to re-execute the instruction that caused the page fault).
  11. Process A is placed on the ready list and eventually is chosen by the scheduler to run. Recall that process A is executing O/S code.
  12. The OS returns to the first assembly language routine.
  13. The assembly language routine restores registers, etc. and returns to user mode.

The user's program running as process A is **unaware** that any of this
occurred.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.6.3 Instruction Backup

A cute horror story. The hardware support for page faults in the original
Motorola 68000 (the first microprocessor with a large address space) was
flawed. If a processor encountered a page fault, there wasn't always enough
information to figure out what to do so (for example did a register pre-
increment occur). That is, one could not safely restart an instruction. This
was thought to make demand paging impossible.

However, one clever system for the 68000 used two processors, one executing
the program and a second processor executing one instruction behind. When a
page fault occurs the executing processor brings in the page and switches to
the second processor, which had not yet executed the instruction, thus
eliminating instruction restart and thereby supporting demand paging.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
The next generation machine, the 68010, provided extra information on the
stack so the horrible/clever 2-processor kludge/hack was no longer necessary.

Don't worry about instruction backup; it is very machine dependent and all
modern implementations get it right.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.6.4 Locking (Pinning) Pages in Memory

We discussed pinning jobs already. The same (mostly I/O) considerations apply
to pages.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.6.5 Backing Store

The issue is where on disk do we put pages that are not in frames.

  * For program text, which is presumably read only, a good choice is the file executable itself.
  * What if we decide to keep the data and stack each contiguous on the backing store? Data and stack grow so we must be prepared to grow the space on disk, which leads to the same issues and problems as we saw with MVT.
  * If those issues/problems are painful, we can scatter the pages on the disk. That is, we can employ paging! Note that this is **not** demand paging.
  * This needs a table to specify where the backing space for each page is located. 
    * This corresponds to the page table used to tell where in real memory a page is located.
    * The format of the memory page table (the one we have studied up to now) is determined by the hardware since the hardware modifies/accesses it. It is machine dependent.
    * The format of the disk page table is decided by the OS designers and 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
is machine independent.
    * If the format of the memory page table were flexible, then we could keep the disk information there as well. But normally the format is not flexible, and hence the disk information is not kept there.
  * What if we felt disk space was too expensive and wanted to put some of these disk pages on say tape or holographic storage?  
Ans: We use _demand paging_ of the disk blocks! That way "unimportant" disk
blocks will migrate out to tape and are brought back in if and when needed.  
Since a tape read requires _seconds_ to complete (because the request is not
likely to be for the sequentially next tape block), it is crucial that we get
**very** few disk block faults.  
I don't know of any systems that actually did this.

**Homework:** Assume every memory reference takes 0.1 microseconds to execute providing the reference page is memory resident. Assume a page fault takes 10 milliseconds to service providing the necessary disk block is actuall
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
y on the disk. Assume a disk block fault takes 10 seconds service. So the worst case time for a memory reference is 10.0100001 seconds. Finally, assume the program requires that a billion memory references be executed.

  1. If the program is always completely resident, how long does it take to execute?
  2. If 0.1% of the memory references cause a page fault, but all the disk blocks are on the disk, how long does the program take to execute and what percentage of the time is the program waiting for a page fault to complete?
  3. If 0.1% of the memory references cause a page fault and 0.1% of the page faults cause a disk block fault, how long does the program take to execute and what percentage of the time is the program waiting for a disk block fault to complete?


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.6.6 Separation of Policy and Mechanism

Skipped.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 3.7 Segmentation

Up to now, the **virtual** address space has been contiguous. In segmentation
the **virtual** address space is divided into a number of **variable**-size
pieces called _segments_. One can view the designs we have studied so far as
having just one segment, the entire address space of the process.

With just one segment (i.e., with all virtual addresses contiguous) memory
management is difficult when there are more that two dynamically growing
regions.

Imagine a program with several large, dynamically-growing, data structures.
The same problem we mentioned for the OS when there are more than two growing
regions, occurs as well here for user programs.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * The user (or some user-mode tool) must decide how much virtual space to leave between the different data structures or the structures must be copied when they are grown. But it is not clear how far apart in virtual memory the data structures should be located, especially if in some runs, one of them gets extremely large but in other runs that structure is small but others are extremely large. (With just two data structures you start them on opposite sides of the virtual space and have them grow towards each other as we did before.) 
  * With segmentation the user instead gives each structure a different segment. That is, the system provides many virtual addresses spaces each starting at zero and the user place one structure in each.

This division of the address space is user _visible_. Recall that user visible
really means visible to user-mode programs.

Unlike (user-invisible) page boundaries, segment boundaries are specified by
the user and thus can be made to occur at logical points, e.g., one table per

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
segment..

Segmentation eases flexible protection and sharing: One places in a single
segment a unit that is logically shared. This would be a natural method to
implement shared libraries.

When shared libraries are implemented on paging systems, the design
essentially mimics segmentation by treating a collection of pages as a
segment. This requires that the end of the unit to be shared occurs on a page
boundary (this is done by padding).

Without segmentation (equivalently said with just one segment) all procedures
are packed together so, if one changes in size, all the virtual addresses
following this procedure are changed and the program must be re-linked. With
each procedure in a separate segment this relinking would be limited to the
symbols defined or used in the modified procedure.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

**Homework:** Explain the difference between internal fragmentation and external fragmentation. Which one occurs in paging systems? Which one occurs in systems using pure segmentation?

#### ** Two Segments

Late PDP-10s and TOPS-10

  * Each process has one _shared_ text segment, that can also contain shared (normally read only) data. As the name indicates, all process running the same executable share the same text segment.
  * The process also contains one (private) writable data segment.
  * Permission bits defined for each segment.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![3-segments](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/3-segments.pn
g)

#### ** Three Segments

Traditional (early) Unix had three segments as shown on the right.

  1. Shared text marked execute only.
  2. Data segment (global and static variables).
  3. Stack segment (automatic variables).

Since the text doesn't grow, this was sometimes treated as 2 segments by
combining text and data into one segment. But then the text could not be
shared.

#### ** General (Not Necessarily Demand) Segmentation

Segmentation is a **user-visible** division of a process into multiple
**variable-size** segments, whose sizes **change dynamically** during

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
execution. It enables fine-grained sharing and protection. For example, one
can share the text segment as done in early unix.

With segmentation, the **virtual** address has two components: the segment
number and the offset in the segment.

Segmentation does **not** mandate how the program is stored in memory.

  * One possibility is that the entire program must be in memory in order to run it. Use whole process swapping. That is either all the segments of a process are memory resident or all of them are swapped ot. Early versions of Unix did this.
  * Can also implement demand segmentation (see below).
  * More recently, segmentation is combined with demand paging (see below).

All segmentation implementations employed a segment table with one entry for
each segment.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

  * A segment table is similar to a page table.
  * Entries are called STEs, Segment Table Entries.
  * Each STE contains the physical base address of the segment and the limit value (the size of the segment).
  * Why is there no limit value in a PTE?  
**Answer**: All pages are the same size so the limit is obvious.

The address translation for segmentation is  
    (seg#, offset) --> if (offset<limit) base+offset else error. 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 3.7.1: Implementation of Pure Segmentation

Pure Segmentation means segmentation without paging.

Segmentation, like whole program swapping, exhibits external fragmentation
(sometimes called _checkerboarding_). (See the treatment of OS/MVT for a
review of external fragmentation and whole program swapping). Since segments
are smaller than programs (several segments make up one program), the external
fragmentation is not as bad as with whole program swapping. But it is still a
serious problem.

As with whole program swapping, compaction can be employed.

#### ** Demand Segmentation

Same idea as demand paging, but applied to segments.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ConsiderationDemand  
Paging Demand  
Segmentation

User (mode) aware

No

Yes

How many addr spaces

1

Many

VA size > PA size

Yes


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Yes

Protect individual  
procedures separately

No

Yes

Accommodate elements  
with changing sizes

No

Yes

Ease user sharing

No


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Yes

Why invented

let the VA  
size exceed  
the PA size

Sharing,  
Protection,  
Independent  
addr spaces

Internal fragmentation

Yes

No, in principle

External fragmentation

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

No

Yes

Placement question

No

Yes

Replacement question

Yes

Yes

  * If a segment is loaded, base and limit are stored in the STE and the valid bit is set in the STE.
  * The STE is accessed on each memory reference (not really, there is prob
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ably a TLB).
  * If the segment is not loaded, the valid bit is unset. The base and limit as well as the disk address of the segment are stored in an OS table.
  * A reference to a non-loaded segment generate a segment fault (analogous to page fault).
  * To load a segment, we must solve both the placement question and the replacement question. This is the same as loading a program in whole programming swapping. For paging, there is no placement question since all frames are exactly the right size.
  * Pure segmentation was once implemented by Burroughs in the B5500. I believe the implementation was in fact demand segmentation.
  * Neither pure segmentation nor demand segmentation is used in modern systems.

The table on the right compares _demand_ paging with _demand_ segmentation.
The portion above the double line is from Tanenbaum.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### ** 3.7.2 and 3.7.3 Segmentation With (Demand) Paging

These two sections of the book cover segmentation combined with demand paging
in two different systems. Section 3.7.2 covers the historic Multics system of
the 1960s (it was coming up at MIT when I was an undergraduate there). Multics
was complicated and revolutionary. Indeed, Thompson and Richie developed (and
named) Unix partially in rebellion to the complexity of Multics. Multics is no
longer used.

Section 3.7.3 covers the Intel Pentium _hardware_, which offers a segmentation
+demand-paging scheme that is not used by any of the current operating systems
(OS/2 used it in the past). The Pentium design permits one to convert the
system into a pure damand-paging scheme and that is the common usage today.
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---


I will present the material in the following order.

  1. Describe segmentation+paging (not demand paging) generically, i.e. not tied to any specific hardware or software.
  2. Note the possibility of using demand paging (again generically).
  3. Give some details of the Multics implementation.
  4. Give some details of the Pentium hardware, especially how it can emulate straight demand paging.

#### ** Segmentation With (non-demand) Paging


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![seg+page](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/seg+page.png)

One can combine segmentation and paging to get advantages of both at a cost in
complexity. In particular, user-visible, variable-size segments are the most
appropriate units for protection and sharing; the addition of (non-demand)
paging eliminates the placement question and external fragmentation (at the
smaller average cost of 1/2-page internal fragmentation per segment).

The basic idea is to employ (non-demand) paging on each segment. A
segmentation plus paging scheme has the following properties.

  * A virtual address becomes a triple: (seg#, page#, offset).
  * Each segment table entry (STE) points to the page table for that segment. Compare this with a  multilevel page table. 
  * The physical size of each segment is a multiple of the page size (since the segment consists of pages). The logical size is not; instead we keep t
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
he exact size in the STE (limit value) and terminate the process (or extend the size of the segment) if it references beyond the limit. In this case the last page of each segment is partially wasted (internal fragmentation).
  * The page# field in the address gives the entry in the chosen page table and the offset gives the offset in the page.
  * From the limit field, one can easily compute the size of the segment in pages (which equals the size of the corresponding page table in PTEs).
  * A straightforward implementation of segmentation with paging would requires 3 memory references (STE, PTE, referenced word) so a TLB is crucial.
  * Some books carelessly say that segments are of fixed size. This is wrong. They are of variable size with a fixed maximum and with the requirement that the physical size of a segment is a multiple of the page size.
  * Keep protection and sharing information on segments. This works well for a number of reasons. 
    1. A segment is variable size.
    2. Segments and their boundaries are user-visible,
    3. Segments are shared by sharing their page tables. This eliminates the problem mentioned above with shared pages.
  * Since we have paging, there is no placement question and no external fragmentation.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * The problems are the complexity and the resulting 3 memory references for each user memory reference. The complexity is real. The three memory references would be fatal were it not for TLBs, which considerably ameliorate the problem. TLBs have high hit rates and for a TLB hit there is essentially no penalty. 

Although it is possible to combine segmentation with non-demand paging, I do
not know of any system that did this.

**Homework:** 46.  
When segmentation and paging are both being used, as in MULTICS, first the
segment descriptor must be looked up, then the page descriptor. Does the TLB
also work this way, with two levels of lookup?

**Homework:** Consider a 32-bit address machine using paging with 8KB pages and 4 byte PTEs. How many bits are used for the offset and what is the size of the largest page table? Repeat the question for 128KB pages. So far this question has been asked before. Repeat both parts assuming the system al
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
so has segmentation with at most 128 segments. Remind me to do this in class next time.

**Homework:** (Ask me about this one next class.) Consider a system with 36-bit addresses that employs both segmentation and paging. Assume each PTE and STE is 4-bytes in size.

  1. Assume the system has a page size of 8K and each process can have up to 256 segments. How large in bytes is the largest possible page table? How large in pages is the largest possible segment?
  2. Assume the system has a page size of 4K and each segment can have up to 1024 pages. What is the maximum number of segments a process can have? How large in bytes is the largest possible segment table? How large in bytes is the largest possible process.
  3. Assume the largest possible segment table is 213 bytes and the largest possible page table is 216 bytes. How large is a page? How large in bytes is the largest possible segment?

#### ** Segmentation With Demand Paging


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
There is very little to say. The previous section employed (non-demand) paging
on each segment. For the present scheme, we employ demand paging on each
segment, that is we perform fetch-on-demand for the pages of each segment.

#### The Multics Scheme

Multics was the first system to employ segmentation plus demand paging. The
implementation was as described above with just a few wrinkles, some of which
we discuss now together with some of the parameter values.

  * The Multics hardware (GE-645) was word addressable, with 36-bit words (the 645 predates bytes).
  * Each virtual address was 34-bits in length and was divided into three parts as mentioned above. The seg# field was the high-order 18 bits; the page# field was the next 6 bits; and the offset was the low-order 10 bits.
  * The actual implementation was more complicated and the full 34-bit virtual address was not present in one place in an instruction.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Thus the system supported up to 218=256K segments, each of size up to 26=64 pages. Each page is of size 210 (36-bit) words.
  * Since the segment table can have 256K STEs (called descriptors), the table itself can be large and was itself demand-paged.
  * Multics permits some segments to be demand-paged while other segments are not paged; a bit in each STE distinguishes the two cases.

#### The Pentium Scheme

The Pentium design implements a trifecta: Depending on the setting of a
various control bits the Pentium scheme can be pure demand-paging (current
OSes use this mode), pure segmentation, or segmentation with demand-paging.

The Pentium supports 214=16K segments, each of size up to 232 bytes.

  * This would seem to require a 14+32=46 bit virtual address, but that is not how the Pentium works. The segment number is _not_ part of the virtual address found in normal instructions.
  * Instead separate instructions are used to specify which are the current
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ly active code segment and data segment (and other less important segments). Technically, the CS register is loaded with the selector of the active code segment and the DS register is loaded with the selector of the active data register.
  * When the selectors are loaded, the base and limit values are obtained from the corresponding STEs (called descriptors).
  * There are actually two flavors of segments, some are private to the process; others are system segments (including the OS itself), which are addressable (but not necessarily accessible) by all processes. 

Once the 32-bit segment base and the segment limit are determined, the 32-bit
address from the instruction itself is compared with the limit and, if valid,
is added to the base and the sum is called the 32-bit linear address. Now we
have three possibilities depending on whether the system is running in pure
segmentation, pure demand-paging, or segmentation plus demand-paging mode.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  1. In pure segmentation mode the linear address is treated as the physical address and memory is accessed.
  2. In segmentation plus demand-paging mode, the linear address is broken into three parts since the system implements 2-level-paging. That is, the high-order 10 bits are used to index into the 1st-level page table (called the page directory). The directory entry found points to a 2nd-level page table and the next 10 bits index that table (called the page table). The PTE referenced points to the frame containing the desired page and the lowest 12 bits of the linear address (the offset) finally point to the referenced word. If either the 2nd-level page table or the desired page are not resident, a page fault occurs and the page is made resident using the standard demand paging model.
  3. In pure demand-paging mode all the segment bases are zero and the limits are set to the maximum. Thus the 32-bit address in the instruction become the linear address without change (i.e., the segmentation part is effectively) disabled. Then the (2-level) demand paging procedure just described is applied.

Current operating systems for the Pentium use mode 3.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 3.8 Research on Memory Management

Skipped


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 3.9 Summary

Read


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## Some Last Words on Memory Management

We have studied the following concepts.

  * Segmentation / Paging / Demand Loading (fetch-on-demand). 
    * Each is a yes or no alternative.
    * This gives 8 possibilities.
  * Placement and Replacement.
  * Internal and External Fragmentation.
  * Page Size and locality of reference.
  * Multiprogramming level and medium term scheduling.

**Remark**: Do on the board the hw problem about sizes.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
# Chapter 4 File Systems

There are three basic requirements for file systems.

  1. **Size**: Store very large amounts of data.
  2. **Persistence**: Data survives the creating process.
  3. **Concurrent Access**: Multiple processes can access the data concurrently.

High level solution: Store data in files that together form a file system.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 4.1 Files


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.1.1 File Naming

Very important. A major function of the file system is to supply uniform
naming. As with files themselves, important characteristics of the file name
space are that it is persistent and concurrently accessible.

Unix-like operating systems extend the file name space to encompass devices as
well

Does each name refer to a unique file?  
Answer: Yes, providing the names start at the same place (absolute or relative
to the same directory).

Does each file have a unique name?  
Answer: Often no. We will discuss this below when we study links.

#### File Name Extensions

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

The extensions are suffixes attached to the file names and are intended to in
some way describe the high-level structure of the file's contents.

For example, consider the .html extension in class-notes.html, the name of the
file we are viewing. Depending on the system and application, these extensions
can have little or great significance. The extensions can be

  1. Conventions just for humans. For example letter.teq (my convention) signifies to me that this letter is written using the troff text-formatting language and employs the eqn preprocessor to handle mathematical equations and the tbl preprocessor to handle tables. Neither linux, troff, tbl, nor equ place any significance in the .teq extension.
  2. Conventions giving default behavior for some programs. 
    * The emacs editor by default edits .html files in html mode. However, emacs can edit such files in any mode and can edit any file in html mode. It just needs to be told to do so during the editing session.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * The firefox browser assumes that an .html extension signifies that the file is written in the html markup language. However, having <html> ... </html> inside the file works as well.
    * The gzip file compressor/decompressor appends the .gz extension to files it compresses, but accepts a --suffix flag to specify another extension.
  3. Default behaviors for the operating system or window manager or desktop environment. 
    * Click on .xls file in windows and excel is started.
    * Click on .xls file in linux and libreoffice is started.
  4. _Required_ for certain programs. 
    * The gnu C compiler (and probably other C compilers) requires C programs be have the .c (or .h) extension, and requires assembler programs to have the .s extension.
  5. Required by the operating system 
    * MS-DOS treats .com files specially.

#### Case Sensitive?

Should file names be case sensitive. For example, do x.y, X.Y, x.Y all name
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

the same file? There is no clear answer.

  * Unix-like systems employ case sensitive file names so the three names given above are distinct.
  * Windows systems employ case insensitive file names so the three names given above are equivalent.
  * Mathematicians (and others) often "consider an element x of a set X" so use case sensitive naming.
  * Similarly, a Java programmer might say String string; (case sensitive). However an Ada programer would know that A:=a+1; is a simple increment (case insensitive).
  * Normal English (and other natural language) usage often employs case insensitivity (e.g. capitalizing a word at the beginning of a sentence does not change the word).


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.1.2 File Structure

How should the file be structured? Said another way, how does the OS interpret
the contents of a file.

A file can be interpreted as a

  1. Byte stream 
    * Unix, MacOS, windows.
    * Maximum flexibility.
    * Minimum structure.
    * All structure on a file is imposed by the applications that use it, not by the system itself.
  2. (fixed size-) Record stream: Out of date 
    * 80-character records for card images.
    * 133-character records for line printer files. Column 1 was for control (e.g., new page) Remaining 132 characters were printed.
  3. Varied and complicated beast. 
    * Indexed sequential.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * B-trees.
    * Supports rapidly finding a record with a specific **key**.
    * Supports retrieving (varying size) records in key order.
    * Treated in depth in database courses.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.1.3 File Types

The traditional file is simply a collection of data that forms the unit of
sharing for processes, even concurrent processes. These are called **regular
files**.

The advantages of uniform naming have encouraged the inclusion in the file
system of objects that are not simply collections of data.

#### Regular Files

  

##### Text vs Binary Files

Some regular files contain lines of text and are called (not surprisingly)
text files or ascii files (the latter name is of decreasing popularity as
latin-1 and unicode become more important). Each text line concludes with some

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
end of line indication: on unix and recent MacOS this is a newline (a.k.a line
feed), in MS-DOS it is the two character sequence carriage return followed by
newline, and in earlier MacOS it was carriage return.

Ascii, with only 7 bits per character, is poorly suited for most human
languages other than English. Latin-1 (8 bits) is a little better with support
for most Western European Languages.

Perhaps, with growing support for more varied character sets, ascii files will
be replaced by unicode (16 bits) files. The Java and Ada programming languages
(and perhaps others) already support unicode.

An advantage of all these formats is that they can be directly printed on a
terminal or printer.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

Other regular files, often referred to as binary files, do not represent a
sequence of characters. For example, a four-byte, twos-complement
representation of integers in the range from roughly -2 billion to +2 billion
is definitely not to be thought of as 4 latin-1 characters, one per byte.

##### Application Imposed File Structure

Just because a file is unstructured (i.e., is a byte stream) from the OS
perspective does not mean that applications cannot impose structure on the
bytes. So a document written without any explicit formatting in MS word is not
simply a sequence of ascii (or latin-1 or unicode) characters.

On unix, an executable file must begin with one of certain magic numbers in
the first few bytes. For a native executable, the remainder of the file has a
well defined format.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

Another option is for the magic number to be the ascii representation of the
two characters #! in which case the next several characters specify the
location of the executable program that is to be run with the current file fed
in as input. That is how some interpreted (as opposed to compiled) languages
work in unix.  
    #!/usr/bin/perl  
    perl script

#### Strongly Typed Files

In some systems the type of the file (which is often specified by the
extension) _determines_ what you can do with the file. This make the easy and
(hopefully) common case easier and, more importantly, safer.

It tends to make the unusual case harder. For example, you have a program t
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
hat
turns out data (.dat) files. Now you want to use it to turn out a java file,
but the type of the output is data and cannot be easily converted to type java
and hence cannot be given to the java compiler.

#### Other-Than-Regular Files

We will discuss several file types that are not called regular.

  1. Directories.
  2. Symbolic Links, which are used to give alternate names to files.
  3. Special files (for devices). These use the naming power of files to unify many actions. 
    
              dir               # prints on screen
          dir > file        # result put in a file
          dir > /dev/audio  # results sent to speaker (sounds awful)
        

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

**Remark**: Lab4 (the last lab) assigned. It is due 10 December 2015.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.1.4 File Access

There are two possibilities, sequential access and random access (a.k.a.
direct access).

With **sequential** access, each access to a given file starts where the
previous access to that file finished (the first access to the file starts at
the beginning of the file). Sequential access is the most common and gives the
highest performance. For some devices (e.g. magnetic or paper tape) access
must be sequential.

With **random** access, the bytes are accessed in any order. Thus each access
must specify which bytes are desired. This is done either by having each
read/write specify the starting location or by defining another system call
(often named seek) that specifies the starting location for the next
read/write.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

For example, in unix, if no seek occurs between two read/write operations,
then the second begins where the first finished. That is, unix treats a
sequences of reads and writes as sequential, but supports seeking to achieve
random access.

Previously, files were declared to be sequential or random. Modern systems do
not do this. Instead all files are random, and optimizations are applied as
the system dynamically determines that a file is (probably) being accessed
sequentially.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.1.5 File Attributes

Various properties that can be specified for a file For example:

  * hidden
  * do not backup
  * owner
  * key length (for keyed files)


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.1.6 File Operations

  * **Create**. The effect of create is essential if a system is to add files. However, it need not be a separate system call. (For example, it can be merged with open).
  * **Delete**. Essential, if a system is to delete files.
  * **Open**. Not essential. It is an optimization in which a process translates a file name to the corresponding disk locations only once per execution rather than once per access. We shall see that for the unix inode-based file systems, this translation can be quite expensive.
  * **Close**. Not essential. Frees resources without waiting for the process to terminate.
  * **Read**. Essential. Must specify filename, file location, number of bytes, and a buffer into which the data is to be placed. Several of these parameters can be set by other system calls and in many operating systems they are.
  * **Write**. Essential, if updates are to be supported. See read for parameters.
  * **Seek**. Not essential (could be in read/write). Specify the offset of the next (read or write) access to this file.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * **Get attributes**. Essential if attributes are to be used.
  * **Set attributes**. Essential if attributes are to be user settable.
  * **Rename**. Copy and delete is not an acceptable substitute for big files. Moreover, copy-delete is not atomic. Indeed link-delete is not atomic so, even if link (discussed below) is provided, renaming a file adds functionality.

**Homework:** 4\. Is the open system call in UNIX absolutely essential? What would be the consequences of not having it?

**Homework:** 5\. Systems that support sequential files always have an operation to rewind files. Do systems that support random-access files need this, too?

**Homework:** 6\. Some operating systems provide a system call RENAME to give a file a new name. Is there any difference at all between using the call to rename a file and just copying the file to a new file with the new name, followed by deleting the old one? 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.1.7 An Example Program Using File System Calls

Let's look at [copyfile.c](diagrams/copyfile.c.pdf) to see the use of file
descriptors and error checks, even though there could be more attention to
errors.

Note specifically, the code checks the return value from each I/O system call.
It is a common error to assume that

  1. Open always succeeds. It can fail due to the file not existing or the process having inadequate permissions.
  2. Read always succeeds. An end of file can occur. Fewer than expected bytes could have been read. Also the file descriptor could be bad, but this should have been caught when checking the return value from open.
  3. Create always succeeds. It can fail when the disk (partition) is full, or when the process has inadequate permissions.
  4. Write always succeeds. It too can fail when the disk is full or the process has inadequate permissions.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 4.2 Directories

Directories form the primary unit of organization for the filesystem.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.2.1-4.2.3 Single-Level (Two-Level) and Hierarchical Directory Systems

![dir sys levels](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/dir-sys-
levels.png)

One often refers to the level structure of a directory system. It is easy to
be fooled by the names given. A single level directory structure results in
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
 a
file system tree with **two** levels: the single root directory and (all) the
files in this directory. That is, there is one level of directories and
another level of files so the full file system tree has two levels.

Possibilities.

  * One directory in the system (single-level). This possibility is illustrated by the top left tree.
  * One directory per user and a root above these (two-level). This possibility is illustrated by the top right tree.
  * One tree in the system. This possibility is illustrated by the bottom tree.
  * One tree per user with a root above. This possibility is also illustrated by the bottom tree.
  * One forest in the system. This possibility is illustrated by viewing all three trees as together constituting the file system forest.
  * One forest per user. This possibility is illustrated by viewing the top two trees as belong to one user and the bottom tree as belonging to a seco
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
nd user (a tree is a special case of a forest).

These possibilities are not as wildly different as they sound or as the
pictures suggests.

  * Assume the system has only one directory, but also assume the character / is allowed in a file name. Then one could fake a tree by having a file named  
/allan/gottlieb/courses/os/class-notes.html  
rather than a directory allan, a subdirectory gottlieb, ..., a file class-
notes.html.

  * The Dos (windows) file system is a forest, the Unix file system is a tree (until we learn about links below, then it is not even a forest). In Dos, there is no common parent of a:\ and c:\, so the file system is not a tree. But Windows explorer makes the dos forest look quite a bit like a tree.
  * You can get an effect similar to one X per user by having just one X in the system and having permissions that permits each user to visit only a subset. Of course if the system doesn't have permissions, this is not possible.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Today's multiuser systems have a tree per system or a forest per system. This is not strictly true due to links, which we will study soon.
  * Simple embedded systems often use a one-level directory system. 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.2.4 Path Names

You can specify the location of a file in the file hierarchy by using either
an **absolute** or a **relative** path to the file.

  * An absolute path starts at the (or one of the, if we have a forest) root(s).
  * A relative path starts at the **current** (a.k.a working) directory. In order to support relative paths, a process must know its current directory and there must be such a field in the process control block.
  * The special directories . and .. represent the current directory and the parent of the current directory respectively. In the (or a) root .. and . both represent the current (root) directory.

**Homework:** Give 8 different path names for the file /etc/passwd.

**Homework:** 8\. A simple operating system supports only a single directory but allows it to have arbitrarily many files with arbitrarily long file names. Can something approximating a hierarchical file system be simulated? 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
How?.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.2.5 Directory Operations

Remember that the job of a director is to map names (of its children) to the
files represented by the those names.

  1. **Create**. Produces an empty directory. Normally the directory created actually contains . and .., so is not really empty
  2. **Delete**. The delete system call requires the directory to be empty (i.e., to contain just . and ..). Delete commands intended for users have options that cause the command to first empty the directory (except for . and ..) and then delete it. These user commands make use of both file and directory delete system calls.
  3. **Opendir**. As with the file open system call, opendir creates a handle for the directory that speeds future access by eliminating the need to process the name of the directory.
  4. **Closedir**. As with the file close system call, closedir is an optimization that enables the system to free resources prior to process termination.
  5. **Readdir**. In the old days (of unix) one could read directories as f
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
iles so there was no special readdir (or opendir/closedir) system call. It was then believed that the uniform treatment would make programming (or at least system understanding) easier as there was less to learn.  
However, experience has taught that this was a poor idea since the structure
of directories was exposed to users. Early unix had a simple directory
structure and there was only one type of structure for all implementations.
Modern systems have more sophisticated structures and more importantly they
are not fixed across implementations. So if programs used read() to read
directories, the programs would have to be changed whenever the structure of a
directory changed. Now we have a readdir() system call that knows the
structure of directories. Therefore if the structure is changed only readdir()
need be changed.  
This is an example of the software principle of information hiding.

  6. **Rename**. Similar to the file rename system call. Again note that re
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
name is atomic; whereas, creating a new directory, moving the contents, and then removing the old one is not.
  7. **Link**. Add another name for a file; discussed below.
  8. **Unlink**. Remove a directory entry. This is how a file is deleted. However, if there are many links and just one is unlinked, the file remains. Unlink is discussed in more detail below.
  9. There is **no** Writedir operation. Directories are written as a side effect of other operations.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.2.A Mounting One Filesystem on Another (Unix)

I have mentioned that in Unix all files can be accessed from the single root.
This does not seem possible when you have two disks (or two partitions on one
disk, which is nearly the same thing). How can you get from the root of one
partition to anywhere in the other?


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![mount](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/mount.png)

One solution might be to make a super-root having the two original roots as
children. However, this is not done. Instead, one of the devices is _mounted_
on the other, as is illustrated in the figures on the right.

The top row shows two filesystems (on separate devices). From either root, you
can easily get to all the files in that filesystem. Filesystems on Windows
machines leave the situation as shown and have syntax to change our focus from
one filesystem to another.

Unix uses a different approach. In the normal case (the only one we will
consider), one **mount**s one filesystem on an empty directory of the other.
For example, the bottom row shows the result of mounting the right filesystem

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
on the directory /y of the left filesystem.

In this way, the forest of the top row becomes a tree in the bottom. However,
we shall see below that the introduction of (hard and symbolic) links in Unix
results in filesystems that are not trees. It is true however, that you can
name any file starting from the (single) root.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 4.3 File System Implementation

Now that we understand how the file system looks to a user, we turn our
attention to how it is implemented.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.3.1 File System Layout

We look at how the file systems are laid out on disk in modern PCs. Much of
this is required by the bios so all PC operating systems have the same lowest
level layout. I do not know the corresponding layout for mainframe systems or
supercomputers.

A system often has more than one physical disk. The first disk is the boot
disk. How do we determine which is the first disk?

  1. Easiest case: only one disk.
  2. Only one disk controller. The disk with the lowest number is the boot disk. The numbering is system dependent, for SCSI (small computer system interconnect, now used on big computers as well) you set switches on the drive itself.
  3. Multiple disk controllers. The controllers are ordered in a system dependent way.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

The BIOS reads the first sector (smallest addressable unit of a disk) of the
boot disk into memory and transfers control to it. A sector contains 512
bytes. The contents of this particular sector is called the MBR (master boot
record).

The MBR contains two key components: the partition table and the first-level
loader.

  * A disk can be logically divided into variable size **partitions**, each acting as a logical disk. That is, normally each partition holds a complete file system. The partition table (like a process's page table) gives the starting point of each partition. It is actually more like the segment table of a pure segmentation system since the objects pointed to (partitions and segments) are of variable size. As with segments, the size of each partition is stored in the corresponding entry of the partition table.
  * One partition in the partition table is marked as the active partition.
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

  * The first level loader then loads 2nd-level loader, which resides in the first sector of the active partition, and transfers control to it. This sector is called the boot sector or boot block.
  * The boot block then loads the OS (actually it can load another loader, etc) 

#### Contents of a Partition (Containing a Filesystem)

The contents of a filesystem vary from one file system to another but there is
some commonality.

  * Each partition has a boot block as mentioned above.
  * Most partitions contain one filesystem.
  * Each such partition must have some information saying what type of file system it contains. The region containing this and other administrative information is often called the superblock. The location of the superblock is fixed.
  * The root directory is either stored in a fixed location, or its locatio
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
n is in a fixed position in the superblock, or (as in unix i-node file systems) the root i-node is in a fixed location (or it is pointed to from a fixed location in the superblock). The root i-node points to the root directory. We will have more to say about i-nodes below.
  * A list of free (i.e., available) disk blocks must be maintained. If these blocks are linked together, the head of the list (or a pointer to it) must be in a well defined spot. If a bitmap is used, it (or a pointer to it) must be in a well defined place.
  * Files and directories (i.e., in use disk blocks). These are of course the reason we have the file system.
  * For i-node based systems, the i-nodes are normally stored in a separate region of the partition.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.3.2 Implementing Files

A fundamental property of disks is that they cannot read or write single
bytes. The smallest unit that can be read or written is called a **sector**
and is normally 512 bytes (plus error correction/detection bytes). This is a
property of the hardware, not the operating system.  Recently some drives have
much bigger sectors, but we will ignore this fact.

The operating system reads or writes disk **blocks**. The size of a block is a
multiple (normally a power of 2) of the size of a sector. Since sectors are
(for us) always 512 bytes, the block size can be 512, 1024=1K, 2K, 4K, 8K,
16K, etc. The most common block sizes today are 4K and 8K.

So files will be composed of blocks.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
When we studied memory management, we had to worry about fragmentation,
processes growing and shrinking, compaction, etc. Many of these same
considerations apply to files; the difference (largely in nominclature) is
that instead of a memory region being composed of bytes, a file is composed of
blocks.

#### Contiguous Allocation

Recall the simplest form of memory management beyond uniprogramming was OS/MFT
where memory was divided into a very few regions and each process was given
one of these regions. The analogue for disks would be to give each file an
entire partition. This is too inflexible and is not used for files.

The next simplest memory management scheme was the one used in OS/MVT (a
swapping system), where the memory for a process was contiguous.

  * The analogous scheme for files is called contiguous allocation.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Each file is stored as one piece.
  * This scheme is simple and fast for access since, as we shall see next chapter, disks give much better performance when accessed sequentially.
  * However contiguous allocation is problematic for growing files. 
    * If a growing file reaches another file, the system must move files.
    * The extreme would be to compactify the disk, which entails moving many files and the resulting configuration with no holes will have trouble with any file growing (except the last file).
    * OS/MVT had the analogous problem when jobs grew.
  * As with memory, there is the problem of external fragmentation.
  * Contiguous allocation is no longer used for general purpose rewritable file systems.
  * It is ideal for file systems where files do not change size.
  * It is used for CD-ROM file systems.
  * It is used (almost) for DVD file systems. A DVD movie is a few gigabytes in size but the 30 bit file length field limits files to 1 gigabyte so each movie is composed of a few contiguous files. The reason I said almost is that the terminology used is that the movie is one file stored as a sequence of **extents** and only the extents are contiguous. 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
**Homework:** 11\. (There is a typo: the first sentence should end at the first comma.) Contiguous allocation of files leads to disk fragmentation. Is this internal fragmentation or external fragmentation? Make an analogy with something discussed in the previous chapter. 

#### Linked Allocation

A file is an ordered sequence of blocks. We just considered storing the blocks
one right after the other (contiguous) the same way that one can store an in-
memory list as an array. The other common method for in-memory lists is to
link the elements together via pointers. This can also be done for files as
follows.

  * The directory entry for the file contains a pointer to the first block of the file.
  * Each block of a file contains a pointer to the next block the file.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
However, this scheme gives horrible performance for random access: N disk
accesses are needed to access block N.

As a result _this implementation_ of linked allocation is not used.

Consider the following two code segments that store the same data but in a
different order. The first is analogous to the horrible linked list file
organization above and the second is analogous to the ms-dos FAT file system
we study next.

    
    
      struct node_type {
          float data;                  float node_data[100];
          int   next;                  int   node_next[100];
      } node[100]
    

With the second arrangement the data can be stored far away from the next

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
pointers. In FAT this idea is taken to an extreme: The data, which is large (a
disk block), is stored on disk; whereas, the next pointers, which are small
(each is an integer) are stored in memory in a File Allocation Table or FAT.
(When the system is shut down the FAT is copied to disk and when the system is
booted, the FAT is copied to memory.)

#### The FAT (File Allocation Table) File System

The FAT file system stores each file as a linked list of disk blocks. The
blocks, which contain file data only (not the linked list structure) are
stored on disk. The pointers implementing the linked list are stored in
memory.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![ms-dos](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/ms-dos.png)

  * There is a long lineage of FAT file systems (FAT-12, FAT-16, vfat, ...) all of which use the file allocation table. The following description of FAT is fairly generic and applies to all of them.
  * FAT was the file system used by dos and and early versions of Windows.
  * The NT series of Windows (NT, 2000, XP, Vista, 7, 8) supports FAT as well as the superior NTFS (NT File System).
  * Linux has full support for FAT (and improving support for NTFS).
  * MacOS has full support for FAT.
  * FAT is used on flash RAMS (USB memory sticks) as well as memory cards for digital cameras.
  * The directory entry for a file points to the first block (i.e., the directory entry specifies the block number).
  * The FAT itself (i.e., the table) is maintained in memory having one (1-word) entry for each disk block. The entry for block N contains the block number of the next block in the same file as N. If block N is the last block of a file, the entry is EOF. An example FAT is on the right. The directory entry for file A contains 4, the entry for B contains 6. Let's trace the steps to find all the blocks in each file.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * FAT implements linked allocation but the links are stored separate from the data.
  * The time needed to access a random block is still is linear in the size of the file but now all the references are to the FAT, which is in memory. So it is bad for random accesses, but not nearly as horrible as plain linked allocation.
  * The size of the table is one pointer per disk block. So the ratio of the disk space supported to the memory space needed is 
    
              (size of a disk block) / (size of a pointer)
        

If the block size is 8KB and a pointer is 2B, the memory requirement is 1/4
megabyte for each disk gigabyte. Large but not prohibitive. (While 8KB is
reasonable today for blocksize, 2B pointers is not since that would mean the
largest partition could be 216 blocks = 216×213 bytes = 229 bytes = 1/2 GB,
which is much too small.)

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

  * If the block size is 512B (the sector size of most disks) and a pointer is 8B then the memory requirement is 16 megabytes for each disk gigabyte, which would most likely be prohibitive. 

More details can be found in [ this lecture
](http://www.c-jump.com/CIS24/Slides/FAT/lecture.html) from Igor Kholodov. (A
local copy is [here).](kholodov-fat.html)

#### The Unix Inode-based Filesystem


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![inodes](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/inodes.png)

Continuing the idea of adapting storage schemes from other regimes to file
storage, why don't we mimic the idea of (non-demand) paging and have a table
giving, for each block of the file, where on the disk that file block is
stored? In other words a ``file block table'' mapping each file block to its
corresponding disk block. This is the idea of (the first part of) the unix
i-node solution, which we study next.

Although Linux and other Unix and Unix-like operating systems have a variety
of file systems, the most widely used Unix file systems are i-node based as
was the original Unix file system from Bell Labs. As we shall see i-node file
systems are more complicated than straight paging, they have aspects of
multilevel paging as well.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
There is some question of what the i stands for. The consensus seems to be
index. Now, however, people often write inode (not i-node) and don't view the
i as standing for anything. For example Dennis Richie doesn't remember why the
name was chosen (lkml.indiana.edu/hypermail/linux/kernel/0207.2/1182.html).

> In truth, I don't know either. It was just a term that we started to use.
"Index" is my best guess, because of the slightly unusual file system
structure that stored the access information of files as a flat array on the
disk, with all the hierarchical directory information living aside from this.
Thus the i-number is an index in this array, the i-node is the selected
element of the array. (The "i-" notation was used in the 1st edition manual;
its hyphen became gradually dropped).


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Inode based systems have the following properties.

  1. Each file and directory has an associated inode, which enables the system to find the blocks of the file or directory.
  2. The inode associated with the root (called the root inode) is at a known location on the disk. In particular, the root inode can be found by the system.
  3. The directory entry for a file contains a pointer to the file's i-node.
  4. The directory entry for a subdirectory contains a pointer to the subdirectory's i-node.
  5. The metadata for a file or directory is stored in the corresponding inode.
  6. The inode itself points to the first few data blocks, often called direct blocks. I believe in early systems there were 10 direct blocks pointers in each inode. In the diagram, the inode contains pointers to six direct blocks, and all data blocks are colored blue.
  7. The inode also points to an indirect block, which then points to a number K of disk blocks. K=(blocksize)/(pointersize). In the diagram, the indirect blocks (technically single indirect blocks) are colored green.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  8. The inode also points to a double indirect block, which points to a K single indirect blocks, each of which points to K data blocks. In the diagram, double indirect blocks are colored magenta.
  9. For some implementations there is a triple indirect block as well. A triple indirect block points to K double indirect blocks, which point ... . In the diagram, the triple indirect block is colored yellow.
  10. The i-node is in memory for open files. So references to direct blocks require just one I/O.
  11. For big files most references require two I/Os (indirect + data).
  12. For huge files most references require three I/Os (double indirect, indirect, and data).
  13. For humongous files most references require four I/Os.
  14. Actually, fewer I/Os are normally required due to caching.
  

##### Retrieving a Block in an Inode-Based File System

Given a block number (byte number / block size), how do you find the block?
Specifically, if we assume

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

  1. The file system does not have a triple indirect block.
  2. We desire block number _N_, where _N=0_ is the first block.
  3. There are _D_ direct pointers in the inode. These pointers are numbered _0..(D-1)_.
  4. There are K pointers in each indirect block. These pointers are numbered _0..(K-1)_.

then the following algorithm can be used to find block _N_.

    
    
      If N < D           // This is a direct block in the i-node
        use direct pointer N in the i-node
      else if N < D + K  // The single indirect block has a pointer to this block
        use pointer D in the inode to get the indirect block
        then use pointer N-D in the indirect block to get block N
      else   // This is one of the K*K blocks obtained via the double indirect block

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
        use pointer D+1 in the inode to get the double indirect block
        let P = (N-(D+K)) DIV K      // Which single indirect block to use
        use pointer P to get the indirect block B
        let Q = (N-(D+K)) MOD K      // Which pointer in B to use
        use pointer Q in B to get block N
    

For example, let D=12, assume all blocks are 1000B, assume all pointers are
4B. Retrieve the block containing byte 1,000,000.

  * K = 1000/4 = 250.
  * Byte 1,000,000 is in block number N=1000.
  * N > D + K so we need the double indirect block.
  * Follow pointer number D+1=13 in the inode to retrieve the double indirect block.
  * P=(1000-(12+250)) DIV 250 = 738 DIV 250 = 2.
  * Follow pointer number P=2 in the double indirect block to retrieve the needed single indirect block.
  * Q=(1000-(12+250)) MOD 250 = 738 MOD 250 = 238.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Follow pointer number 238 in the single indirect block to retrieve the desired block (block number 1000). 

With a triple indirect block, the ideas are the same, but there is more work.

**Homework:** Consider an inode-based system with the same parameters as just above, D=12, K=250, etc.

  1. What is the largest file that can be stored.
  2. How much space is used to store this largest possible file assuming the attributes require 64B?
  3. What percentage of the space used actually holds file data?
  4. Repeat all the above, now assuming the file system supports a triple indirect block.
  5. Remind me to do this one in class next time.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.3.3 Implementing Directories

Recall that the primary function of a directory is to map the file name (in
ASCII, Unicode, or some other text-based encoding) to whatever is needed to
retrieve the data of the file itself.

There are several ways to do this depending on how files are stored.

  * For contiguously allocated files, the directory entry for a file contains the starting address on the disk and the file size. Since disks can only be accessed by sectors, we store the sector number. The system can choose to start all files on a block (rather than sector) boundary in which case the block number, which is smaller, is stored instead.
  * For linked allocation (pure linked or FAT-based) the directory entry again points to the first block of the file.
  * For inode-based file systems, the directory entry points to the inode. 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Another important function is to enable the retrieval of the various
attributes (e.g., length, owner, size, permissions, etc.) associated with a
given file.

  * One possibility is to store the attributes in the directory entry for the file. Windows does this.
  * Another possibility for inode-based systems, is to store the attributes in the inode as we have suggested above. The inode-based file systems for Unix-like operating systems do this.

**Homework:** 30 It has been suggest that the first part of each Unix file be kept in the same disk block as its i-node. What good would this do?

#### Long File Names

It is convenient to view the directory as an array of entries, one per file.
This view tacitly assumes that all entries are the same size and, in early
operating systems, they were. Most of the contents of a directory are

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
inherently of a fixed size. The primary exception is the file name.

Early systems placed a severe limit on the maximum length of a file name and
allocated this much space for all names. DOS used an 8+3 naming scheme (8
characters before the dot and 3 after). Unix version 7 limited names to 14
characters.

Later systems raised the limit considerably (255, 1023, etc) and thus
allocating the maximum amount for each entry was inefficient and other schemes
were used. Since we are storing variable size quantities, a number of the
consideration that we saw for non-paged memory management arise here as well.

#### Searching Directories for a File

The simple scheme is to search the list of directory entries linearly, when
looking for an entry with a specific file name. This scheme becomes

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
inefficient for very large directories containing hundreds or thousands of
files. In this situation a more sophisticated technique (such as hashing or
B-trees) is used.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.3.4 Shared (Multinamed) Files (Links)

We often think of the files and directories in a file system as forming a tree
(or forest). However in most modern systems this is not necessarily the case,
the same file can appear in two (or more) different directories (not two
copies of the file, but the _same_ file). It can also appear multiple times in
the same directory, having different names each time.

I like to say that the same file has two different names. One can also think
of the file as being shared by the two directories (but those words don't work
so well for a file with two names in the same directory).

  * Shared files is Tanenbaum's terminology.
  * If a file exists, one can create another name for it (quite possibly in another directory).

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * This is often called creating another link to the file.
  * Unix has two flavor of links, **hard links** and **symbolic links** (a.k.a. **symlinks**).
  * Dos/windows has shortcuts, which behave somewhat like symlinks, but I don't believe it has an analogue of hard links.
  * We will concentrate on both flavors of unix links.
  * These links often cause confusion, but I really believe that the diagrams I created make it all clear. 

#### Hard Links

With unix hard links there are multiple names for the same file and each name
has **equal status**. The directory entries for both names point to the
**same** inode.

  * Hard links are thus _symmetric_ multinamed files.
  * When a hard link is created _another_ name is created for the _same_ file. The number of files in the system is _the same_ before and after the hard link is created.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * In the i-node implementation of Unix file systems, creating a hard link does _not_, repeat **NOT** create a new i-node.
  * It is _not_, I repeat **NOT**, true that one name is the real name and the other one is just a link.
  * Indeed, after the hard link has been created it is not possible to tell which was the original name and which is the newly created link. 
![dir-tree](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/dir-tree.png)

For example, the diagram on the right illustrates the result that occurs wh
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
en,
starting with an empty file system (i.e., just the root directory) one
executes

    
    
      cd /
      mkdir /A; mkdir /B
      touch /A/X; touch /B/Y
    

The diagrams in this section use the following conventions

  * Yellow circles represent ordinary files.
  * Blue squares represent directories.
  * Names are written on the edges. For example, one name for the left circle is /A/X. 
    * It is not customary to write the names on the edges, normally they are written in the circles and squares.
    * When there are no multi-named files, it doesn't matter if they are wr
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
itten in the node or on the edge.
    * We will see that when files can have multiple names it is much better to write the name on the edge. 
![hard-link](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/hard-link.png)

Now we execute ` ln /B/Y /A/New ` which leads to the next diagram on the
right.

Note that there are still exactly 5 inodes and 5 files: two regular files a
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
nd
three directories. All that has changed is that there is another name for one
of the regular files. At this point there are two equally valid name for the
right hand yellow file, /B/Y and /A/New. The fact that /B/Y was created first
is **NOT** detectable.

  * The directory entries for both file names point to the **same** i-node.
  * The file has only one owner (the one who created the file initially).
  * The file has one date of last access (the last access by any of its names), one set of permissions, one ... .
  * Note the usage: the file nameS (plural) vs the file (singular).
  * Note also that when creating a hard link the existing file must be an ordinary file and **not** a directory. In particular, we could not have said 
    
              ln /B /A/NewDir

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
        

![hard-link-2](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/hard-
link-2.png)

Next assume Bob created /B and /B/Y and Alice created /A, /A/X, and /A/New.
Later Bob tires of /B/Y and removes it by executing

    

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    
      rm /B/Y
    

The file /A/New is still fine (see the diagram on the right). But it is owned
by Bob, who can't find it! If the system enforces quotas Bob will likely be
charged (as the owner), but he can neither find nor delete the file (since Bob
cannot unlink, i.e. remove, files from /A).

If, prior to removing /B/Y, Bob had examined its link count (an attribute of
the file), he would have noticed that there is another (hard) link to the
file, but would not have been able to determine in which directory (/A in this
case) the hard link was located or what is the name of the file in that
directory (New in this case).


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Since hard links are only permitted to files (not directories) the resulting
file system is a dag (directed acyclic graph). That is, there are no directed
cycles. We will now proceed to give away this useful property by studying
symlinks, which _can_ point to directories.

#### Symlinks

As just noted, hard links do **NOT** create a new file, just another name for
an existing file. Once the hard link is created the two names have equal
status.

Symlinks, on the other hand **DO** create another file, a non-regular file,
that itself serves as another name for the original file. Specifically

  * Creation of a symlink results in an _asymmetric_ multi-named file. Assuming the original was a regular file, the symlink does indeed have a differ
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ent status.
  * When a symlink is created **another** file is created. The **contents** of the new file is the **name** of the original file.
  * A hard link in contrast **is** (another name for) the original file.
  * The examples should make this clear.

Again start with an empty file system and this time execute the following code
sequence (the only difference from the above is the addition of a -s).

    
    
      cd /
      mkdir /A; mkdir /B
      touch /A/X; touch /B/Y
      ln -s /B/Y /A/New
    


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![symlink](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/symlink.png)

We now have an additional file /A/New, which is a symlink to /B/Y.

  * The additional file entails an additional inode. The diagram on the right shows 3 directories, 2 regular files, and 1 symlink. This count implies 3+2+1=6 inodes (assuming an i-node based file system).
  * The file named /A/New has the string /B/Y as its _data_ (_not_ metadata).
  * The system notices that /A/New is a red diamond (a symlink, **not** a regular file) so reading /A/New will return the contents of /B/Y (assuming the reader has read permission for /B/Y).
  * It is also possible to read the contents of /A/New itself (those contents are the four characters /B/Y).
  * The _size_ of A/New is 4 bytes, one byte per character '/', 'B', '/', and 'Y'.
  * If /B/Y is removed, /A/New becomes invalid.
  * If a new /B/Y is created, /A/New is once again valid.
  * Removing /A/New has no effect of /B/Y.
  * Examining /B/Y does not reveal the existence of /A/New.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * If a user has write permission for /B/Y, then writing /A/New is possible and writes /B/Y.

The bottom line is that, with a hard link, a new **name** is created for the
file. This new name has equal status with the original name. This can cause
some surprises (e.g., _you_ create a link but _I_ own the file). With a
symbolic link a new **file** is created (owned by the creator naturally) that
contains the name of the original file. We often say the new file points to
the original file.

**Question**: Consider the hard link setup above. If Bob removes /B/Y and then creates another /B/Y, what happens to /A/New?  
**Answer**: Nothing. /A/New is still a file owned by **Bob** having the same contents, creation time, etc. as the _original_ /B/Y.

**Question**: What about with a symlink?  

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
**Answer**: /A/New becomes invalid and then valid again, this time pointing to the _new_ /B/Y. (It can't point to the old /B/Y as that is completely gone.)

**Note:**  
Shortcuts in windows contain more than symlinks contain in unix. In addition
to the file name of the original file, they can contain arguments to pass to
the file if it is executable. So ashortcut to firefox.exe can specify
firefox.exe //cs.nyu.edu/~gottlieb/courses/os/class-notes.html

Moreover, as was pointed out by students in my 2006-07 fall class, the
shortcuts are not a feature of the windows FAT file system itself, but simply
the actions of the command interpreter when encountering a file named *.lnk  
**End of Note**.

##### Symlinking a Directory

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

![simlink-dir](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/symlink-
dir.png)

What happens if the target of the symlink is an existing directory? For
example, consider the code below (again starting with an empty file system),
which gives rise to the diagram on the right.

    

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    
      cd /
      mkdir /A; mkdir /B
      touch /A/X; touch /B/Y
      ln -s /B /A/New
    

Some questions

  * Is there a file named /A/New/Y ?  
Answer: Yes.

  * What happens if you execute cd /A/New; dir ?  
Answer: You see a listing of the files in /B, in this case the single file Y.

  * What happens if you execute cd /A/New/.. ?  
Answer: Not clear!  
Clearly you are changing the current working directory to the parent directory

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
of /A/New. But is that /A or /?  
The command interpreter I use offers both possibilities.

    * cd -L /A/New/.. takes you to A (L for logical).
    * cd -P /A/New/.. takes you to / (P for physical).
    * cd /A/New/.. takes you to A (logical is the default).
  * What did I mean when I said the pictures made it all clear?  
Answer: From the file system perspective it is clear. It is not always so
clear what programs will do.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.3.5 Log-Structured File Systems

This research project of the early 1990s was inspired by the key observation
that systems are becoming limited in speed by small writes. The factors
contributing to this phenomenon were (and still are).

  1. The CPU speed increases have far surpassed the disk speed increases so the system has become I/O limited.
  2. The large buffer cache found on modern systems has led to fewer read requests actually requiring I/Os.
  3. A disk I/O requires almost 10ms of preparation before any data is transferred, and then can transfer a block in less than 1ms. Thus, a one block transfer spends most of its time getting ready to transfer.

The goal of the log-structured file system project was to design a file system
in which all writes are large and sequential (most of the preparation is
eliminated when writes are sequential). These writes can be thought of as
being appended to a log, which gave the project its name.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

  * The project worked with a unix-like file system, i.e. it was i-node based.
  * The system accumulates writes in a buffer until have (say) 1MB to write.
  * When the buffer is full, write it to the end of the disk (treating the disk as a log).
  * Thus writes are sequential and large and hence fast.
  * When any part of a file is changed, the i-node is rewritten.
  * The 1MB units on the disk are called (unfortunately) segments. I will refer to the buffer as the segment buffer.
  * A segment can contain i-nodes, direct blocks, indirect blocks, blocks forming part of a file, and blocks forming part of a directory. In short a segment contains the most recently modified (or created) 1MB of blocks.
  * Note that the now useless overwritten blocks are not reclaimed!
  * The system keeps a map of where the most recent version of each i-node is located. This map is on disk (but the heavily accessed parts will be in the buffer cache).
  * So the (most up to date) i-node of a file can be found and from that the entire file can be found.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * But the disk will fill with garbage since modified blocks are not reclaimed.
  * A cleaner process runs in the background and examines segments starting from the beginning. It removes overwritten blocks and then adds the remaining blocks to the segment buffer. (This is very much not trivial.)
  * Thus the disk is compacted and is treated like a circular array of segments. 

Despite the advantages given, log-structured file systems have not caught on.
They are incompatible with existing file systems and the cleaner has proved to
be difficult.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.3.6 Journaling File Systems

Many seemingly simple I/O operations are actually composed of sub-actions. For
example, deleting a file on an i-node based system (really this means deleting
the last link to the i-node) requires removing the entry from the directory,
placing the i-node on the free list, and placing the file blocks on the free
list.

What happens if the system crashes during a delete and some, but not all
three, of the above actions occur?

  * If the operations are guaranteed to be done in the order given, then the worst that can occur is that the entry is removed from the directory, but some file blocks and possibly the i-node are not reclaimed. This wastes resources, but is not a disaster.
  * As we will learn later, I/O performance is sometimes improved if operat
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ions are executed out of order.
  * In that case we can have a directory entry pointing to an i-node that has been freed or an i-node referring to blocks that have been freed.
  * Since free blocks and i-nodes are later reassigned to other files, the results can be catastrophic.

A journaling file system prevents these problems by using an idea from
database theory, namely transaction logs. To ensure that the multiple sub-
actions are all performed, the larger I/O operation (delete in the example) is
broken into 3 steps.

  1. Write a log entry stating what has to be done and ensure it is written to disk.
  2. Start doing the sub-actions.
  3. When all sub-actions complete, mark the log entry as complete (and eventually erase it)..

After a crash, the log (called a journal) is examined and if there are pending

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
sub-actions, they are done before the system is made available to users.

Since sub-actions may be repeated (once before the crash, and once after), it
is required that they all be _idempotent_ (applying the action twice is the
same as applying it once).

Some history.

  * IBM's AIX had a journaling file system in 1990.
  * NTFS had journaling from day 1 (1993).
  * Many Unix systems have it now.
  * The main linux file system (ext2) added journaling (and became ext3, then ext4) in 2001. Journaling appeared earlier that year in other Linux file systems.
  * Journaling was added to HFS+, the MacOS file system, in 2002.
  * FAT has never had journaling (and probably never will).


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.3.7 Virtual File Systems

![vfs](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/vfs.png)

A single operating system needs to support a variety of file systems. The
software support for each file system would have to handle the various I/O
system calls defined.

Not surprisingly the various file systems often have a great deal in common

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
and large parts of the implementations would be essentially the same. Thus for
software engineering reasons one would like to abstract out the common part.

This was done by Sun Microsystems when they introduced NFS the Network File
System for Unix and by now most unix-like operating systems have adopted this
idea. The common code is called the VFS layer and is illustrated on the right.

The original motivation for Sun was to support NFS (Network File System),
which permits a file system residing on machine A to be mounted onto a file
system residing on machine B. The result is that by cd'ing to the appropriate
directory on machine B, a user with sufficient privileges can
read/write/execute the files in the machine A file system.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Note that mounting one file system onto another (whether they are on different
machines or not) does **not** require that the two file systems be the same
type. For example, I routinely mount FAT file systems (from MP3 players,
cameras, ets) on to my Linux inode-based file system. The involvement of
multiple file system software components for a single operation is another
point in VFS's favor.

Nonetheless, I consider the idea of VFS to be mainly good (perhaps superb)
software engineering more than OS design. The details are naturally OS
specific.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 4.4 File System Management and Optimization

Since I/O operations can dominate the time required for complete user
processes, considerable effort has been expended to improve the performance of
these operations.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.4.1 Disk Space Management

All general purpose file systems use a paging-like algorithm for file storage
(read-only systems, which often use contiguous allocation, are the major
exception). Files are broken into fixed size pieces, called **blocks** that
are scattered over the disk.

Note that although this algorithm is similar to paging, it is _not_ called
paging and often does not have an explicit page table.

Note also that all the blocks of the file are present at all times, i.e., this
system is **not** _demand_ paging.

One can imagine systems that do utilize demand-paging-like algorithms for disk
block storage. In such a system only some of the file blocks would be stored

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
on disk with the rest on tertiary storage (some kind of tape, or holographic
storage perhaps). NASA might do this with their huge datasets.

#### Choice of Block Size

We discussed a similar question before when studying page size. The sizes
chosen are similar, both blocks and pages are measured in kilobytes. Common
choices are 4KB and 8KB, for each.

There are two conflicting goals, performance and efficient memory
utilization..

  1. We will learn next chapter that large disk transfers achieve much higher total bandwidth than small transfers due to the comparatively large startup time required before any bytes are transferred. This favors a large block size.
  2. Internal fragmentation favors a small block size. This is especially true for small files, which would use only a tiny fraction of a large block 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
and thus waste much more than the 1/2 block average internal fragmentation found for random sizes. 

For some systems, the vast majority of the space used is consumed by the very
largest files. For example, it would be easy to have a few hundred gigabytes
of video. In that case the space efficiency of small files is largely
irrelevant since most of the disk space is used by very large files.

#### Keeping Track of Free Blocks

There are basically two possibilities, a bit map and a linked list.

##### Free Block Bitmap

A region of kernel memory is dedicated to keeping track of the free blocks.
One bit is assigned to each block of the file system. The bit is 1 if the
block is free.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

If the block size is 8KB the bitmap uses 1 bit for every 64K bits of disk
space. Thus a 64GB disk would require 1MB of RAM to hold its bitmap.

One can break the bitmap into (fixed size) pieces and apply demand paging.
This saves RAM at the cost of increased I/O.

##### Linked List of Free Blocks

A naive implementation would simply link the free blocks together and just
keep a pointer to the head of the list. Although it wastes no space, this
simple scheme has poor performance since it requires an I/O for every
acquisition or return of a free block.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![free-blocks](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/free-
blocks.png)

In the naive scheme a free disk block contains just one pointer; even though
it could hold a thousand pointers. The improved scheme, shown on the right,
has only a small number of the blocks on the list. Those blocks point not only
to the next block on the list, but also to many other free blocks that are not
directly on the list.

As a result only one in about 1000 requests for a free block requires an I/O,
a great improvement.

Unfortunately, a bad case still remains. Assume the head block on the list is
exhausted, i.e. points only to the next block on the list. A request for a

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
free block will receive this block, and the next one on the list is brought
it. It is full of pointers to free blocks not on the list (so far so good).

If a free block is now returned we repeat the process and get back to the in-
memory block being exhausted. This can repeat forever, with one extra I/O per
request.

Tanenbaum shows an improvement where you try to keep the one in-memory free
block half full of pointers. Similar considerations apply when splitting and
coalescing nodes in a B-tree.

#### Disk Quotas

Two limits can be placed on disk blocks owned by a given user, the so calle
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
d
soft and hard limits. A user is never permitted to exceed the hard limit. This
limitation is enforced by having system calls such as write return failure if
the user is already at the hard limit.

A user is permitted to exceed the soft limit during a login session provided
it is corrected prior to logout. This limitation is enforced by forbidding
logins (or issuing a warning) if the user is above the soft limit.

Often files on directories such as /tmp are not counted towards either limit
since the system is permitted to deleted these files when needed.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.4.2 File System Backups (a.k.a. Dumps)

A **physical backup** simply copies every block in order onto a tape or second
disk or the cloud (or other backup media). It is simple and useful for
disaster protection, but not useful for retrieving individual files.

We will study **logical backups**, i.e., dumps that are file and directory
based not simply block based.

Tanenbaum describes the (four phase) unix dump algorithm.

All modern systems support **full** and **incremental** dumps.

  * A level 0 dump is a called a full dump (i.e., dumps everything).
  * A level n dump (n>0) is called an incremental dump and the standard unix utility backs up all files that have changed since the most recent dump of level k<n.
  * Some other dump utilities dump all files that have changed since the most recent dump at level k≤n.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Assume you a level 0 dump (i.e. a full dump) is done yearly, a level 4 dump is done every sunday and a level 5 is done every monday-saturday (I personally use this scheme). Thursday's dump will have all files that changed since sunday and thus will be bigger than wednesday's dump. The other style dump will only dump the files that changed since wednesday and hence daily dumps will be about the same size.
  * Restoring a unix dump would require restoring the most recent level-0, level-4, and level-5. The other dump style would require the most recent level-0, **all** level-4s since the last level-0, and **all** level-5s since the last level-4.
  * The system keeps on the disk the dates of the most recent level i dumps for all i so that the dump program can determine which files need to be dumped for a level-k incremental. In Unix these dates are traditionally kept in the file /etc/dumpdates.
  * What about the nodump attribute? 
    * Default policy (for Linux at least) is to dump such files anyway when doing a full dump, but not dump them for incremental dumps.
    * Another way to say this is the nodump attribute is honored for level n dumps if n>0.
    * The dump command has an option to override the default policy (can sp
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ecify k so that nodump is honored for level n dumps if n>k). 

Traditionally, disks were dumped onto tapes since the latter were cheaper per
byte. Since tape densities are increasing slower than disk densities, an ever
larger number of tapes are needed to dump a disk. This has lead to disk-to-
disk dumps.

Another possibility is to utilize raid, which we study next chapter.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.4.3 File System Consistency

Modern systems have utility programs that check the consistency of a file
system. A different utility is needed for each file system type in the system,
but a wrapper program is often created so that the user is unaware of the
different utilities.

The unix utility is called fsck (file system check) and the windows utility is
called chkdsk (check disk).

  * If the system crashed, it is possible that not all metadata was written to disk. As a result the file system may be inconsistent. These programs check, and often correct, inconsistencies.
  * Scan all i-nodes (or fat) to check that each block is in exactly one file, or on the free list, but **not** both.
  * Also check that the number of links to each file (part of the metadata in the file's i-node) is correct (by looking at all directories).
  * Other checks as well.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * Offers to fix the errors found (for most errors).

#### Bad blocks on disks

Not so much of a problem now. Disks are more reliable and, more importantly,
disks and disk controllers take care most bad blocks themselves.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.4.4 File System Performance

#### Caching

Demand paging _again_!

Demand paging is a form of caching: Conceptually, the process resides on disk
(the big and slow medium) and only a portion of the process (hopefully a small
portion that is heavily access) resides in memory (the small and fast medium).

The same idea can be applied to files. The file resides on disk but a portion
is kept in memory. The area in memory used to for those file blocks is called
the **buffer cache** or **block cache**.

Some form of LRU replacement is used.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

The buffer cache is clearly good and simple for blocks that are only read.

What about writes?

  * The system must update the buffer cache if the file block is present (otherwise subsequent reads will return the old value).
  * If the file block is not present, then a cache block is allocated for it, which likely causes an old file block to be evicted. 

This decision to allocate a cache block for writes that are not present in the
cache is called a write-allocate policy Although no-write-allocate is possible
and sometimes used for memory caches, it performs poorly for disk caching.

  * The major question is whether, on a write to a block that already exists in the cache, the system should write the new value of the block to disk _in addition_ to updating the buffer cache.
  * The simplest alternative is **write through** in which each write is pe
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
rformed at the disk as well. 
    * Since floppy disk drivers adopt a write-through policy, one can remove a floppy as soon as an operation is complete.
    * Write through results in heavy I/O write traffic. 
      * If a block is written many times all the writes are sent the disk. Only the last one was needed.
      * If a temporary file is created, written, read, and deleted, _all_ the disk writes were wasted.
    * No modern system uses write-through for hard drives.
  * The other alternative is **write back**, also known as **copy-back**, in which the disk is **not** updated until the in-memory copy is evicted (i.e., at _replacement_ time). 
    * Write-back generates much less write traffic than write through. Hence all modern systems use write back for hard drives.
    * Trouble if a crash occurs or if the disk is removed before the write back completes (think of a removable flash drive).
    * The system is permitted to write dirty blocks back before they are evicted. It is common for a system to write back all dirty blocks about once a minute. This limits the possible damage, but also the possible gain.
    * Ordered writes. Do not write a block containing pointers until the bl
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ocks pointed to have been written. This is especially important if the block pointed to contains pointers since the version of these pointers on disk may be wrong and you are giving a file pointers to some random blocks. 

**Homework:** 32\. The performance of a file system depends upon the cache hit rate (fraction of blocks found in the cache. If it take 1 msec to satisfy a request from the cache, but 40 msec to satisfy a request if a disk read is needed, give a formula for the mean time required to satisfy a request if the hit rate is h. Plot this function for values of h ranging from mo to 1.

#### Block Read Ahead

When the access pattern looks sequential, read ahead is employed. This means
that after completing a read() request for block n of a file, the system
guesses that a read() request for block n+1 will shortly be issued and hence
automatically fetches block n+1.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * How does the system decide that the access pattern looks sequential? 
    * If a seek system call is issued, the access pattern is not sequential.
    * If a process issues consecutive read() system calls for block n-1 and then n, the access pattern is guessed to be sequential.
  * What if block n+1 is already in the block cache?  
Ans: Don't issue the read ahead.

  * Would it be reasonable to read ahead two or three blocks?  
Ans: Yes.

  * Would it be reasonable to read ahead the entire file?  
Ans: No, it could easily pollute the cache evicting needed blocks, and could
waste considerable disk bandwidth.

#### Reducing Disk Arm Motion

The idea is to try to place near each other blocks that are likely to be used

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
together.

  1. If the system uses a bitmap for the free list, it can allocate a new block for a file close to the previous block (guessing that the file will be accessed sequentially).
  2. The system can perform allocations in super-blocks, consisting of several contiguous blocks. 
    * The block cache and I/O requests are still in blocks not super-blocks.
    * If the file is accessed sequentially, consecutive blocks of a super-block will be accessed in sequence and these are contiguous on the disk.
  3. For a unix-like file system, the i-nodes can be placed in the middle of the disk, instead of at one end, to reduce the seek time needed to access an i-node followed by a block of the file.
  4. The system can logically divide the disk into **cylinder groups**, each of which is a consecutive group of cylinders. 
    * Each cylinder group has its own free list and, for a unix-like file system, its own space for i-nodes.
    * If possible, the blocks for a file are allocated in the same cylinder group as the i-node.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * This reduces seek time if consecutive accesses are for the same file. 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.4.5 Defragmenting Disks

If clustering is not done, files can become spread out all over the disk and a
utility (defrag on windows) can be run which makes files contiguous on the
disk.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 4.5 Example File Systems


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.5.A The CP/M File System

CP/M was a very early and simple OS. It ran on primitive hardware with very
little ram and disk space. CP/M had only one directory in the entire system.
The directory entry for a file contained pointers to the disk blocks of the
file. If the file contained more blocks than could fit in a directory entry, a
second entry was used.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.5.1 CD-ROM File Systems

File systems on cdroms do not need to support file addition or deletion and as
a result have no need for free blocks. A CD-R (recordable) does permit files
to be added, but they are always added at the end of the disk. The space
allocated to a file is not recovered even when the file is deleted, so the
(implicit) free list is simply the blocks after the last file recorded.

The result is that the file systems for these devices are quite simple.

#### The ISO9660 File System

This international standard forms the basis for essentially all file systems
on data cdroms (music cdroms are different and are not discussed). Most Unix
systems use iso9660 with the Rock Ridge extensions, and most windows systems

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
use iso9660 with the Joliet extensions.

The ISO9660 standard permits a single physical CD to be partitioned and
permits a cdrom file system to span many physical CDs. However, these features
are rarely used and we will not discuss them.

Since files do not change, they are stored contiguously and each directory
entry need only give the starting location and file length.

File names are 8+3 characters (directory names just 8) for iso9660-level-1 and
31 characters for -level-2. There is also a -level-3 in which a file is
composed of extents which can be shared among files and even shared within a
single file (i.e. a single physical extent can occur multiple times in a given
file).

Directories can be nested only 8 deep.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

#### Rock Ridge Extensions

The Rock Ridge extensions were designed by a committee from the unix community
to permit a unix file system to be copied to a cdrom without information loss.

These extensions included.

  1. The unix rwx bits for permissions.
  2. Major and Minor numbers to support special files, i.e. including devices in the file system name structure.
  3. Symbolic links.
  4. An alternate (long) name for files and directories.
  5. A somewhat kludgy work around for the limited directory nesting levels.
  6. Unix timestamps (creation, last access, last modification).

#### Joliet Extensions

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

The Joliet extensions were designed by Microsoft to permit a windows file
system to be copied to a cdrom without information loss.

These extensions included.

  1. Long file names.
  2. Unicode.
  3. Arbitrary depth of directory nesting.
  4. Directory names with extensions.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.5.2 The MS-DOS (and Windows) FAT File System

We discussed this linked-list, File-Allocation-Table-based file system
previously. Here we add a little history. More details can be found in [ this
lecture ](http://www.c-jump.com/CIS24/Slides/FAT/lecture.html) from Igor
Kholodov. (A local copy is [here).](kholodov-fat.html)

#### MS-DOS and Early Windows

The FAT file system has been supported since the first IBM PC (1981) and is
still widely used. Indeed, considering the number of cameras, MP3 players, and
smart phones, it is very widely used.

Unlike CP/M, MS-DOS always had support for subdirectories and metadata such as
date and size.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
File names were restricted in length to 8+3.

As described previously, the directory entries point to the first block of
each file and the FAT contains pointers to the remaining blocks.

The free list was supported by using a special code in the FAT for free
blocks. You can think of this as a bitmap with a wide bit.

The first version FAT-12 used 12-bit block numbers so a partition could not
exceed 212 blocks. A subsequent release went to FAT-16.

#### The Windows 98 File System

Two changes were made: Long file names were supported and the file allocation
table was switched from FAT-16 to FAT-32. These changes first appeared in the
second release of Windows 95.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
##### Long File Names

The hard part of supporting long names was keeping compatibility with the old
8+3 naming rule. That is, new file systems created with windows 98 using long
file names must be accessible if the file system is subsequently used with an
older version of windows that supported only 8+3 file names. The ability for
old systems to read data from new systems was important since users often had
both new and old systems and kept many files on floppy disks that were used on
both systems.

This abiliity to access new objects on old systems is called backwards
compatibility and is not always achieved. For example files produced by new
versions of microsoft word many not be fully comprehended by old versions o
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
f
word. The reverse capability, the ability to read old files on new systems, is
much easier to accomplish and is almost always achieved. For example, newer
versions of microsoft word could always read documents produced by older
versions.

Backwards compatibility of Windows file names was achieved by permitting a
file to have two names: a long one and an 8+3 one. The primary directory entry
for a file in windows 98 is the same format as it was in MS-DOS and contains
the 8+3 file name. If the long name fits the 8+3 format, the story ends here.

If the long name does not fit in 8+3, an (often ugly) 8+3 alternate name is
produced and stored in the normal location. The long name is stored in one or

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
more axillary directory entries adjacent to the main entry. These axillary
entries are set up to appear invalid to the old OS, which (surprisingly)
ignores them.

##### FAT-32

FAT-32 used 32 bit words for the block numbers (actually, it used 28 bits) so
the FAT could be huge (228 entries). Windows 98 kept only a portion of the
FAT-32 table in memory at a time, a form of caching / demand-paging.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 4.5.3 The Unix V7 File System

I presented the inode system in some detail above. Here we just describe a few
properties of the filesystem beyond the inode structure itself.

  * Each directory entry contains a name and a pointer to the corresponding i-node.
  * The metadata for a file or directory is stored in the corresponding inode.
  * Early unix limited file names to 14 characters, stored in a fixed length field.
  * The name field now is of varying length and file names can be quite long. On my linux system 
    
              touch 255-char-name
        

is OK but


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    
              touch 256-char-name
        

is not.

  * To go down a level in the directory hierarchy takes two steps: get the inode, get the file (or subdirectory).
  * This shows how important it is not to parse filenames for each I/O operation, i.e., why the open() system call is important.
  * Do on the blackboard the steps for /a/b/X.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 4.6 Research on File Systems


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 4.6 Summary

Read

**Remark**: A practice final is on the web site.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
# Chapter 5 Input/Output


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 5.1 Principles of I/O Hardware


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.1.1 I/O Devices

The most noticeable characteristic of the current ensemble of I/O devices is
their great diversity.

  * Some, e.g. disks, transmit megabytes per second; others, like keyboards, don't transmit a megabyte in their lifetime.
  * Some, like ethernet, are purely electronic; others, like disks are mechanical marvels.
  * A mouse can be put in your pocket; a high-speed printer needs at least two people to move it.
  * Some devices, e.g. a keyboard, are input only. But note that this is in some sense a crazy classification since a keyboard produces output, which is sent to the computer, and receives very little input from the computer (mostly to turn on/off a few lights). Really a keyboard is a transducer, taking mechanical input from a human and producing electronic output for a computer.
  * Similarly, an output only device such as a printer supplies very little output to the computer (perhaps an out&hyphen;of&hyphen;paper indication) 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
but receives voluminous input from the computer. Again it is better thought of as a transducer, converting electronic data from the computer to paper data for humans.
  * Many storage devices are input/output, but again the words can be funny. A disk is viewed as an input device when it is being read, i.e., when it is outputting data; it is viewed as an output device when it is inputting data.
  * Often devices are characterized as **block devices** or as **character devices**. The distinction being that devices like disks are read/written in blocks and individual blocks can be addressed (i.e., not all accesses are sequential). An ethernet interface or a printer has no notion of block or addresses. Instead, it just deals with a stream of characters.
  * However, the block/character device distinction is fuzzy. What about tapes? They read/write blocks and are sort of block addressable (rewind then skip forward). Clocks are weird and hard to classify; they simply generate periodic interrupts. 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.1.2 Device Controllers

To perform I/O typically requires both mechanical and electronic activity. The
mechanical component is called the device itself. The electronic component is
called a _controller_ or an _adapter_.

The controllers _are_ the devices as far as the OS is concerned. That is, the
OS code is written with the controller specification in hand not with the
device specification.

  * A controller abstracts away some of the low level features of the devices it controls. For example, a disk controller performs error checking and assembles the bit stream coming off the disk into blocks of bytes.
  * Graphics controllers do a great deal. They often contain processors at least as powerful as the CPU on which the OS and applications run.
  * In the old days controllers handled interleaving of sectors. Sectors are interleaved if the controller or CPU cannot handle the data rate and woul
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
d otherwise have to wait a full revolution between sectors. This is not a concern with modern systems since the electronics have increased in speed faster than the devices and now all disk controllers can handle the full data rate of the disks they support.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
###  5.1.3 Memory-Mapped I/O (vs I/O Space Instructions)

Consider a disk controller processing a read request. The goal is to copy data
from the disk to some portion of the central memory. How is this to be
accomplished?

The controller contains a microprocessor and memory, and is connected to the
disk (by wires). When the controller requests a sector from the disk, the
sector is transmitted to the control via the wires and is stored by the
controller in **its** memory.

The separate processor and memory on the controller gives rise to two
questions.

  1. How does the OS request that the controller, which is running on another processor, perform an I/O and how are the parameters of the request transmitted to the controller?
  2. How is the data that has been read from the disk moved from the contro
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ller's memory to the general system memory? Similarly, how is data that is to be written to the disk moved from the system memory to the controller's memory?

Typically the interface the controller presents to the OS consists of a few
registers located on the controller board.

  * These are memory locations into which the OS writes information such as the sector to access, read vs. write, length, where in system memory to put the data (for a read) or from where to take the data (for a write). The controller reads these registers.
  * There are also devices registers that the OS reads and the controller writes, such as the status of the controller, any errors that were detected, etc.
  * There is also typically a device register that acts as a go button.

So the first question above becomes, how does the OS read and write the device
register?

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

  * With **memory-mapped I/O** the device registers appear as normal memory. All that is needed is to know at which address each device register appears. Then the OS uses normal load and store instructions to read and write the registers.
  * Some systems instead have a special I/O space into which the registers are mapped. In this case special I/O space instructions are used to accomplish the loads and stores.
  * From a conceptual point of view there is no difference between the two models, but the implementations differ 
    1. Memory-mapped I/O is a more elegant solution in that it uses an existing mechanism to accomplish a second objective.
    2. Since normal loads and stores are used for memory-mapped I/O, the algorithm can be written in a high-level language. Assembly language is needed for I/O space instructions since such instructions cannot be expressed in (normal) high-level languages.
    3. A memory-mapped implementation must arrange that these addresses are sent to the appropriate bus and must insure that they are not cached.
  * The remainder of these notes are written assuming a memory mapped implementation.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![dma](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/dma.png)


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.1.4 Direct Memory Access (DMA)

We now address the second question, moving data between the controller and the
main memory. Recall that the disk controller, when processing a read request,
pulls the desired data from the disk to its own buffer. Similarly, it pushes
data from the buffer to the disk when processing a write).

Without DMA, i.e., with **programmed I/O** (PIO), the read is completed by
having the cpu issue loads and stores (assuming the controller buffer is
memory mapped, or uses I/O instructions if it is not) to copy the data from
the buffer to the desired memory locations.

With DMA the controller writes the main memory itself, without intervention of
the CPU.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Clearly DMA saves CPU work. However, the number of memory references does not
decrease so the CPU may be delayed due to busy memory.

An important point is that there is less data movement with DMA so the buses
are used less and the entire operation takes less time. Compare the two blue
arrows vs. the single red arrow.

Since PIO is pure software it is easier to change, which is an advantage.

Initiating DMA requires a number of bus transfers from the CPU to the
controller to write the device registers. So DMA is most effective for large
transfers where the setup is amortized.

A serious complexity of DMA is that the bus must support multiple masters and
hence requires _arbitration_, which leads to issues similar to those we fac
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ed
with critical sections.

Why have the buffer? Why not just go from the disk straight to the memory?

  1. Speed matching.  
The disk supplies data at a fixed rate, which might exceed the rate the memory
can accept it. In particular the memory might be busy servicing a request from
the processor or from another DMA controller.  
Alternatively, the disk might supply data at a slower rate than the memory
(and memory bus) can handle thus under-utilizing an important system resource.

  2. Error detection and correction.  
The disk controller verifies the checksum written on the disk.

**Homework:** 15\. A local area network is used as follows. The user issues a system call to write data packets to the network. The operating system t
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
hen copies the data to a kernel buffer. Then it copies the data to the network controller board. When all the bytes are safely inside the controller, they are sent over the network at a rate of 10 megabits/sec. The receiving network controller stores each bin a microsecond after it is sent. When the last bit arrives, the destination CPU is interrupted, and the kernel copies the newly arrived packet to a kernel buffer to inspect it. Once it has figured out which user the packet is for, the kernel copies the data to the user space. If we assume that each interrupt and its associated processing takes 1 msec, that packets are 1024 bytes (ignore the headers), and that copying a byte takes 1 microsecond, what is the maximum rate at which one process can pump data to another? Assume that the sender is blocked until the work is finished at the receiving side and acknowledegment comes back. For simplicity, assume that the time to get the acknowledgement back is so small it can be ignored.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.1.5 Interrupts Revisited

#### Precise and Imprecise Interrupts


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 5.2 Principles of I/O Software

As with any large software system, good design and layering is important.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.2.1 Goals of the I/O Software

#### Device Independence

We want to have most of the OS to be unaware of the characteristics of the
specific devices attached to the system. (This principle of device
independence is not limited to I/O; we also want the OS to be largely unaware
of the specific CPU employed.)

This objective has been accomplished quite well for files stored on various
devices. Most of the OS, including the file system code, and most applications
can read or write a file without knowing if the file is stored on an internal
SATA hard disk, an external USB SCSI disk, an external USB Flash Ram, a tape,
or (for read-only applications) a CD-ROM.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
This principle also applies for user programs reading or writing streams. A
program reading from standard input, which is normally the user's keyboard can
be told to instead read from a disk file with no change to the application
program. Similarly, standard output can be redirected to a disk file. However,
the low-level OS code dealing with disks is rather different from that dealing
keyboards and (character-oriented) terminals.

One can say that device independence permits programs to be implemented as if
they will read and write generic or abstract devices, with the actual devices
specified at run time. Although writing to a disk has differences from writing
to a terminal, Unix **cp**, DOS **copy**, and many programs we compose need
not be aware of these differences.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

However, there are devices that really are special. The graphics interface to
a monitor (that is, the graphics interface presented by the video controller--
often called a ``video card'') does not resemble the ``stream of bytes'' we
see for disk files.

**Homework:** What is device independence?

#### Uniform naming

We have already discussed the value of the name space implemented by file
systems. There is no dependence between the name of the file and the device on
which it is stored. So a file called IAmStoredOnAHardDisk might well be stored
on a floppy disk.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
More interesting once a device is mounted on (Unix) directory, the device is
named exactly the same as the directory was. So if a CD-ROM was mounted on
(existing) directory /x/y, a file named joe on the CD-ROM would now be
accessible as /x/y/joe.

#### Error handling

There are several aspects to error handling including: detection, correction
(if possible) and reporting.

  1. Detection should be done as close to where the error occurred as possible before more damage is done (fault containment). Moreover, the error may be obvious at the low level, but harder to discover and classify if the erroneous data is passed to higher level software.
  2. Correction is sometimes easy, for example ECC memory does this automatically (but the OS wants to know about the error so that it can request replacement of the faulty chips before unrecoverable double errors occur). Other easy cases include successful retries for failed ethernet transmissions.
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
 In this example, while logging is appropriate, it is quite reasonable for no action to be taken.
  3. Error reporting tends to be awful. The trouble is that the error occurs at a low level but by the time it is reported the context is lost.

#### Creating the illusion of synchronous I/O

I/O _must_ be asynchronous for good performance. That is the OS _cannot_
simply wait for an I/O to complete. Instead, it proceeds with other activities
and responds to the interrupt that is generated when the I/O has finished.

    
    
      Read X
      Y = X+1
      Print Y
    

Users (mostly) want no part of this. The code sequence on the right should

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
print a value one greater than that read. But if the assignment is performed
before the read completes, the wrong value can easily be printed.

Performance junkies sometimes _do_ want the asynchrony so that they can have
another portion of their program executed while the I/O is underway. That is,
they implement a mini-scheduler in their application code.

See [this message](asyncIO-ingo) from linux kernel developer Ingo Molnar for
his take on asynchronous IO and kernel/user threads/processes. You can find
the entire discussion [ here.
](http://www.ussg.iu.edu/hypermail/linux/kernel/0701.3/2371.html)

#### Buffering

Buffering is often needed to hold data for examination prior to sending it 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
to
its desired destination.

When two buffers are used the producer can deposit more data while the old
data is being consumed. (Recall the bounded buffer a.k.a. producer-consumer
problem).

Since this involves copying the data, which can be expensive, modern systems
try to avoid as much buffering as possible. This is especially noticeable in
network transmissions, where the data could conceivably be copied _many_
times.

  1. From user space to kernel space as part of the write system call.
  2. From kernel space to a kernel I/O buffer.
  3. From the I/O buffer to a buffer on the network adaptor.
  4. From the adapter on the source to the adapter on the destination.
  5. From the destination adapter to an I/O buffer.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  6. From the I/O buffer to kernel space.
  7. From kernel space to user space as part of the read system call.

I am not sure if any systems actually do all seven.

#### Sharable vs. Dedicated Devices

For devices like printers and CD-ROM drives, only one user at a time is
permitted. These are called **serially reusable** devices, which we studied in
the deadlocks chapter. Devices such as disks and ethernet ports can, on the
contrary, be shared by concurrent processes without any deadlock risk.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.2.2 Programmed I/O

As mentioned just above, with programmed I/O the main processor (i.e., the one
on which the OS runs) moves the data between memory and the device. This is
the most straightforward method for performing I/O.

One question that arises is, How does the processor know when the device is
ready to accept or supply new data?.

    
    
      while (device-not-available)
        do-useful-work
    

The simplest implementation is shown on the right. The processor, when it
seeks to use a device, loops continually querying the device status, until 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
the
device reports that it is free. This is called **polling** or **busy
waiting**.

If we poll infrequently (and do useful work in between), there can be a
significant delay from when the previous I/O is complete to when the OS
detects the device availability.

If we poll frequently (and thus are able to do little useful work in between)
and the device is (sometimes) slow, polling is clearly wasteful.

The extreme case is where the process does nothing between polls. For a slow
device this can take the CPU out of service for a significant period. This bad
situation leads us to ... .


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.2.3 Interrupt-Driven (Programmed) I/O

As we have just seen, a difficulty with polling is determining the frequency
with which to poll. Another problem is that the OS must continually return to
the polling loop, i.e., we must arrange that do-useful-work takes the desired
amount of time. Really we want the device to tell the CPU when it is
available, which is exactly what an interrupt does.

The device interrupts the processor when it is ready and an interrupt handler
(a.k.a. an interrupt service routine) then initiates transfer of the next
datum.

Normally interrupt schemes perform better than polling, but not always since
interrupts are expensive on modern machines. To minimize interrupts, better

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
controllers often employ ...


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.2.4 I/O Using DMA

We discussed DMA above.

An additional advantage of dma, not mentioned above, is that the processor is
interrupted only at the end of a command not after each datum is transferred.
Many devices receive a character at a time, but with a dma controller, an
interrupt occurs only after a buffer has been transferred.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 5.3 I/O Software Layers

Layers of abstraction as usual prove to be effective. Most systems are
believed to use the following layers (but for many systems, the OS code is not
available for inspection).

  1. User-level I/O routines.
  2. Device-independent (kernel-level) I/O software.
  3. Device drivers.
  4. Interrupt handlers.

We will give a bottom up explanation.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.3.1 Interrupt Handlers

We discussed behavior similar to an interrupt handler before when studying
page faults. Then it was called assembly-language code. A difference is that
page faults are caused by specific user instructions, whereas interrupts just
happen. However, the assembly-language code for a page fault accomplishes
essentially the same task as the interrupt handler does for I/O.

In the present case, we have a process blocked on I/O and the I/O event has
just completed. So the goal is to make the process ready and then call the
scheduler. Possible methods are.

  * Releasing a semaphore on which the process is waiting.
  * Sending a message to the process.
  * Inserting the process table entry onto the ready list.

Once the process is ready, it is up to the scheduler to decide when it shou
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ld
run.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.3.2 Device Drivers

We gave an overview before.

Device drivers form the portion of the OS that is tailored to the
characteristics of individual controllers. They form the dominant portion of
the source code of the OS since there are hundreds of drivers. Normally some
mechanism is used so that the only drivers loaded on a given system are those
corresponding to hardware actually present.

Indeed, modern systems often have loadable device drivers, which are loaded
dynamically when needed. This way if a user buys a new device, no changes to
the operating system are needed. When the device is installed it will be
detected during the boot process and the corresponding driver is loaded.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![components](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/components.pn
g)

Sometimes an even fancier method is used and the device can be plugged in
while the system is running (USB devices are like this). In this case it is
the device insertion that is detected by the OS and that causes the driver to
be loaded.

Some systems can dynamically **un**load a driver, when the corresponding
device is unplugged.

The driver has two parts corresponding to its two access points. The figure on
the right, which we first encountered at the beginning of the course, helps
explain the two parts and their names.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
The driver is accessed by the main line OS via the envelope in response to an
I/O system call. The portion of the driver accessed in this way is sometimes
called the top part.

The driver is also accessed by the interrupt handler when the I/O completes
(this completion is signaled by an interrupt). The portion of the driver
accessed in this way is sometimes called the bottom part.

In some system the drivers are implemented as user-mode processes. Indeed,
Tannenbaum's MINIX system works that way, and in previous editions of the
text, he describes such a scheme. However, most systems have the drivers in
the kernel itself and the 3e describes this scheme. I previously included both
descriptions, but have eliminated the user-mode process description (actually
I greyed it out).

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

![unblocking-1](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/unblocking-
1.png)

#### Driver in the Kernel

The three-part diagram to the right and below shows the high-level actions
that occur. On the right we see the initial state, process A is running and is

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
about it issue a read system call. Process B is ready to run; it is waiting to
be scheduled. (Although only A and B are shown, there may be many other
processes ready and running as well.)

Below we see later states. The second diagram shows the situation after
process A has issued its read system call. The process is now blocked waiting
for the read to complete. The scheduler has chosen to run process B. In the
third diagram, the read is complete and process A is now ready. Perhaps the
scheduler will run it soon.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![unblocking-2-3](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/unblockin
g-2-3.png)  
  

##### A Detailed view

What follows is the Unix-like view in which the driver is invoked by the OS
acting in behalf of a user process (alternatively stated, the process shifts
into kernel mode). Thus one says that the scheme follows a self-service
paradigm in that the process itself (now in kernel mode) executes the driver.

The numbers in the diagram to the right correspond to the numbered steps in
the description that follows. The previous diagram showed the state of
processes A and B at steps 1, 6, and 9 in the execution sequence.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
![driver](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/driver.png)

  1. The currently running process (say A) issues an I/O system call.
  2. The main line, machine independent, OS prepares a generic request and calls (the top part of) the driver. 
    1. If the driver was idle (i.e., the controller was idle), the driver writes device registers on the controller ending with a command for the controller to begin the actual I/O (the go button mentioned previously).
    2. If the controller was busy (doing work the driver gave it previously), the driver simply queues the current request (the driver dequeues this request below).
  3. The driver jumps to the scheduler indicating that the current process should be blocked.
  4. The scheduler blocks A and runs (say) B.
  5. B starts running; eventually an interrupt occurs (the I/O for A has completed).
  6. The interrupt handler is invoked.
  7. The interrupt handler invokes (the bottom part of) the driver. 
    1. The driver stores information concerning the I/O just performed for process A. The information might include data read and surely includes stat
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
us (error, OK).
    2. The top part is called to start another I/O if the queue is nonempty. We know the controller is free. Why?  
Answer: We just received an interrupt saying so.

  8. The driver jumps to the scheduler indicating that process A should be made ready.
  9. The scheduler picks a ready process to run. Assume it picks A.
  10. A resumes in the driver, which returns to the main line, which returns to the user code.

#### Driver as a Process (Less Detailed Than Above)

Actions that occur when the user issues an I/O request.

  1. The main line OS prepares a generic request (e.g. read, not read using Buslogic BT-958 SCSI controller) for the driver and the driver is awakened. Perhaps a message is sent to the driver to do both jobs.
  2. The driver wakes up. 
    1. If the driver was idle (i.e., the controller is idle), the driver wr
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ites device registers on the controller ending with a command for the controller to begin the actual I/O.
    2. If the controller is busy (doing work the driver gave it), the driver simply queues the current request (the driver dequeues this below). 
  3. The driver blocks waiting for an interrupt or for more requests. 

Actions that occur when an interrupt arrives (i.e., when an I/O has been
completed).

  1. The driver wakes up.
  2. The driver informs the main line perhaps passing data and surely passing status (error, OK).
  3. The driver finds the next work item or blocks. 
    1. If the queue of requests is non-empty, dequeue one and proceed as if just received a request from the main line.
    2. If queue is empty, the driver blocks waiting for an interrupt or a request from the main line. 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.3.3 Device-Independent I/O Software

The device-independent code cantains most of the I/O functionality, but not
most of the code since there are _very many_ drivers. All drivers of the same
class (say all hard disk drivers) do essentially the same thing in slightly
different ways due to slightly different controllers.

#### Uniform Interfacing for Device Drivers

As stated above the bulk of the OS code is made of device drivers and thus it
is important that the task of driver writing not be made more difficult than
needed. As a result each class of devices (e.g. the class of all disks) has a
defined driver interface to which all drivers for that class of device
conform. The device independent I/O portion processes user requests and cal
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ls
the drivers.

#### Naming

Naming is again an important O/S functionality. In addition it offers a
consistent interface to the drivers. The Unix method works as follows

  * Each device is associated with a special file in the /dev directory.
  * The i-nodes for these files contain an indication that these are special files and also contain so called major and minor device numbers.
  * The major device number gives the number of the driver. (These numbers are rather ad hoc, they correspond to the position of the function pointer to the driver in a table of function pointers.)
  * The minor number indicates for which device (e.g., which scsi cdrom drive) the request is intended.
  * For example my office workstation has two scsi disks (one is external via USB, but that is not relevant). The two disks are named by linux sda and sdb. The partitions of sda are named sda1, sda2, etc. From the following listing we can see that the scsi driver is number 8 and that numbers are res
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
erved for 15 partitions for each scsi drive, which is the limit scsi supports. The result is as follows. 
    
              allan dev # ls -l /dev/sd*
          brw-r----- 1 root disk 8,  0 Apr 25 09:55 /dev/sda
          brw-r----- 1 root disk 8,  1 Apr 25 09:55 /dev/sda1
          brw-r----- 1 root disk 8,  2 Apr 25 09:55 /dev/sda2
          brw-r----- 1 root disk 8,  3 Apr 25 09:55 /dev/sda3
          brw-r----- 1 root disk 8,  4 Apr 25 09:55 /dev/sda4
          brw-r----- 1 root disk 8,  5 Apr 25 09:55 /dev/sda5
          brw-r----- 1 root disk 8,  6 Apr 25 09:55 /dev/sda6
          brw-r----- 1 root disk 8, 16 Apr 25 09:55 /dev/sdb
          brw-r----- 1 root disk 8, 17 Apr 25 09:55 /dev/sdb1
          brw-r----- 1 root disk 8, 18 Apr 25 09:55 /dev/sdb2
          brw-r----- 1 root disk 8, 19 Apr 25 09:55 /dev/sdb3
          brw-r----- 1 root disk 8, 20 Apr 25 09:55 /dev/sdb4
          allan dev #
        

#### Protection

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

A wide range of possibilities are actually done in real systems. Including
both extreme examples of everything is permitted and nothing is (directly)
permitted.

  * In ms-dos any process can write to any file. Presumably, our offensive nuclear missile launchers never ran dos.
  * In IBM 360/370/390 mainframe OS's, normal processors do not access devices. Indeed the main CPU doesn't issue the I/O requests. Instead an I/O channel is used and the mainline constructs a channel program and tells the channel to invoke it.
  * Unix uses normal rwx bits on files in /dev (I don't believe x is used). 

#### Buffering

Buffering is necessary since requests come in a sizes specified by the user
and data is delivered by reads and accepted by writes in sizes specified by

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
the device. Buffering is also important so that a user process using getchar()
in C or the Scanner in java is not blocked and unblocked for each character
read.

The text describes double buffering and circular buffers, which are important
programming techniques, but are not specific to operating systems.

#### Error Reporting

#### Allocating and Releasing Dedicated Devices

The system must enforce **exclusive access** for non-shared devices like CD-
ROMs. We discussed the issues involved when studying deadlocks.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.3.4 User-Space Software

A good deal of I/O software is actually executed by unprivileged code running
in user space. This code includes library routines linked into user programs,
standard utilities, and daemon processes.

If one uses the strict definition that the operating system consists of the
(supervisor-mode) kernel, then this I/O code is not part of the OS. However,
very few use this strict definition.

#### Library Routines

Some library routines are very simple and just move their arguments into the
correct place (e.g., a specific register) and then issue a trap to the kernel

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
to do the real work.

I think everyone considers these routines to be part of the operating system.
Indeed, they implement the published user interface to the OS. For example,
when we specify the (Unix) read system call by  
`   count = read (fd, buffer, nbytes)  
` as we did in chapter 1, we are really giving the parameters and return value
of such a library routine.

Although users could write these routines, it would make their programs non-
portable and would require them to write in assembly language since neither
trap nor specifying individual registers is available in high-level languages.

Other library routines, notably standard I/O (stdio) in Unix, are definitel
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
y
not trivial. For example consider the formatting of floating point numbers
done in System.out.printf() and the reverse operation done by the Scanner in
nextInt().

In unix-like systems the graphics libraries and the gui itself are outside the
kernel. Graphics libraries are quite large and complex. In windows, the gui is
inside the kernel.

#### Utilities and Daemons

Printing to a local printer is often performed in part by a regular program
(lpr in Unix) that copies (or links) the file to a standard place, and in part
by a daemon (lpd in Unix) that reads the copied files and sends them to the

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
printer. The daemon might be started when the system boots.

Note that this implementation of printing uses **spooling**, i.e., the file to
be printed is copied somewhere by lpr and then the daemon works with this
copy. Mail uses a similar technique (but generally it is called queuing, not
spooling).


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.3.A Summary

![IO layers](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/IO-layers.png)

The diagram on the right shows the various layers and some of the actions that
are performed by each layer.

The arrows show the flow of control. The blue downward arrows show the

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
execution path made by a request from user space eventually reaching the
device itself. The red upward arrows show the response, beginning with the
device supplying the result for an input request (or a completion
acknowledgement for an output request) and ending with the initiating user
process receiving its response.

**Homework:** 14\. In which of the four I/O software layers is each of the following done.

  1. Computing the track, sector ahd head for a disk read.
  2. Writing commands to the device registers.
  3. Checking to see if the user is permitted to use the device.
  4. Converting binary integers to ASCII for printing.

**Homework:** 16\. Why are output files for the printer normally spooled on disk before being printed?


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 5.4 Disks

The ideal storage device is

  1. Fast
  2. Big (in capacity)
  3. Cheap
  4. Impossible

When compared to central memory, disks are big and cheap, but slow.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.4.1 Disk Hardware

#### Magnetic Disks (Hard Drives)

Show a real disk opened up and illustrate the components.

  * Platter
  * Surface
  * Head
  * Track
  * Sector
  * Cylinder
  * Seek time
  * Rotational latency
  * Transfer rate

Consider the following characteristics of a disk.

  * RPM (revolutions per minute).
  * Seek time. This is actually quite complicated to calculate since you ha
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ve to worry about, acceleration, travel time, deceleration, and "settling time".
  * Rotational latency. The average value is the time for (approximately) one half a revolution.
  * Transfer rate. This is determined by the RPM and bit density.
  * Sectors per track. This is determined by the bit density.
  * Tracks per surface (i.e., the number of cylinders). This is determined by the bit density.
  * Tracks per cylinder (i.e, the number of surfaces).

Overlapping I/O operations is important when the system has more than one
disk. Many disk controllers can do overlapped seeks, i.e. issue a seek to one
disk while another disk is already seeking.

As technology improves the space taken to store a bit decreases, i.e., the bit
density increases. This changes the number of cylinders per inch of radius
(the cylinders are closer together) and the number of bits per inch along a

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
given track.

Despite what Tanenbaum says later, it is not true that when one head is
reading from cylinder C, all the heads can read from cylinder C with no
penalty. It is, however, true that the penalty is very small.

#### Choice of Block Size

Current commodity disks for commodity computers require a little less than
10ms. before transferring the first byte and then transfer roughly 100K bytes
per ms. (if contiguous). Specifically

  * The rotation rate is normally 7200 RPM with 10k, 15k and 20k available.
  * Recall that 6000 RPM is 100 rev/sec or one rev per 10ms. So half a revolution (the average rotation needed to reach a given point) is less than 5ms.
  * Transfer rates are around 100MB/sec = 100KB/ms.
  * Seek time is around 5ms.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

This is quite extraordinary. For a large **sequential** transfer, in the first
10ms, no bytes are transmitted; in the next 10ms, 1,000,000 bytes are
transmitted. The analysis suggests using large disk blocks, 100KB or more.

But the internal fragmentation would be severe since many files are small.
Moreover, transferring small files would take longer with a 100KB block size.

In practice typical block sizes are 4KB-8KB.

Multiple block sizes have been tried (e.g., blocks are 8KB but a file can also
have fragments that are a fraction of a block, say 1KB).

Some systems employ techniques to encourage consecutive blocks of a given file
to be stored near each other. In the best case, logically sequential blocks

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
are also physically sequential and then the performance advantage of large
block sizes is obtained without the disadvantages mentioned.

In a similar vein, some systems try to cluster related files (e.g., files in
the same directory).

**Homework:** Consider a disk with an average seek time of 5ms, an average rotational latency of 5ms, and a transfer rate of 40MB/sec.

  1. If the block size is 1KB, how long would it take to read a block?
  2. If the block size is 100KB, how long would it take to read a block?
  3. If the goal is to read 1K, a 1KB block size is better as the remaining 99KB are wasted. If the goal is to read 100KB, the 100KB block size is better since the 1KB block size needs 100 seeks and 100 rotational latencies. What is the minimum size request for which a disk with a 100KB block size would complete faster than one with a 1KB block size? 

#### Virtual Geometry and LBA (Logical Block Addressing)


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Originally, a disk was implemented as a three dimensional array  
      Cylinder#, Head#, Sector#  
The cylinder number determined the cylinder, the head number specified the
surface (recall that there is one head per surface), i.e., the head number
determined the track within the cylinder, and the sector number determined the
sector within the track.

But there is something wrong here. An outer track is longer (in centimeters)
than an inner track, but each stores the same number of sectors. Essentially
some space on the outer tracks was wasted.

Later disks lied. They said they had a virtual geometry as above, but really
had more sectors on outer tracks (like a ragged array). The electronics on the
disk converted between the published virtual geometry and the real geometry.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

Modern disk continue to lie for backwards compatibility, but also support
Logical Block Addressing in which the sectors are treated as a simple one
dimensional array with no notion of cylinders and heads.

#### RAID (Redundant Array of Inexpensive Disks)

The name and its acronym RAID came from Dave Patterson's group at Berkeley.
IBM changed the name to Redundant Array of _Independent_ Disks. I wonder why?

The basic idea is to utilize multiple drives to simulate a single larger
drive, but with redundancy and increased performance.

The different RAID configurations are often called different levels, but this
is not a good name since there is no hierarchy and it is not clear that higher
levels are better than low ones. However, the terminology is commonly used 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
so
I will follow the trend and describe them level by level, but having very
little to say about some levels.

  0. Striping.  
Consecutive blocks are interleaved across the multiple drives. The is no
redundancy so it is strange to have it called RAID, but it is. Recall that a
block may consist of multiple sectors.

  1. Mirroring.  
The next level simply replicates the previous one. That is, the number of
drives is doubled and two copies of each block are written, one in each of the
two replicas. A read may access either replica. One might think that both
replicas are read and compared, but this is not done, the drives themselves
have check bits. The reason for having two replicas is to survive a single
disk failure. In addition, read time is improved since the heads on one set of

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
drives may be closer to the desired block.

  2. Synchronized disks, bit interleaved, multiple Hamming checksum disks.  
I don't believe this scheme is currently used.

  3. Synchronized disks, bit interleaved, single parity disk.  
I don't believe this scheme is currently used.

  4. Striping plus a parity disk.  
Use N (say 4) data disks and one parity disk. Data is striped across the data
disks and the bitwise parity of these blocks is written in the corresponding
block of the parity disk.

    * On a read, if the block is bad (e.g., if the entire disk is bad or even missing), the system automatically reads the other blocks in the stripe and the parity block in the stripe. Then the missing block is just the bitwise exclusive or (aka XOR) of all these blocks.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * For reads this is very good. The failure free case has no penalty (beyond the space overhead of the parity disk). The error case requires (N-1)+1=N (say 4) reads.
    * A serious concern is the small write problem. Writing a sector requires 4 I/Os: Read the old data sector, compute the change, read the parity, compute the new parity, write the new parity and the new data sector. Hence one sector I/O became 4, which is a 300% penalty. Writing a full stripe is not bad. Compute the parity of the N (say 4) data sectors to be written and then write the data sectors and the parity sector. Thus 4 sector I/Os become 5, which is only a 25% penalty and is smaller for larger N, i.e., larger stripes.
  5. Rotated parity.  
That is, for some stripes, disk 1 has the parity block; for others stripes,
disk 2 has the parity; etc. The purpose is to avoid having a single parity
disk since that disk is needed for all small writes and could easily become a
point of contention.

  6. Additional parity blocks.  

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
Able to survive multiple faults. In particular, can survive a fault while
rebuilding from a fault.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.4.2 Disk Formatting


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.4.3 Disk Arm Scheduling Algorithms

There are three components to disk response time: seek, rotational latency,
and transfer time. Disk arm scheduling is concerned with minimizing seek time
by reordering the requests.

These algorithms are relevant only if there are several I/O requests pending.
For many PCs, the system is so underutilized that there are rarely multiple
outstanding I/O requests and hence no scheduling is possible. At the other
extreme, many large servers, are I/O bound with significant queues of pending
I/O requests. For these systems, effective disk arm scheduling is crucial.

Although disk scheduling algorithms are performed by the OS, they are also
sometimes implemented in the electronics on the disk itself. The disks I
brought to class were somewhat old so I suspect those didn't implement

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
scheduling, but the then-current operating systems definitely did.

We study the following algorithms all of which are quite simple.

  1. FCFS (First Come First Served).  
The most primitive. Some would called this no scheduling, but I wouldn't.

  2. Pick.  
Same as FCFS but pick up requests for cylinders that are passed on the way to
the next FCFS request.

  3. SSTF or SSF (Shortest Seek (Time) First).  
Use the greedy algorithm and go to the closest requested cylinder. This
algorithm can starve requests. To prevent starvation, one can periodically
enter a FCFS mode, but SSTF would still be unfair. Typically, cylinders in the
middle receive better service than do cylinders on both extremes.

  4. Scan (Look, Elevator).  

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
This is the method used by an old fashioned jukebox (remember Happy Days) and
by elevators.

Those jukeboxes stole coins since requesting an already requested song was a
nop.

The disk arm proceeds in one direction picking up all requests until there are
no more requests in this direction at which point it goes back the other
direction. This favors requests in the middle, but can't starve any requests.

  5. N-step Scan.  
This is what the natural implementation of Scan actually does. The idea is
that requests are serviced in batches. Specifically, it works as follows.

    * While the disk is servicing a Scan direction, the controller gathers up new requests and sorts them.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
    * At the end of the current sweep, the new list becomes the next sweep.
    * Compare this to selfish round robin (SRR) with a≥b=0. 
  6. C-Scan (C-look, Circular Scan/Look).  
Similar to Scan but only service requests when moving in one direction. Let's
assume it services requests when the head is moving from low-numbered
cylinders to high-numbered one. When there are no pending requests for a
cylinder with number higher than the present head location, the head is sent
to the lowest-numbered, requested cylinder. C-Scan doesn't favor any spot on
the disk. Indeed, it treats the cylinders as though they were a clock, i.e.,
after the highest numbered cylinder comes cylinder 0.

#### Minimizing Rotational Latency

Once the heads are on the correct cylinder, there may be several requests to

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
service. All the systems I know, use Scan based on sector numbers to retrieve
these requests.  
**Question**: Why in this case is Scan same as C-Scan?  
**Answer**: Because the disk rotates in only one direction.

The above is certainly correct for requests to the same track. If requests are
for different tracks on the same cylinder, a question arises of how fast the
disk can switch from reading one track to another on the same cylinder. There
are two components to consider.

  1. How fast can it switch the electronics so that the signal from a different head is the one outputted by the disk?
  2. If the disk are is positioned so that one head is over cylinder k, are all the heads _exactly_ over cylinder k. 

The electronic switching is very fast. I doubt that would be an issue. The

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
second point is more problematic. I know it was not true in the 1980s: I
proposed a disk in which all tracks in a cylinder were read simultaneously and
coupled this parallel readout disk with with some network we had devised.
Alas, a disk designer explained to me that the heads are _not_ perfectly
aligned with the tracks.

**Homework:** 31\. Disk requests come into to the disk driver for sylinders 10, 22, 20, 2, 40, 6, and 38, in that order. A seek takes 6 ms per cylinder. How much seek time is needed for

  1. First come, first served.
  2. Closest cylinder next.
  3. Elevator algorithm (initially moving upward).

**Homework:** 33\. A salesman claimed that their version of Unix was very fast. For example, their disk driver used the elevator algorithm to reorder requests for different cylinders. In addition, the driver queued multiple requests for the same cylinder in sector order. Some hacker bought a version of the OS and tested it with a program that read 10,000 blocks randomly ch
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
osen across the disk. The new Unix was not faster than an old one that did FCFS for all requests. What happened?

#### Track Caching

Often the disk/controller caches (a significant portion of) the entire track
whenever it access a block, since the seek and rotational latency penalties
have already been paid. In fact modern disks have multi-megabyte caches that
hold many recently read blocks. Since modern disks cheat and don't have the
same number of blocks on each track, it is better for the disk electronics
(and not the OS or controller) to do the caching since the disk is the only
part of the system to know the true geometry.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.4.4 Error Handling

Most disk errors are handled by the device/controller and not the OS itself.
That is, disks are manufactured with more sectors than are advertised and
spares are used when a bad sector is referenced. Older disks did not do this
and the operating system would form a secret file of bad blocks that were
never to be used.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.4.A Ram Disks

  * Fairly clear. Organize a region of memory as a set of blocks and pretend it is a disk.
  * A problem is that memory is volatile.
  * Often used during OS installation, before disk drivers are available (there are many types of disk but all memory looks the same so only one ram disk driver is needed). 


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.4.5 Stable Storage

Skipped.

![clock](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/clock.png)


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 5.5 Clocks (Timers)


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.5.1 Clock Hardware

The hardware is simple. It consists of

  * An oscillator that generates a pulse at a known fixed frequency.
  * A counter that is decremented at each pulse.
  * A register that can be used to reload the counter.
  * Electronics that generate an interrupt whenever the counter reaches zero. 

The counter reload can be automatic or under OS control. If it is done
automatically, the interrupt occurs periodically (the frequency is the
oscillator frequency divided by the value in the register).

The value in the register can be set by the operating system and thus this
**programmable clock** can be configured to generate periodic interrupts and
any desired frequency (providing that frequency divides the oscillator
frequency).


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.5.2 Clock Software

As we have just seen, the clock hardware simply generates a periodic
interrupt, called the **clock interrupt**, at a set frequency. Using this
interrupt, the OS software can accomplish a number of important tasks.

#### Time of Day (TOD)

The basic idea is to increment a counter each clock tick (i.e., each
interrupt). The simplest solution is to initialize this counter at boot time
to the number of ticks since a fixed date (Unix traditionally uses midnight, 1
January 1970). Thus the counter always contains the number of ticks since that
date and hence the current date and time is easily calculated. Two questions
arise.

  1. From what value is the counter initialized?

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  2. What about overflow?

Three methods are used for initialization. The system can contact one or more
know time sources (see the Wikipedia entry for NTP), the human operator can
type in the date and time, or the system can have a battery-powered, backup
clock. The last two methods only give an approximate time.

Overflow is a real problem if a 32-bit counter is used. In this case two
counters are kept, the low-order and the high-order. Only the low order is
incremented each tick; the high order is incremented whenever the low order
overflows. That is, a counter with twice as many bits is simulated.

#### Time Quantum for Round Robin Scheduling

The system decrements a counter at each tick. The quantum expires when the
counter reaches zero. The counter is loaded when the scheduler runs a proce
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
ss
(i.e., changes the state of the process from ready to running). This is what I
(and I would guess you) did for the (processor) scheduling lab.

#### Accounting

At each tick, bump a counter in the process table entry for the currently
running process.

#### Alarm System Call and System Alarms

Users can request a signal at some future time (the Unix alarm system call).
The system also on occasion needs to schedule some of its own activities to
occur at specific times in the future (e.g., exercise a network time out).

The conceptually simplest solution is to have one timer for each event.
Instead, we simulate many timers with just one using the data structure on 
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
the
right with one node for each event.

![multiple-timers](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams
/multiple-timers.png)

  * The first entry in each node is the time _after_ the preceding event that this event's alarm is to ring.
  * For example, if the time is zero, this event occurs at the same time as the previous event.

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  * The second entry in the node is a pointer to the action to perform.
  * At each tick, the system decrements next-signal.
  * When next-signal goes to zero, we process the first entry on the list and any others immediately following with a time of zero (which means they are to be simultaneous with this alarm). We then set next-signal to the value in the next alarm.

#### Profiling

The objective is to obtain a histogram giving how much time was spent in each
software module of a given user program.

The program is logically divided into blocks of say 1KB and a counter is
associated with each block. At each tick the profiled code checks the program
counter and bumps the appropriate counter.

After the program is run, a (user-mode) utility program can determine the
software module associated with each 1K block and present the fraction of

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
execution time spent in each module.

If we use a finer granularity (say 10B instead of 1KB), we get increased
accuracy but more memory overhead.

**Homework:** 37\. The clock interrupt handler on a certain computer requires 2msec (including process switching overhead) per clock tick. The clock runs at 60 Hz. What fraction of the CPU is devoted to the clock.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.5.3 Soft Timers

Skipped.


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
## 5.6 User Interfaces: Keyboard, Mouse, Monitor


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
### 5.6.2 Input Software

#### Keyboard Software

At each key press and key release a scan code is written into the keyboard
controller and the computer is interrupted. By remembering which keys have
been depressed and not released the software can determine Cntl-A, Shift-B,
etc.

There are two fundamental modes of input, traditionally called _raw_ and
_cooked_ in Unix and now sometimes called noncanonical and canonical in POSIX.
In raw mode the application sees every character the user types. Indeed, raw
mode is character oriented. All the OS does is convert the keyboard scan codes
to characters and and pass these characters to the application. For example


 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---
  1. down-cntl down-x up-x up-cntl is converted to cntl-x
  2. down-cntl up-cntl down-x up-x is converted to x
  3. down-cntl down-x up-cntl up-x is converted to cntl-x (I just tried it to be sure).
  4. down-x down-cntl up-x up-cntl is converted to x

Full screen editors use this mode.

Cooked mode is line oriented. The OS delivers lines to the application program
after cooking them as follows.

  * Special characters are interpreted as editing characters (erase-previous-character, erase-previous-word, kill-line, etc).
  * Erased characters are not seen by the application but are erased by the keyboard driver.
  * Also needed is an escape character so that the editing characters can be passed to the application if desired.
  * The cooked characters must be echoed (what should one do if the application is also generating output at this time?)

 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

---

The (possibly cooked) characters must be buffered until the application iss</textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>