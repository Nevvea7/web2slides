<!DOCTYPE html>
<html>
  <head>
    <title>NYU OS 202 Lecture Slides</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

Start Lecture #5



---
### 2.1.6A: An Addendum on Interrupts

This should be compared with the addenda on transfer of control and trap.

In a well defined location in memory (specified by the hardware) the OS stores
an **interrupt vector**, which contains the address of the interrupt handler.

  * Tanenbaum calls the interrupt handler the interrupt service routine.
  * Actually one can have different priorities of interrupts and the interrupt vector then contains one pointer for each level. This is why it is called a vector.

Assume a process P is running and a disk interrupt occurs indicating the
completion of a disk read previously issued by process Q, which is currently
blocked. Note that disk interrupts are unlikely to be for the currently
running process (because the process that initiated the disk access is likely


---
to be blocked).

#### Actions by P Just Prior to the Interrupt:

  1. Who knows??  
This is the difficulty of debugging code depending on interrupts, the
interrupt can occur (almost) anywhere. Thus, we do _not_ know what happened
just before the interrupt.

    * With hardware monitors we _can_ determine what happened for this particular execution.
    * But, if we re-execute the process, the interrupt will likely occur at a different point.
    * Indeed, we do not even know which process P was running when the interrupt occurred (again we can determine this after the fact for one execution, but a re-execution will likely be different).
    * We cannot (even for one specific execution) point to an instruction and say this instruction _caused_ the interrupt.
    * The best we can do is (for one specific execution) point to an instru

---
ction and say  this instruction _immediately preceeded_ the interrupt .

#### Executing the interrupt itself:

  2. The hardware saves the program counter and some other registers (or switches to using another set of registers, the exact mechanism is machine dependent).
  3. The hardware finds the address of the interrupt handler in a table and jumps to it.  
Steps 2 and 3 are similar to a procedure call. But the interrupt is
asynchronous.

  4. As with a trap, the hardware automatically switches the system into supervisor mode. (It might have been in supervisor mode already. That is, an interrupt can occur in supervisor or user mode.)

#### Actions by the interrupt handler (et al) upon being activated

  5. An assembly language routine saves registers.
  6. The assembly routine sets up a new stack. (These last two steps are of

---
ten called setting up the C environment.)
  7. The assembly routine calls a procedure in a high level language, often the C language (Tanenabaum forgot this step).
  8. The C procedure does the real work. 
    * Determines what caused the interrupt (in this case a disk completed an I/O).
    * How does it figure out the cause? 
      * It might know the priority of the interrupt being activated.
      * The controller might write information in memory before the interrupt.
      * The OS might read registers in the controller.
    * Mark process Q as ready to run. 
      * That is move Q to the _ready list_ (note that again we are viewing Q as a data structure).
      * Q is now in the ready state; it was in the blocked state before.
      * The code that Q needs to run initially is likely to be OS code. For example, the data just read is probably now in kernel space and the OS needs to copy it into user space.
    * Now we have at least two processes ready to run, namely P and Q. There may be arbitrarily many others.


---
  9. The scheduler decides which process to run, P or Q or something else. (This very loosely corresponds to g calling other procedures in the simple f calls g case we discussed previously). Eventually the scheduler decides to run P.

#### Actions by The OS When Returning Control to P

  10. The C procedure (that did the real work in the interrupt processing) continues and returns to the assembly code.
  11. Assembly language restores P's state (e.g., registers) and starts P at the point it was when the interrupt occurred.

#### Properties of Interrupts

  * Phew.
  * Unpredictable (to an extent). We cannot tell what was executed just _before_ the interrupt occurred. That is, the control transfer is asynchronous; it is difficult to ensure that everything is always prepared for the transfer.
  * The user code is _unaware_ of the difficulty and cannot (easily) detect

---
 that it occurred. This is another example of the OS presenting the user with a virtual machine environment that is more pleasant than reality (in this case synchronous rather asynchronous behavior).
  * Interrupts can also occur when the OS itself is executing. This can cause _serious_ difficulties since both the main line code (i.e., the code that was interrupted and the interrupt handling code are from the same program, namely the OS, and hence might well be using the same variables. We will soon see how this can cause great problems even in what appear to be trivial cases.
  * The interprocess control transfer is _neither_ stack-like _nor_ queue-like:  
If P was running, then Q was running, then R was running, then S was running,
and finally the interrupt occurs, the next process to be run might be any of
P, Q, R, or S (or some other process).

  * The system might have been in user-mode or supervisor mode when the interrupt occurred. The interrupt processing starts in supervisor mode.



---
#### The OS Running As a User Process

In traditional Unix and Linux, if an interrupt occurs while a user process
with PID=_P_ is running, the system switches to kernel mode and OS code is
executed, but the PID is still _P_. The owner of process _P_ is charged for
this execution. Try running the time program on one of the Unix systems and
noting the output.



---
### 2.1.7 Modeling Multiprogramming (Crudely)

Consider a job that is unable to compute (i.e., it is waiting for I/O) a
fraction p of the time.

  * With monoprogramming, the CPU utilization is 1-p.
  * Note that p is often > .5, so CPU utilization is poor.
  * But, if n jobs are in memory, then the probability that all n are waiting for I/O is approximately pn. So, with a **multiprogramming level** (MPL) of n, the CPU utilization is approximately 1-pn.
  * If p=.5 and n=4, then the utilization 1-pn=15/16 is much better than the monoprogramming (n=1) utilization of 1/2.

There are at least two causes of inaccuracy in the above modeling procedure.

  * Some CPU time is spent by the OS in switching from one process to another. So the "useful utilization", i.e. the proportion of time the CPU is executing user code, is lower than predicted.
  * The model assumes that the probability that one process is waiting for 

---
I/O is independent of the probability that another process is waiting for I/O. This assumption was used when we asserted that the probability all n jobs are waiting for I/O is pn.

Nonetheless, it is correct that increasing MPL does increase CPU utilization
up to a point.

An important limitation is memory. That is, we assumed that we have many jobs
loaded at once, which means we must have enough memory for them. There are
other memory-related issues as well and we will discuss them later in the
course.

**Homework:**

  * 1\. In Figure 2-2, three process states are shown. In theory, with three states, there could be six transitions. However, only four transitions are shown. Are there any circumstances in which either of both of the missing transitions might occur?


---
  * What is the key difference between a trap and an interrupt?
  * 7\. Multiple jobs can run in parallel and finish faster than if they had run sequentially. Suppose that two jobs, each of which needs 10 minutes of CPU time, start simultaneously. How long with the last one take to complete if they run sequentially? How log if they run in parallel? Assume 50% I/O wait.

**Remark**: Our final exam is Thursday 12 May at 4PM. Check out [ the official list](//cs.nyu.edu/dynamic/courses/exams)



---
![threads](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/threads.png)



---
## 2.2 Threads

The idea behind _threads_ to have separate threads of control (hence the name)
running in the address space of a single process as shown in the diagram to
the right. An _address space_ is a memory management concept. For now think of
an address space as the memory in which a process runs. (In reality it also
includes the mapping from virtual addresses, i.e., addresses in the program,
to physical addresses, i.e., addresses in the machine).

Each thread is somewhat like a process (e.g., it shares the processor with
other threads), but a thread contains less state than a process (e.g., the
address space belongs to the process in which the thread runs.)



---
### 2.2.1 Thread Usage

Often, when a process P executing an application is blocked (say for I/O),
there is still computation that can be done for the application. Another
process can't do this computation since it doesn't have access to P's memory.
But two threads in the same process do share memory so that problem doesn't
occur.

The downside of this memory sharing among threads is that each thread is not
protected from the others in its process. We will see in section 2.3 that
having multiple threads concurrently accessing the same memory can cause
subtle bugs is programs that look too simple to be wrong.

So it is a ofter performance/simplicity trade-off.

#### Two Threads in One Process vs Two Processes



---
Although there are many differences, we will be primarily interested in just
two.

  1. Threads in the same process share memory.
  2. Switching execution from one thread to another in the same process is much faster than switching execution from one process to another processes.

#### A Producer Consumer Pipeline

    
    
    loop
      Read 10KB from disk1 to inBuffer
      Compute from inBuffer to outBuffer
      Write outBuffer to disk2
    end loop
     
     


---
     
    //  process 1
    loop
      Read data from disk1 to inBuffer
    end loop
     
    //  process 2
    loop
      Compute from inBuffer to outBuffer
    end loop
     
    //  process 3
    loop
      Write outBuffer to disk2
    end loop
    

Consider the first frame of code on the right. Assume for simplicity each line
takes 10ms so the entire loop processes 10KB every 30ms. However, the CPU i

---
s
busy only during the second line and, (if the I/O system is sophisticated),
the first and third lines use separate hardware.

Hence in principle the three lines could all proceed at the same time. That is
we could turn the three steps into a pipeline so that after the startup phase,
the loop would process 10KB every 10ms, a 3X speed improvement.

The second frame show an attempt to speed up the application by splitting it
into three processes: a reader, a computer, and a writer. However this can't
work since the two inBuffers are not the same so processes 1 and 2 aren't
communicating. Similarly for processes 2 and 3.

If instead the three loops were each a thread within the same process, then


---
the two uses of inBuffer would refer to the same variable and similarly for
outBuffer. Hence our desired speedup would occur.

Another advantage of the threaded solution over the separate process non-
solution, is that the system can switch between threads in the same process
faster than it can switch between separate processes.

##### Double Buffering in the Producer Consumer Pipeline

The solution above is simplistic and would fail. Users of the same buffer must
coordinate their actions, and you need at least two inBuffers and two
outBuffers as shown in the solution immediately following.

The diagram on the right shows the actions during the first four time stems.
The disk on the right contains the input and the one on the left will contain


---
the output. The two circles on the right are input buffers and the two on the
left are output buffers. Initially, all the buffers are invalid (i.e., contain
no data).

![pipeline](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/pipeline.png)

  1. During the first time step, as indicated by the arrow in the diagram, 

---
thread 1 reads data from the input disk to the top input buffer. For this time step one disk is active (thread 1), the other disk (thread 3) and the cpu (thread 2) are inactive, which is no better that the simpler non-threaded solution.  
  

  2. The second time step is better. Thread 1 again reads data from the input disk, but this time into the bottom input buffer. The top input buffer is blue indicating that it contains valid data. Thread 2 uses the cpu to compute during this time step. It reads from the top input buffer and writes to the top output buffer. Note that the thread is reading valid data. Both thread 1 and thread 2 are active during this time step.   
  

  3. Starting with the third time step, we hit top speed, all three threads are busy. Thread 1 reads the input disk into the top input buffer, overwriting what was written before (that is why the circle is no longer blue). Thread 2 computes using the bottom buffers. Thread 3 writes from the top (blue, i.e. valid) output buffer to the output disk.  
  


---

  4. Subsequent time step are similar to time step 3 (until the input is exhausted). As shown in the diagram, these steps alternate their usage of the top and bottom buffers.

#### A Multithreaded Web Server

An important modern example of threading is a multithreaded web server. Each
thread is responding to a single WWW connection. While one thread is blocked
on I/O, additional threads can process other WWW connections.  
**Question**: Why not use separate processes, i.e., what is the shared memory?  
**Answer**: The cache of frequently referenced pages.

#### Dispatchers and Workers

A common organization for a multithreaded application is to have a dispatcher


---
thread that fields requests and then passes each request on to an idle worker
thread. Since the dispatcher and workers share memory, passing the request is
very low overhead.

A multithreaded web server can be organized this way.

#### Helper Tasks

A final (related) example occurs when a main line task interfaces with the
user and sometimes needs to perform a lengthy task that does not directly
affect the user-interface.

Tanenbaum considers a word processor currently editing a large file (say a
book the user is writing) and the user deletes a word early in the book. This
can cause changes on all subsequent pages. Hence, reformatting the book could
cause a detectable delay in the user interface. With a threaded


---
implementation, a second thread can be assigned the reformatting task while
the primary thread continues to interface with the user. It is only when the
user wishes to examine a page near the end of the book that they must wait for
the second thread to finish. Hopefully, the user has been doing other editing
in the beginning of the book so that the second thread is finished prior to
the user needing to access pages near the end. Even if the second thread is
not finished, it will have accomplished some of the work while the user is
still editing near the book's beginning.

In this same example, the word processor may wish to perform automatic
backups. Again another thread to do this. In this way the thread that
interfaces with the user is not blocked during the backup. However some
coordination between threads may be needed so that the backup is of a
consistent state.


---



---
### 2.2.2 The Classical Thread Model

Process-Wide vs Thread-Specific Items

Per process itemsPer thread items

Address space

Program counter

Global variables

Machine registers

Open files

Stack

Child processes



---
Pending alarms

Signals and signal handlers

Accounting information

A process contains a number of resources such as address space, open files,
accounting information, etc. In addition to these resources, a process has a
thread of control, e.g., program counter, register contents, stack. The idea
of threads is to permit multiple threads of control to execute within one
process. This is often called **multithreading** and threads are sometimes
called **lightweight processes**. Because threads in the same process share so
much state, switching between them is much less expensive than switching
between separate processes. The table on the right shows which properties are
common to all threads in a given process and which properties are thread


---
specific.

Individual threads within the same process are not completely independent. For
example there is no memory protection between them. This is typically not a
security problem as the threads are cooperating and all are from the same user
(indeed the same process). However, the shared resources do make debugging
harder. For example one thread can easily overwrite data needed by another
thread in the process and when the second thread fails, the cause may be hard
to determine because the tendency is to assume that the failed thread caused
the failure.

You may recall that a serious advantage of microkernel OS design was that the
separate OS processes could not, even if buggy, damage each others data
structures.


---

A new thread in the same process is created by a routine named something like
_thread_create_; similarly there is _thread_exit_. The analogue to waitpid is
_thread_join_ (the name presumably comes from the fork-join model of parallel
execution).

The routine _tread_yield_, which relinquishes the processor, does not have a
direct analogue for processes. The corresponding system call (if it existed)
would move the process from running to ready. It would be as if the process
preempted itself.

**Homework:** 15\. Why would a thread ever voluntarily give up the CPU by calling thread_yield? After all, since there is no periodic clock interrupt, it may never get back the CPU?


---

#### Challenges and Questions

Assume a process has several threads. What should we do if one of these
threads

  1. Executes a fork?
  2. Closes a file?
  3. Requests more memory?
  4. Moves a file pointer via lseek?



---
### 2.2.3 POSIX Threads

POSIX threads (pthreads) is an IEEE standard specification that is supported
by many Unix and Unix-like systems. Pthreads follows the classical thread
model above and specifies routines such as pthread_create, pthread_yield, etc.

An alternative to the classical model are the so-called Linux threads, which
are discussed in section 10.3 of the 4e.



---
### 2.2.4 Implementing Threads in User Space

Write a (threads) library that acts as a mini-scheduler and implements
_thread_create_, _thread_exit_, _thread_wait_, _thread_yield_, etc. This
library acts as a run-time system for the threads in this process. The central
data structure maintained and used by this library is a _thread table_, the
analogue of the process table in the operating system itself.

There is a thread table and an instance of the threads library in _each_
multithreaded process.

Advantages of User-Mode Threads:

  * Requires **no** OS modification. Previously, this was the primary advantage since most operating systems did not support threads. Now, the major systems do support threads so this advantage is less significant. 
  * Very fast since no context switching.
  * Can customize the scheduler for each application.


---

Disadvantages

  * Blocking system calls cannot be executed directly since that would block the entire process. For example, consider the producer consumer example above implemented in the natural manner with user-mode threads. This implementation would not work well since, whenever an I/O was issued that caused the process to block, all the threads would be unable to run (but see just below).
  * Similarly a page fault would block the entire process (i.e., all the threads).
  * In addition, a thread with an infinite loop prevents all other threads in this process from ever running.

#### Possible Methods of Dealing With Blocking System Calls

  * Perhaps the OS supplies a non-blocking version of the system call, e.g. a non-blocking read.
  * Perhaps the OS supplies another system call that tells if the blocking system call will in fact block. For example, a unix select() can be used to

---
 tell if a read would block. It might not block if, for example, 
    * The requested disk block is in the buffer cache (see the I/O chapter).
    * The request was for a keyboard or mouse or network event that has already happened.

#### Relevance to Multiprocessors/Multicore

For a uniprocessor, which is all we are officially considering, there is
little gain in splitting pure computation into pieces. If the CPU is to be
active all the time for all the threads, it is simpler to just have one
(unithreaded) process.

But this changes for multiprocessors/multicores. Now it is **very** useful to
split computation into threads and have each executing on a separate
processor/core. In this case, user-mode threads are wonderful, there are no
system calls and the extremely low overhead is beneficial.



---
However, there are serious issues involved is programming applications for
this environment.



---
### 2.2.5 Implementing Threads in the Kernel

Modern operating systems have direct support for threads, i.e., the thread
operations are implemented in the kernel itself. This naturally required that
the operating system was (significantly) modified and was not a trivial
undertaking.

  * There is only one thread table for the entire system and it is in the OS.
  * Thread-create and friends are now system calls and hence much slower than with user-mode threads. They are, however, still much faster than creating/switching/etc processes since there is so much shared state that does not need to be recreated.
  * A thread that blocks causes no particular problem. The kernel can run another thread from this process (or can run a thread from another process).
  * Similarly a page fault or infinite loop in one thread does not automatically block the other threads in the process.



---
### 2.2.6 Hybrid Implementations

One can write a (user-level) thread library even if the kernel also has
threads. This is sometimes called the N:M model since N user-mode threads run
on M kernel threads. In this scheme, the kernel threads cooperate to execute
the user-level threads.

  * Different kernel threads in the same process can have differing numbers of user threads assigned to them.
  * Switching between user-level threads within one kernel thread is very fast (no context switch). It is essentially the same as in the case of pure user-mode threads.
  * Switching between kernel threads of the same process requires a system call and is essentially the same as in the case of pure kernel-level threads.
  * Since a blocking system call or page fault blocks only one kernel thread, the multi-threaded application as a whole can still run since user-level threads in other kernel-level threads of this process are still runnable.


---

An offshoot of the N:M terminology is that kernel-level threading (without
user-level threading) is sometimes referred to as the 1:1 model since one can
think of each thread as being a user level thread executed by a dedicated
kernel-level thread.

**Homework:**.

  * 16\. Can a thread ever be preempted by a clock interrupt. If so, under what circumstances? If not, why not?
  * 18\. What is the biggest advantage of implementing threads in user space? What is the biggest disadvantage?



---
### 2.2.7 Scheduler Activations

Skipped



---
### 2.2.8 Popup Threads

The idea is to automatically issue a thread-create system call upon message
arrival. (The alternative is to have a thread or process blocked on a
_receive_ system call.) If implemented well, the latency between message
arrival and thread execution can be very small since the new thread does not
have state to restore.



---
### 2.2.9 Making Single-threaded Code Multithreaded

Definitely _NOT_ for the faint of heart.

  * There often is state that should not be shared. A well-cited example is the unix _errno_ variable that contains the error number (zero means no error) of the error encountered by the last system call. Errno is hardly elegant (even in normal, single-threaded, applications), but its use is widespread. If multiple threads issue faulty system calls the errno value of the second overwrites the first and thus the first errno value may be lost.
  * Much existing code, including many libraries, are not _re-entrant_.
  * Managing the shared memory inherent in multi-threaded applications opens up the possibility of race conditions that we will be studying next.
  * What should be done with a signal sent to a process. Does it go to all or one thread?
  * How should stack growth be managed. Normally the kernel grows the (single) stack automatically when needed. What if there are multiple stacks?

**Note**: We shall do section 2.4 before section 2.3 for two reasons.



---
  1. Sections 2.3 and 2.5 are closely related; having 2.4 in between seems awkward to me.
  2. Lab 2 will use material from 2.4 so I don't want to push 2.4 after 2.5.

![process-states](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/process-
states.png)



---
## 2.4 Process Scheduling

Scheduling processes on the processor is often called processor scheduling or
process scheduling or simply scheduling. As we shall see later in the course,
a more precise name would be short-term, processor scheduling.

At this point we are discussing the two arcs connecting runningâ†”ready in the
diagram on the right, which shows the various states of a process and the
transitions between those states. Medium term scheduling is discussed later
(as is disk-arm scheduling).

As you would expect, the part of the OS responsible for (short-term,
processor) scheduling is called the (short-term, processor) **scheduler** and
the algorithm used is called the (short-term, processor) **scheduling
algorithm**.


---



---
### 2.4.1 Introduction to Scheduling

#### Importance of Scheduling for Various Generations and Circumstances

Early computer systems were monoprogrammed and, as a result, scheduling was a
non-issue.

For many current personal computers, which are definitely multiprogrammed,
there is in fact very rarely more than one runnable process. As a result,
scheduling is not critical.

For servers, scheduling is indeed important and these are the systems you
should think of.

#### Process Behavior

A processes alternates between CPU activity and I/O activity, which I often
refer to as CPU bursts and I/O bursts. In particular the Scheduling lab wil

---
l
use that terminology.

Since (as we shall see when we study I/O) the time required for a disk access
depends only weakly on the size of the request, the key distinguishing factor
between compute-bound (aka CPU-bound) and I/O-bound jobs is the length of the
CPU bursts.

The trend over the past decade or two has been for more and more jobs to
become I/O-bound since the CPU speed has increased much faster than I/O speed.

</textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>