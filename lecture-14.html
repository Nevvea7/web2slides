<!DOCTYPE html>
<html>
  <head>
    <title>NYU OS 202 Lecture Slides</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">
 _Contents generated from: http://cs.nyu.edu/~gottlieb/courses/os202/class-notes.html_

Start Lecture #14

#### Structure of a Page Table Entry

Each page has a corresponding page table entry (PTE). The information in a PTE
is used by the hardware and its format is machine dependent; thus the OS
routines that access PTEs are not portable. Information set by and used by the
OS is normally kept in other OS tables.

(Actually some systems, those with software TLB reload, do not require
hardware access to the page table.)

The page table is indexed by the page number; thus the page number is **not**
stored in the table.

The following fields are often present in a PTE.



---
  1. The _Frame Number_. This field is the main reason for the table. It gives the virtual to physical address translation. It is the only field in the page table for (non-demand) paging.
  2. The _Valid_ bit. This tells if the page is currently loaded (i.e., is in a frame). If the bit is set, the page is in memory and the frame number in the PTE is is valid It is also called the _presence_ or _presence/absence_ bit. If a page is accessed whose valid bit is unset, a _page fault_ is generated by the hardware.
  3. The _Modified_ or _Dirty_ bit. Indicates that some part of the page has been written since it was loaded. This is needed when the page is evicted so that the OS can tell if the page must be written back to disk.
  4. The _Referenced_ or _Used_ bit. Indicates that some word in the page has been referenced. Used to select a victim: unreferenced pages make good victims by the locality property (discussed below).
  5. _Protection_ bits. For example one can mark text pages as execute only. This requires that boundaries between regions with different protection are on page boundaries. Normally many consecutive (in logical address) pages have the same protection so many page protection bits are redundant. Protection is more naturally done with segmentation, but in many current systems, it is done with paging since the systems don't utilize segmentation (even

---
 though the hardware supports it).

**Question**: Why not store the disk addresses of non-resident pages in the PTE?  
**Answer**: On most systems the PTEs are accessed by the hardware automatically on a TLB miss (see immediately below). Thus the format of the PTEs is determined by the hardware and contains only information used on page hits. Hence the disk address, which is only used on page faults, is not present.



---
### 3.3.3 Speeding Up Paging

As mentioned above, the simple scheme of storing the page table in its
entirety in central memory alone appears to be both too slow and too big. We
address both these issues here, but note that a second solution (segmentation)
to the size question is discussed later.

####  Translation Lookaside Buffers (and General Associative Memory)

**Note:** Tanenbaum suggests that associative memory and translation lookaside buffer are synonyms. This is wrong. Associative memory is a general concept of which translation lookaside buffer is a specific example.

An **associative memory** is a _content addressable memory_. That is you
access the memory by giving the _value_ of some field (called the index) and
the hardware searches all the records and returns the record whose index field


---
contains the requested value.

For example

    
    
      Name  | Animal | Mood     | Color
      ======+========+==========+======
      Moris | Cat    | Finicky  | Grey
      Fido  | Dog    | Friendly | Black
      Izzy  | Iguana | Quiet    | Brown
      Bud   | Frog   | Smashed  | Green
    

If the index field is Animal and Iguana is given, the associative memory
returns

    
    
      Izzy  | Iguana | Quiet    | Brown


---
    

A **Translation Lookaside Buffer** or **TLB** is an associate memory where the
index field is the page number. The other fields include the frame number,
dirty bit, valid bit, etc.

Note that, unlike the situation with a the page table, the page number **is**
stored in the TLB; indeed it is the index field.

A TLB is _small and expensive_ but at least it is _fast_. When the page number
is in the TLB, the frame number is returned very quickly.

On a miss, a _TLB reload_ is performed. The page number is looked up in the
page table. The record found is placed in the TLB and a victim is discarded
(not really discarded, dirty and referenced bits are copied back to the PTE

---
).
There is no placement question since all TLB entries are accessed at once and
hence are equally suitable. But there is a replacement question.

**Homework:** 22\. A computer whose processes have 1024 pages in their address spaces keeps its page tables in memory. The overhead required for reading a word from the page table is 5 nsec. To reduce this overhead, the computer has a TLB, which holds 32 (virtual page, physical page frame) pairs, and can do a look up in 1 nsec. What hit rate is needed to reduce the mean overhead to 2 nsec?

As the size of the TLB has grown, some processors have switched from single-
level, fully-associative, unified TLBs to multi-level, set-associative,
separate instruction and data, TLBs.

We are actually discussing caching, but using different terminology.

  * Page frames are a cache for pages (one could say that central memory is

---
 a cache of the disk).
  * The TLB is a cache of the page table.
  * Also the processor almost surely has a cache (most likely several) of central memory.
  * In all the cases, we have small-and-fast acting as a cache of big-and-slow. However what is big-and-slow in one level of caching, can be small-and-fast in another level.

#### Software TLB Management

The words above assume that, on a TLB miss, the MMU (i.e., hardware and not
the OS) loads the TLB with the needed PTE and then performs the virtual to
physical address translation.

Some newer systems do this in software, i.e., the OS **is** involved.



---
![2-level-page-tables](http://cs.nyu.edu/~gottlieb/courses/os202/diagrams/2
-level-page-table.png)

#### Multilevel Page Tables

Recall the diagram above showing the data and stack growing towards each
other. Most of the virtual memory is the unused space between the data and
stack regions. However, with demand paging this space does _not_ waste real
memory. But the single large page table **does** waste real memory.

The idea of multi-level page tables (a similar idea is used in Unix i-node-
based file systems, which we study later when we do I/O) is to add a level of
indirection and have a page table containing pointers to page tables.

  * Imagine one big page table, which we will (eventually) call the second level page table.


---
  * We want to apply paging to this large table, viewing it as simply memory not as a page table. So we (logically) cut it into pieces each the size of a page. Note that many (typically 1024 or 2048) PTEs fit in one page so there are far fewer of these pages than PTEs.
  * Now construct a _first level page table_ containing PTEs that point to the pages produced in the previous bullet.
  * This first level PT is small enough to store in memory. It contains one PTE for every page of PTEs in the 2nd level PT, which reduces space by a factor of one or two thousand.
  * But since we still have the 2nd level PT, we have made the world bigger not smaller!
  * Don't store in memory those 2nd level page tables all of whose PTEs refer to unused memory. That is _use demand paging on the (second level) page table_!

This idea can be extended to three or more levels. The largest I know of has
four levels. We will be content with two levels.

#### Address Translation With a 2-Level Page Table


---

For a two level page table the virtual address is divided into three pieces

    
    
      +-----+-----+-------+
      | P#1 | P#2 | Offset|
      +-----+-----+-------+
    

  * P#1 gives the index into the first level page table.
  * Follow the pointer in the corresponding PTE to reach the frame containing the relevant 2nd level page table.
  * P#2 gives the index into this 2nd level page table.
  * Follow the pointer in the corresponding PTE to reach the frame containing the (originally) requested page.
  * Offset gives the offset in this frame where the originally requested word is located.



---
Do an example on the board.

The VAX used a 2-level page table structure, but with some wrinkles (see
Tanenbaum for details).

Naturally, there is no need to stop at 2 levels. In fact the SPARC has 3
levels and the Motorola 68030 has 4 (and the number of bits of Virtual Address
used for P#1, P#2, P#3, and P#4 can be varied). More recently, x86-64 also has
4-levels.

#### Inverted Page Tables

For many systems the virtual address range is much bigger that the size of
physical memory. In particular, with 64-bit addresses, the range is 264 bytes,
which is 16 million terabytes. If the page size is 4KB and a PTE is 4 bytes, a
full page table would be 16 thousand terabytes.


---

A two level table would still need 16 terabytes for the first level table,
which is stored in memory. A three level table reduces this to 16 gigabytes,
which is still large and only a 4-level table gives a reasonable memory
footprint of 16 megabytes.

An alternative is to instead keep a table indexed by frame number. The content
of entry f contains the number of the page currently loaded in frame f. This
is often called a **frame table** as well as an **inverted page table**.

Now there is one entry per **frame**. Again using 4KB pages and 4 byte PTEs,
we see that the table would be a constant 0.1% of the size of real memory.

But on a TLB miss, the system must **search** the inverted page table, which
would be hopelessly slow except that some tricks are employed. Specifically

---
,
hashing is used.

Also it is often convenient to have an inverted table as we will see when we
study global page replacement algorithms. Some systems keep both page and
inverted page tables.



---
## 3.4 Page Replacement Algorithms (PRAs)

These are solutions to the _replacement_ question. Good solutions take
advantage of _locality_ when choosing the victim page to replace.

  1. **Temporal locality**: If a word is referenced now, it is _likely_ to be referenced in the near future.  
This argues for _caching_ referenced words, i.e. keeping the referenced word
near the processor for a while.

  2. **Spatial locality**: If a word is referenced now, nearby words are _likely_ to be referenced in the near future.  
This argues for _prefetching_ words around the currently referenced word.

  3. Temporal and spacial locality are lumped together into **locality**: If any word in a page is referenced, each word in the page is likely to be referenced. So it is good to bring in the entire page on a miss and to keep the page in memory for a while.



---
When programs begin there is no history so nothing to base locality on. At
this point the paging system is said to be undergoing a cold start.

Programs exhibit phase changes in which the set of pages referenced changes
abruptly (similar to a cold start). An example would occurs in your linker lab
when you finish pass 1 and start pass 2. At the point of a phase change, many
page faults occur because locality is poor.

Pages belonging to processes that have terminated are of course perfect
choices for victims.

Pages belonging to processes that have been blocked for a long time are good
choices as well.



---
### Random PRA

A lower bound on performance. Any decent scheme should do better.



---
### 3.4.1 The Optimal Page Replacement Algorithm

Replace the page whose **next** reference will be furthest in the future.

  * Also called Belady's min algorithm.
  * Provably optimal. That is, no algorithm generates fewer page faults.
  * Unimplementable: Requires predicting the future.
  * Good upper bound on performance.



---
### 3.4.2 The Not Recently Used (NRU) PRA

Divide the frames into four classes and make a random selection from the
lowest nonempty class.

  0. Not referenced, not modified.
  1. Not referenced, modified.
  2. Referenced, not modified.
  3. Referenced, modified.

Assumes that in each PTE there are two extra flags R (for referenced;
sometimes called U, for used) and M (for modified, often called D, for dirty).

NRU is based on the belief that a page in a lower priority class is a better
victim, i.e., is less important.

  * If a page is not referenced, locality suggests that it probably will not referenced again soon and hence is a good candidate for eviction.


---
  * If a clean page (i.e., one that is not modified) is chosen to evict, the OS does not have to write it back to disk and hence the cost of the eviction is lower than for a dirty page.

Implementation

  * When a page is brought in, the OS resets R and M (i.e. R=M=0).
  * On a read, the hardware sets R.
  * On a write, the hardware sets R and M.

Old cartoons often had prisoners wearing broad horizontal stripes and using
sledge hammers to break up rocks.

This gives what I sometimes call the prisoner problem: If you do a good job of
making little ones out of big ones, but a poor job job of the reverse, you
soon wind up with all little ones.

In this case we do a great job setting R but rarely reset it. We need more


---
resets. Therefore, every k clock ticks, the OS resets all R bits.

**Question**: Why not reset M as well?  
**Answer**: If a dirty page has a clear M, we will not copy the page back to disk when it is evicted, and thus the only accurate version of the page will be lost!

What if the hardware doesn't set these bits?  
Answer: The OS can uses tricks. When the bits are reset, the PTE is made to
indicate that the page is not resident (which is a lie). On the ensuing page
fault, the OS sets the appropriate bit(s).

So now the R and M bits tell us the NRU class

  0. If R=M=0, the page has not been referenced (recently) and is not modified (clean).
  1. If R=0 and M=1, the page has not been referenced and has been modified (dirty).


---
  2. If R=1 and M=0, the page has been referenced and is clean.
  3. If R=1 and M=1, the page has been referenced and is dirty.



---
### 3.4.3 FIFO PRA

Simple algorithm. Basically, we try to be fair to the pages: the first one
loaded is the first one evicted.

The natural implementation is to have a queue of nodes each referring to a
resident page (i.e., pointing to a frame).

  * When a page is loaded, a node referring to the page is appended to the tail of the queue.
  * When a page needs to be evicted, the head node is removed and the page referenced is chosen as the victim.

This sound reasonable at first, but it is not a good policy. The trouble is
that a page referenced say every other memory reference and thus **very**
likely to be referenced soon will be evicted because we only look at the
**first** reference to a page, when we should be particularly interested in
**recent** references to the page.


---



---
### 3.4.4 Second chance PRA

Similar to the FIFO PRA, but altered so that a page recently referenced is
given a second chance.

  * When a page is loaded, a node referring to the page is appended to the tail of list queue. The list is maintained in apparent order of loading (i.e. apparently, but not really, FIFO) The R bit of the page is cleared.
  * When a page needs to be evicted, the head node is removed and the page referenced is the **potential** victim.
  * If the R bit is unset (the page hasn't been referenced recently), then the page **is** the victim.
  * If the R bit is set, the page is given a second chance. Specifically, the R bit is cleared, the time of loading is changed to NOW, and the node referring to this page is appended to the rear of the queue (so it appears to have just been loaded), and the current head node becomes the potential victim.
  * What if all the R bits are set?
  * We will move each page from the front to the rear and will arrive at the initial condition but with all the R bits now clear. Hence we will remove

---
 the same page as fifo would have removed, but will have spent more time doing so.
  * As in NRU we periodically clear all the R bits.



---
### 3.4.5 Clock PRA

Same algorithm as 2nd chance, but uses a better implementation, namely a
circular list with a single pointer serving as both head and tail pointer.

We assume that the most common operation is to choose a victim and replace it
by a new page.

  * We use a circular list for the nodes and have a pointer pointing to the head entry. Think of the list as the hours on a clock and the pointer as the hour hand. (Hence the name clock PRA.)
  * The operation we need to support efficiently is replace the oldest, unreferenced page by a given new page.
  * Examine the node pointed to by the clock hand. If the R bit of the corresponding page is set, we give the page a second chance: clear the R bit and set the time to NOW (so the page looks freshly loaded), advance the hand, and examine the next node.
  * Eventually we will reach a node whose R bit is clear. The corresponding page is the victim.


---
  * Replace the victim with the new page (may involve 2 I/Os as always).
  * Update the node to refer to this new page.
  * Move the hand forward another hour so that the new page is at the rear.

#### LIFO PRA

This is terrible! Why?  
Ans: All but the last frame are frozen once loaded so you can replace only one
frame. This is especially bad after a phase shift in the program as now the
program is references mostly new pages but only one frame is available to hold
them.

</textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>